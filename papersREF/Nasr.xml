<xml><article><preambule>Nasr.pdf</preambule><titre>MACAON
</titre><auteur>Alexis Nasr Fre&#769;de&#769;ric Be&#769;chet Jean-Franc&#807;ois Rey Beno&#305;&#770;t Favre Joseph Le Roux&#8727;
</auteur><abstract>Abstract
MACAON is a tool suite for standard NLP tasks
developed for French. MACAON has been designed to process both human-produced text
and highly ambiguous word-lattices produced
by NLP tools. MACAON is made of several native modules for common tasks such as a tokenization, a part-of-speech tagging or syntactic parsing, all communicating with each other
through XML files . In addition, exchange protocols with external tools are easily definable.
MACAON is a fast, modular and open tool, distributed under GNU Public License.

</abstract><introduction>1is a suite of tools developped to process ambiguous input and extend inference of input modules within a global scope.It consists in several modules that perform classicalNLP tasks (tokenization, word recognition, part-ofspeech tagging, lemmatization, morphological analysis, partial or full parsing) on either native textor word lattices. MACAON is distributed underGNU public licence and can be downloaded fromhttp://www.macaon.lif.univ-mrs.fr/.From a general point of view, a MACAON modulecan be seen as an annotation device1 which adds anew level of annotation to its input that generally depends on annotations from preceding modules. Themodules communicate through XML files that allowthe representation different layers of annotation aswell as ambiguities at each layer. Moreover, the initial XML structuring of the processed files (logicalstructuring of a document, information from the Automatic Speech Recognition module . . . ) remainsuntouched by the processing stages.As already mentioned, one of the main characteristics of MACAON is the ability for each moduleto accept ambiguous inputs and produce ambiguousoutputs, in such a way that ambiguities can be resolved at a later stage of processing. The compactrepresentation of ambiguous structures is at the heartof the MACAON exchange format, described in section 2. Furthermore every module can weight thesolutions it produces. such weights can be used torank solutions or limit their number for later stagesMACAONIntroductionThe automatic processing of textual data generatedby NLP software, resulting from Machine Translation, Automatic Speech Recognition or AutomaticText Summarization, raises new challenges for language processing tools. Unlike native texts (textsproduced by humans), this new kind of texts is theresult of imperfect processors and they are madeof several hypotheses, usually weighted with confidence measures. Automatic text production systems can produce these weighted hypotheses as nbest lists, word lattices, or confusion networks. It iscrucial for this space of ambiguous solutions to bekept for later processing since the ambiguities of thelower levels can sometimes be resolved during highlevel processing stages. It is therefore important tobe able to represent this ambiguity.This work has been funded by the French Agence Nationalepour la Recherche, through the projects SEQUOIA (ANR-08EMER-013) and DECODA (2009-CORD-005-01)1Annotation must be taken here in a general sense which includes tagging, segmentation or the construction of more complex objets as syntagmatic or dependencies trees.86Proceedings of the ACL-HLT 2011 System Demonstrations, pages 8691,Portland, Oregon, USA, 21 June 2011. c 2011 Association for Computational Linguisticsof processing.</introduction><corps>2Several processing tools suites alread exist forFrench among which SXPIPE (Sagot and Boullier,2008), OUTILEX (Blanc et al., 2006), NOOJ2 or UNI TEX 3 . A general comparison of MACAON with thesetools is beyond the scope of this paper. Let us justmention that MACAON shares with most of them theuse of finite state machines as core data representation. Some modules are implemented as standardoperations on finite state machines.The MACAON exchange format is based on four concepts: segment, attribute, annotation level and segmentation.A segment refers to a segment of the text orspeech signal that is to be processed, as a sentence,a clause, a syntactic constituent, a lexical unit, anamed entity . . . A segment can be equipped with attributes that describe some of its aspects. A syntactic constituent, for example, will define the attributetype which specifies its syntactic type (Noun Phrase,Verb Phrase . . . ). A segment is made of one or moresmaller segments.A sequence of segments covering a whole sentence for written text, or a spoken utterance for oraldata, is called a segmentation. Such a sequence canbe weighted.An annotation level groups together segments ofa same type, as well as segmentations defined onthese segments. Four levels are currently defined:pre-lexical, lexical, morpho-syntactic and syntactic.Two relations are defined on segments: the precedence relation that organises linearly segments of agiven level into segmentations and the dominancerelation that describes how a segment is decomposedin smaller segments either of the same level or of alower level.We have represented in figure 2, a schematic representation of the analysis of the reconstructed output a speech recognizer would produce on the input time flies like an arrow11 . Three annotation levels have been represented, lexical, morpho-syntacticand syntactic. Each level is represented by a finitestate automaton which models the precedence relation defined over the segments of this level. Segment time, for example, precedes segment flies. Thesegments are implicitly represented by the labels ofthe automatons arcs. This label should be seen asa reference to a more complex objet, the actual segment. The dominance relations are represented withdashed lines that link segments of different levels.Segment time, for example, is dominated by segment NN of the morpho-syntactic level.This example illustrates the different ambiguitycases and the way they are represented.can also be compared to the numerousdevelopment frameworks for developping processing tools, such as GATE4 , F REE L ING5 , E LLOGON6or L INGPIPE7 that are usually limited to the processing of native texts.MACAONThe MACAON exchange format shares a certain number of features with linguistic annotationscheme standards such as the Text Encoding Initiative8 , XCES9 , or EAGLES10 . They all aim at definingstandards for various types of corpus annotations.The main difference between MACAON and theseapproaches is that MACAON defines an exchange format between NLP modules and not an annotationformat. More precisely, this format is dedicated tothe compact representation of ambiguity: some information represented in the exchange format areto be interpreted by MACAON modules and wouldnot be part of an annotation format. Moreover,the MACAON exchange format was defined from thebottom up, originating from the authors need to useseveral existing tools and adapt their input/outputformats in order for them to be compatible. This is incontrast with a top down approach which is usuallychosen when specifying a standard. Still, MACAONshares several characteristics with the LAF (Ide andRomary, 2004) which aims at defining high levelstandards for exchanging linguistic data.2www.nooj4nlp.net/pages/nooj.htmlwww-igm.univ-mlv.fr/unitex4gate.ac.uk5garraf.epsevg.upc.es/freeling6www.ellogon.org7alias-i.com/lingpipe8www.tei-c.org/P59www.xml-ces.org10www.ilc.cnr.it/eagles/home.html3The MACAON exchange format11For readability reasons, we have used an English example,as mentioned above, currently exists for French.MACAON ,87NPPPNPVPVPNPJJNNVBNN&lt;segment&gt;NPVPVPINVPVBZthyme type indicates the type of the segment, four different types are currently defined: atome (prelexical unit usually referred to as token in english), ulex (lexical unit), cat (part of speech)and chunk (a non recursive syntactic unit).NPDTNNVBVBVBlikeDTanwhich has four mandatory attributes: id associates to a segment a unique identifier inthe document, in order to be able to referenceit.NNarrowfliestimelikenarowFigure 1: Three annotation levels for a sample sentence.Plain lines represent annotation hypotheses within a levelwhile dashed lines represent links between levels. Triangles with the tip up are and nodes and triangles withthe tip down are or nodes. For instance, in the part-ofspeech layer, The first NN can either refer to time orthyme. In the chunking layer, segments that span multiple part-of-speech tags are linked to them through andnodes.The most immediate ambiguity phenomenon isthe segmentation ambiguity: several segmentationsare possible at every level. This ambiguity is represented in a compact way through the factoring ofsegments that participate in different segmentations,by way of a finite state automaton.The second ambiguity phenomenon is the dominance ambiguity, where a segment can be decomposed in several ways into lower level segments.Such a case appears in the preceding example, wherethe NN segment appearing in one of the outgoingtransition of the initial state of the morpho-syntacticlevel dominates both thyme and time segments of thelexical level. The triangle with the tip down is anor node, modeling the fact that NN corresponds totime or thyme.Triangles with the tip up are and nodes. Theymodel the fact that the PP segment of the syntactic level dominates segments IN, DT and NN of themorpho-syntactic level.2.1XML representationThe MACAON exchange format is implemented inXML . A segment is represented with the XML tag88 start and end define the span of the segment.These two attributes are numerical and represent either the index of the first and last character of the segment in the text string or thebeginning and ending time of the segment ina speech signal.A segment can define other attributes that can beuseful for a given description level. We often findthe stype attribute that defines subtypes of a giventype.The dominance relation is represented through theuse of the &lt;sequence&gt; tag. The domination of thethree segments IN, DT and NN by a PP segment,mentionned above is represented below, where p1,p2 and p3 are respectively the ids of segments IN,DT and NN.&lt;segment type="chunk" stype="PP" id="c1"&gt;&lt;sequence&gt;&lt;elt segref="p1"/&gt;&lt;elt segref="p2"/&gt;&lt;elt segref="p3"/&gt;&lt;/sequence&gt;&lt;/segment&gt;The ambiguous case, described above where segment NN dominates segments time or thyme is represented below as a disjunction of sequences insidea segment. The disjunction itself is not representedas an XML tag. l1 and l2 are respectively the idsof segments time and thyme.&lt;segment type="cat" stype="NN" id="c1"&gt;&lt;sequence&gt;&lt;elt segref="l1" w="-3.37"/&gt;&lt;/sequence&gt;&lt;sequence&gt;&lt;elt segref="l2" w="-4.53"/&gt;&lt;/sequence&gt;&lt;/segment&gt;The dominance relation can be weighted, by wayof the attribute w. Such a weight represents in thepreceding example the conditional log-probabilityof a lexical unit given a part of speech, as in a hiddenMarkov model.The precedence relation (i.e. the organizationof segments in segmentations), is represented as aweighted finite state automaton. Automata are represented as a start state, accept states and a list oftransitions between states, as in the following example that corresponds to the lexical level of our example.&lt;fsm n="9"&gt;&lt;start n="0"/&gt;&lt;accept n="6"/&gt;&lt;ltrans&gt;&lt;trans o="0" d="1"&lt;trans o="0" d="1"&lt;trans o="1" d="2"&lt;trans o="2" d="3"&lt;trans o="3" d="4"&lt;trans o="2" d="4"&lt;trans o="4" d="5"&lt;trans o="5" d="6"&lt;trans o="4" d="6"&lt;/ltrans&gt;&lt;/fsm&gt;i="l1"i="l2"i="l3"i="l4"i="l5"i="l6"i="l7"i="l8"i="l9"w="-7.23"/&gt;w="-9.00"/&gt;w="-3.78"/&gt;w="-7.37"/&gt;w="-3.73"/&gt;w="-6.67"/&gt;w="-4.56"/&gt;w="-2.63"/&gt;w="-7.63"/&gt;The &lt;trans/&gt; tag represents a transition, itso,d,i and w features are respectively the origin, anddestination states, its label (the id of a segment) anda weight.An annotation level is represented by the&lt;section&gt; tag which regroups two tags, the&lt;segments&gt; tag that contains the different segmenttags defined at this annotation level and the &lt;fsm&gt;tag that represents all the segmentations of this level.3The MACAON architectureThree aspects have guided the architecture ofMACAON : openness, modularity, and speed. Openness has been achieved by the definition of an exchange format which has been made as general aspossible, in such a way that mapping can be defined from and to third party modules as ASR, MTsystems or parsers. Modularity has been achievedby the definition of independent modules that communicate with each other through XML files usingstandard UNIX pipes. A module can therefore be replaced easily. Speed has been obtained using efficient algorithms and a representation especially de89signed to load linguistic data and models in a fastway.MACAON is composed of libraries and components. Libraries contain either linguistic data, models or API functions. Two kinds of components arepresented, the MACAON core components and thirdparty components for which mappings to and fromthe MACAON exchange format have been defined.3.1LibrariesThe main MACAON library is macaon common.It defines a simple interface to the MACAON exchange format and functions to load XML MACAONfiles into memory using efficient data structures.Other libraries macaon lex, macaon code andmacaon tagger lib represent the lexicon, themorphological data base and the tagger models inmemory.MACAON only relies on two third-party libraries,which are gfsm12 , a finite state machine library andlibxml, an XML library13 .3.2The MACAON core componentsA brief description of several standard componentsdeveloped in the MACAON framework is given below. They all comply with the exchange format described above and add a &lt;macaon stamp&gt; to theXML file that indicates the name of the component,the date and the component version number, and recognizes a set of standard options.maca select is a pre-processing component: it addsa macaon tag under the target tags specified bythe user to the input XML file. The following components will only process the documentparts enclosed in macaon tags.maca segmenter segments a text into sentences byexamining the context of punctuation with aregular grammar given as a finite state automaton. It is disabled for automatic speech transcriptions which do not typically include punctuation signs and come with their own segmentation.12ling.uni-potsdam.de/moocow/projects/gfsm/13xmlsoft.orgmaca tokenizer tokenizes a sentence into prelexical units. It is also based on regular grammars that recognize simple tokens as well as apredefined set of special tokens, such as timeexpressions, numerical expressions, urls. . . .maca lexer allows to regroup pre-lexical units intolexical units. It is based on the lefff French lexicon (Sagot et al., 2006) which contains around500,000 forms. It implements a dynamic programming algorithm that builds all the possiblegrouping of pre-lexical units into lexical units.maca tagger associates to every lexical unit one ormore part-of-speech labels. It is based on atrigram Hidden Markov Model trained on theFrench Treebank (Abeille et al., 2003). The estimation of the HMM parameters has been realized by the SRILM toolkit (Stolcke, 2002).maca anamorph produces the morphological analysis of lexical units associated to a part ofspeech. The morphological information comefrom the lefff lexicon.maca chunker gathers sequences of part-of-speechtags in non recursive syntactic units. This component implements a cascade of finite statetransducers, as proposed by Abney (1996). Itadds some features to the initial Abney proposal, like the possibility to define the head ofa chunk.maca conv is a set of converters from and to theMACAON exchange format.htk2macaonand fsm2macaon convert word lattices fromthe HTK format (Young, 1994) and ATTFSM format (Mohri et al., 2000) to theMACAON exchange format. macaon2txt andtxt2macaon convert from and to plain textfiles. macaon2lorg and lorg2macaonconvert to and from the format of the LORGparser (see section 3.3).maca view is a graphical interface that allows to inspect MACAON XML files and run the components.3.3sentation of many NLP tools input and output in theMACAON format. MACAON has been interfaced withthe SPEERAL Automatic Speech Recognition System (Nocera et al., 2006). The word lattices produced by SPEERAL can be converted to pre-lexicalMACAON automata.MACAON does not provide any native module forparsing yet but it can be interfaced with any alreadyexisting parser. For the purpose of this demonstration we have chosen the LORG parser developed atNCLT, Dublin14 . This parser is based on PCFGswith latent annotations (Petrov et al., 2006), a formalism that showed state-of-the-art parsing accuracy for a wide range of languages. In addition it offers a sophisticated handling of unknown words relying on automatically learned morphological clues,especially for French (Attia et al., 2010). Moreover,this parser accepts input that can be tokenized, postagged or pre-bracketed. This possibility allows fordifferent settings when interfacing it with MACAON.4ApplicationsMACAON has been used in several projects, two ofwhich are briefly described here, the D EFINIENSproject and the L UNA project.D EFINIENS (Barque et al., 2010) is a project thataims at structuring the definitions of a large coverageFrench lexicon, the Tresor de la langue francaise.The lexicographic definitions have been processedby MACAON in order to decompose the definitionsinto complex semantico-syntactic units. The dataprocessed is therefore native text that possesses arich XML structure that has to be preserved duringprocessing.L UNA15 is a European project that aims at extracting information from oral data about hotel booking.The word lattices produced by an ASR system havebeen processed by MACAON up to a partial syntacticlevel from which frames are built. More details canbe found in (Bechet and Nasr, 2009). The key aspectof the use of MACAON for the L UNA project is theability to perform the linguistic analyses on the multiple hypotheses produced by the ASR system. It istherefore possible, for a given syntactic analysis, to14Third party componentsis an open architecture and provides a richexchange format which makes possible the repreMACAON90www.computing.dcu.ie/lorg.This softwareshould be freely available for academic research by the timeof the conference.15www.ist-luna.euFigure 2: Screenshot of the MACAON visualization interface (for French models). It allows to input a text and seethe n-best results of the annotation.find all the word sequences that are compatible withthis analysis.Figure 2 shows the interface that can be used tosee the output of the pipeline.5</corps><conclusion>ConclusionIn this paper we have presented MACAON, an NLPtool suite which allows to process native text as wellas several hypotheses automatically produced by anASR or an MT system. Several evolutions are currently under development, such as a named entityrecognizer component and an interface with a dependency parser.ReferencesAnne Abeille, Lionel Clement, and Francois Toussenel.2003. Building a treebank for french. In AnneAbeille, editor, Treebanks. Kluwer, Dordrecht.Steven Abney. 1996. Partial parsing via finite-state cascades. In Workshop on Robust Parsing, 8th EuropeanSummer School in Logic, Language and Information,Prague, Czech Republic, pages 815.M. Attia, J. Foster, D. Hogan, J. Le Roux, L. Tounsi, andJ. van Genabith. 2010. Handling Unknown Words inStatistical Latent-Variable Parsing Models for Arabic,English and French. In Proceedings of SPMRL.Lucie Barque, Alexis Nasr, and Alain Polguere. 2010.From the definitions of the tresor de la langue francaiseto a semantic database of the french language. In EURALEX 2010, Leeuwarden, Pays Bas.91Frederic Bechet and Alexis Nasr. 2009. Robust dependency parsing for spoken language understanding ofspontaneous speech. In Interspeech, Brighton, UnitedKingdom.Olivier Blanc, Matthieu Constant, and Eric Laporte.2006. Outilex, plate-forme logicielle de traitement detextes ecrits. In TALN 2006, Leuven.Nancy Ide and Laurent Romary. 2004. Internationalstandard for a linguistic annotation framework. Natural language engineering, 10(3-4):211225.M. Mohri, F. Pereira, and M. Riley. 2000. The designprinciples of a weighted finite-state transducer library.Theoretical Computer Science, 231(1):1732.P. Nocera, G. Linares, D. Massonie, and L. Lefort. 2006.Phoneme lattice based A* search algorithm for speechrecognition. In Text, Speech and Dialogue, pages 83111. Springer.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein. 2006. Learning Accurate, Compact, and Interpretable Tree Annotation. In ACL.Benot Sagot and Pierre Boullier. 2008. Sxpipe 2:architecture pour le traitement presyntaxique de corpus bruts. Traitement Automatique des Langues,49(2):155188.Benot Sagot, Lionel Clement, Eric Villemonte de laClergerie, and Pierre Boullier. 2006. The lefff 2 Syntactic Lexicon for French: Architecture, Acquisition,Use. In International Conference on Language Resources and Evaluation, Genoa.Andreas Stolcke. 2002. Srilm - an extensible languagemodeling toolkit. In International Conference on Spoken Language Processing, Denver, Colorado.S.J. Young. 1994. The HTK Hidden Markov ModelToolkit: Design and Philosophy. Entropic CambridgeResearch Laboratory, Ltd, 2:244.</conclusion><discussion></discussion><biblio>ReferencesAnne Abeille, Lionel Clement, and Francois Toussenel.2003. Building a treebank for french. In AnneAbeille, editor, Treebanks. Kluwer, Dordrecht.Steven Abney. 1996. Partial parsing via finite-state cascades. In Workshop on Robust Parsing, 8th EuropeanSummer School in Logic, Language and Information,Prague, Czech Republic, pages 815.M. Attia, J. Foster, D. Hogan, J. Le Roux, L. Tounsi, andJ. van Genabith. 2010. Handling Unknown Words inStatistical Latent-Variable Parsing Models for Arabic,English and French. In Proceedings of SPMRL.Lucie Barque, Alexis Nasr, and Alain Polguere. 2010.From the definitions of the tresor de la langue francaiseto a semantic database of the french language. In EURALEX 2010, Leeuwarden, Pays Bas.91Frederic Bechet and Alexis Nasr. 2009. Robust dependency parsing for spoken language understanding ofspontaneous speech. In Interspeech, Brighton, UnitedKingdom.Olivier Blanc, Matthieu Constant, and Eric Laporte.2006. Outilex, plate-forme logicielle de traitement detextes ecrits. In TALN 2006, Leuven.Nancy Ide and Laurent Romary. 2004. Internationalstandard for a linguistic annotation framework. Natural language engineering, 10(3-4):211225.M. Mohri, F. Pereira, and M. Riley. 2000. The designprinciples of a weighted finite-state transducer library.Theoretical Computer Science, 231(1):1732.P. Nocera, G. Linares, D. Massonie, and L. Lefort. 2006.Phoneme lattice based A* search algorithm for speechrecognition. In Text, Speech and Dialogue, pages 83111. Springer.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein. 2006. Learning Accurate, Compact, and Interpretable Tree Annotation. In ACL.Benot Sagot and Pierre Boullier. 2008. Sxpipe 2:architecture pour le traitement presyntaxique de corpus bruts. Traitement Automatique des Langues,49(2):155188.Benot Sagot, Lionel Clement, Eric Villemonte de laClergerie, and Pierre Boullier. 2006. The lefff 2 Syntactic Lexicon for French: Architecture, Acquisition,Use. In International Conference on Language Resources and Evaluation, Genoa.Andreas Stolcke. 2002. Srilm - an extensible languagemodeling toolkit. In International Conference on Spoken Language Processing, Denver, Colorado.S.J. Young. 1994. The HTK Hidden Markov ModelToolkit: Design and Philosophy. Entropic CambridgeResearch Laboratory, Ltd, 2:244.</biblio></article></xml>