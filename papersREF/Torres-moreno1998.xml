<xml><article><preambule>Torres-moreno1998.pdf</preambule><titre>LETTER
</titre><auteur>J. Manuel Torres Moreno
</auteur><abstract></abstract><introduction></introduction><corps>2 The Incremental Learning Strategy2.1 Definitions. We are given a training set of P input-output examples {E  ,   }, where  = 1, 2, . . . , P. The inputs E  = (1, 1 , 2 , . . . , N ) may bebinary or real valued N+1 dimensional vectors. The first component 0  1,the same for all the patterns, allows us to treat the bias as a supplementaryweight. The outputs are binary,   = 1. These patterns are used to learnthe classification task with the growth algorithm. Assume that, at a givenstage of the learning process, the network already has h binary neurons1010J. Manuel Torres Moreno and Mirta B. Gordonin the hidden layer. These neurons are connected to the N + 1 input unitsE k = (wk0 , wk1    wkN ), 1  k  h, wk0 being thethrough synaptic weights wbias.Then, given an input pattern E , the states k of the hidden neurons (1 k  h) given by!NXE k  E )wki i  sign(w(2.1)k = signi=0define the patterns h-dimensional IR, E (h) = (1, 1 , . . . , h ). The networksoutput  (h) is:!hhiXEWk k  sign W(h) E (h)(2.2) (h) = signk=0Ewhere W(h)= (W0 , W1 , . . . , Wh are the output unit weights. Hereafter,E (h) = (1, 1 , . . . , h ) is the h-dimensional IR associated by the networkof h hidden units to pattern E  . During the training process, h increasesthrough the addition of hidden neurons, and we denote the final numberof hidden units as H.2.2 Example. We first describe the general strategy on a schematic example (see Figure 1). Patterns in the gray region belong to class  = +1, theothers to  = 1. The algorithm proceeds as follows. A first hidden unitis trained to separate the input patterns at best and finds one solution, sayE 1 , represented on Figure 1 by the line labeled 1, with the arrow pointingwinto the positive half-space. Because training errors remain, a second hidden neuron is introduced. It is trained to learn targets 2 = +1 for patternswell classified by the first neuron and 2 = 1 for the others (the oppositeconvention could be adopted, both being strictly equivalent), and supposeE 2 is found. Then an output unit is connected to the two hidthat solution wden neurons and is trained with the original targets. Clearly it will fail toseparate all the patterns correctly because the IR (1, 1) and (+) are notfaithful, as patterns of both classes are mapped onto them. The output neuron is dropped, and a third hidden unit is appended and trained with targets3 = +1 for patterns that were correctly classified by the output neuron andE 3 is found, and it is easy to see that now3 = 1 for the others. Solution wthe IRs are faithful, that is, patterns belonging to different classes are givendifferent IRs. The algorithm converged with three hidden units that definethree domain boundaries determining six regions or domains in the inputspace. It is straightforward to verify that the IRs corresponding to each domain on Figure 1 are linearly separable. Thus, the output unit will find thecorrect solution to the training problem. If the faithful IRs were not linearlyseparable, the output unit would not find a solution without training errors,and the algorithm would go on appending hidden units that should learnClassification Tasks with Binary Units10113-++-+++-1+++2+-+-+Figure 1: Patterns inside the gray region belong to one class, those in the whiteregion to the other. The lines (labeled 1, 2, and 3) represent the hyperplanes foundwith the NetLines strategy. The arrows point into the correspondent positivehalf-spaces. The IRs of each domain are indicated (the first component, 0 = 1,is omitted for clarity).targets  = 1 for well-learned patterns, and  = 1 for the others. A proofthat a solution to this strategy with a finite number of hidden units exists isleft to the appendix.2.3 The Algorithm NetLines. Like most other adaptive learning algorithms, NetLines combines a growth heuristics with a particular learningalgorithm for training the individual units, which are simple perceptrons.In this section, we present the growth heuristics first, followed by the description of Minimerror, our perceptron learning algorithm.We first introduce the following useful remark: if a neuron has to learn atarget  , and the learned state turns out to be  , then the product   = 1 ifthe target has been correctly learned, and   = 1 otherwise.Given a maximal accepted number of hidden units, Hmax , and a maximalnumber of tolerated training errors, Emax , the Netlines algorithm may besummarized as follows:Algorithm. Initializeh = 0;set the targets h+1 =   for  = 1, . . . , P;1012J. Manuel Torres Moreno and Mirta B. Gordon Repeat1. /* train the hidden units */h = h + 1; /* connect hidden unit h to the inputs */learn the training set {E  , h },  = 1, . . . , P;E h  E  ),  = 1, . . . , P;after learning, h = sign(wif h = 1 /* for the first hidden neuron */if 1 = 1  then stop. /* the training set is LS */;else set h+1 = h   for  = 1, . . . , P; go to 1;end if2. /* learn the mapping between the IRs and the outputs */connect the output neuron to the h trained hidden units;learn the training set {E  (h),   };  = 1, . . . , P;E E ,  = 1, . . . , P;after learning,  (h) = sign W(h)set h+1 =     for  = 1, . . . , P;Pcount the number of training errors e =  (1  h+1 )/2; Until (h = Hmax or e  Emax );The generated network has H = h hidden units. In the appendix we presenta solution to the learning strategy with a bounded number of hidden units.In practice, the algorithm ends up with much smaller networks than thisupper bound, as will be shown in section 5.2.4 The Perceptron Learning Algorithm. The final number of hiddenneurons, which are simple perceptrons, depends on the performance of thelearning algorithm used to train them. The best solution should minimizethe number of errors. If the training set is LS, it should endow the units withthe lowest generalization error. Our incremental algorithm uses Minimerror(Gordon &amp; Berchier, 1993) to train the hidden and output units. Minimerror is based on the minimization of a cost function E that depends on theE through the stabilities of the training patterns. If theperceptron weights winput vector is E  and   the corresponding target, then the stability   ofpattern  is a continuous and derivable function of the weights, given by:  = E  E w,Ekwk(2.3)E  w.E The stability is independent of the norm of the weightsE = wwhere kwkE It measures the distance of the pattern to the separating hyperplane,kwk.E it is positive if the pattern is well classified, negativewhich is normal to w;Classification Tasks with Binary Units1013otherwise. The cost function E is:E=P 1X.1  tanh2 =12T(2.4)The contribution to E of patterns with large negative stabilities is ' 1, thatis, they are counted as errors, whereas the contribution of patterns withlarge, positive stabilities is vanishingly small. Patterns at both sides of thehyperplane within a window of width  4T contribute to the cost functioneven if they have positive stability.The properties of the global minimum of equation 2.4 have been studiedtheoretically with methods of statistical mechanics (Gordon &amp; Grempel,1995). It was shown that in the limit T  0, the minimum of E correspondsto the weights that minimize the number of training errors. If the trainingset is LS, these weights are not unique (Gyorgyi &amp; Tishby, 1990). In that case,there is an optimal learning temperature such that the weights minimizingE at that temperature endow the perceptron with a generalization errornumerically indistinguishable from the optimal (Bayesian) value.The algorithm Minimerror (Gordon &amp; Berchier, 1993; Raffin &amp; Gordon,1995) implements a minimization of E restricted to a subspace of normalizedweights, through a gradient descent combined with a slow decrease of thetemperature T, which is equivalent to a deterministic annealing. It has beenshown that the convergence is faster if patterns with negative stabilities areconsidered at a temperature T larger than those with positive stabilities,T+ , with a constant ratio  = T /T+ . The weights and the temperatures areiteratively updated through:#"XX  E   E E =+(2.5) w(t)2 2 /  0 cosh ( /2T )/  &gt;0 cosh ( /2T+ )11(t + 1) = T+(t) + T1 ; T =  T+ ;T+pE +  w(t)Ew(t)E + 1) = N + 1.w(tEEkw(t) +  w(t)k(2.6)(2.7)Notice from equation 2.5 that only the incorrectly learned patterns at distances shorter than  2T from the hyperplane, and those correctly learnedlying closer than  2T+ , contribute effectively to learning. The contribution of patterns outside this region is vanishingly small. By decreasing thetemperature, the algorithm selects to learn patterns increasingly localizedin the neighborhood of the hyperplane, allowing for a highly precise determination of the parameters defining the hyperplane, which are the neurons weights.Normalization 2.7 restricts the search to the subspace withE = N + 1.kwkThe only adjustable parameters of the algorithm are the temperature ratio = T /T+ , the learning rate , and the annealing rate T1 . In principle,1014J. Manuel Torres Moreno and Mirta B. Gordonthey should be adapted to eachspecific problem. However, as a result ofour normalizing the weights to N + 1 and to data standardization (see thenext section), all the problems are brought to the same scale, simplifying thechoice of the parameters.2.5 Data Standardization. Instead of determining the best parametersfor each new problem, we standardize the input patterns of the training setthrough a linear transformation, applied to each component:i =i  hi i; 1  i  N.1i(2.8)The mean hi i and the variance 42i , defined as usual,hi i =P1XiP =11i 2 =PP1X1X2(i  hi i) =(i )2  (hi i)2 ,P =1P =1(2.9)(2.10)need only a single pass of the P training patterns to be determined. Afterlearning, the inverse transformation is applied to the weights,w0 =wi =pN + 1 rhpN + 1 rhw0 NPwi hi i/1ii=1w0 PNj=1 wj hj i/1ji2+PNj=1 (wj /1j(2.11))2wi /1i,i2 PPNN2w0  j=1 wj hj i/1j + j=1 (wj /1j )(2.12)so that the normalization (see equation 2.8) is completely transparent to theuser: with the transformed weights (see equations 2.11 and 2.12), the neuralclassifier is applied to the data in the original users units, which do notneed to be renormalized.As a consequence of the weights scaling (see equation 2.7) and the inputs standardization (see equation 2.8), all the problems are automaticallyrescaled. This allows us to use always the same values of Minimerrors parameters: the standard values  = 0.02, T1 = 103 , and  = 6. They wereused throughout this article, the reported results being highly insensitive toslight variations of them. However, in some extremely difficult cases, likelearning the parity in dimensions N &gt; 10 and finding the separation of thesonar signals (see section 5), larger values of  were needed.Classification Tasks with Binary Units10152.6 Interpretation. It has been shown (Gordon, Peretto, &amp; Berchier, 1993)that the contribution of each pattern to the cost function of Minimerror,[1  tanh(  /2T)]/2, may be interpreted as the probability of misclassification at the temperature T at which the minimum of the cost function hasbeen determined. By analogy, the neurons prediction on a new input E maybe given a confidence measure by replacing the (unknown) pattern stabilE  E k/kwk,E which is its distance to theity by its absolute value k k = kwhyperplane. This interpretation of the sigmoidal function tanh(k k/2T) asthe confidence on the neurons output is similar to the one proposed earlier(Goodman, Smyth, Higgins, &amp; Miller, 1992) within an approach based oninformation theory.The generalization of these ideas to multilayered networks is not straightforward. An estimate of the confidence on the classification by the outputneuron should include the magnitude of the weighted sums of the hiddenneurons, as they measure the distances of the input pattern to the domainboundaries. However, short distances to the separating hyperplanes are notalways correlated to low confidence on the networks output. For an example, we refer again to Figure 1. Consider a pattern lying close to hyperplane1. A small, weighted sum on neuron 1 may cast doubt on the classificationif the patterns IR is ( + +) but not if it is ( + ), because a change of thesign of the weighted sum in the latter case will map the pattern to the IR(+ + ) which, being another IR of the same class, will be given the sameoutput by the network. It is worth noting that the same difficulty is met bythe interpretation of the outputs of multilayered perceptrons, trained withbackpropagation, as posterior probabilities. We do not explore this problemany further because it is beyond the scope of this article.3 Comparison with Other StrategiesThere are few learning algorithms for neural networks composed of binaryunits. To our knowledge, all of them are incremental. In this section, wegive a short overview of some of them, in order to put forward the maindifferences with NetLines. We discuss the growth heuristics and then theindividual unit training algorithms.The Tiling algorithm (Mezard &amp; Nadal, 1989) introduces hidden layers,one after the other. The first neuron of each layer is trained to learn an IR thathelps to decrease the number of training errors; supplementary hidden unitsare then appended to the layer until the IRs of all the patterns in the training set are faithful. This procedure may generate very large networks. TheUpstart algorithm (Frean, 1990) introduces successive couples of daughterhidden units between the input layer and the previously included hiddenunits, which become their parents. The daughters are trained to correctthe parents classification errors, one daughter for each class. The obtainednetwork has a treelike architecture. There are two different algorithms implementing the Tilinglike Learning in the Parity Machine (Biehl &amp; Opper,1016J. Manuel Torres Moreno and Mirta B. Gordon1991), Offset (Martinez &amp; Esteve, 1992), and MonoPlane (Torres Moreno &amp;Gordon, 1995). In both, each appended unit is trained to correct the errorsof the previously included unit in the same hidden layer, a procedure thathas been shown to generate a parity machine: the class of the input patternsis the parity of the learned IRs. Unlike Offset, which implements the paritythrough a second hidden layer that needs to be pruned, MonoPlane goeson adding hidden units (if necessary) in the same hidden layer until thenumber of training errors at the output vanishes. Convergence proofs forbinary input patterns have been produced for all these algorithms. In thecase of real-valued input patterns, a solution to the parity machine with abounded number of hidden units also exists (Gordon, 1996).The rationale behind the construction of the parity machine is that itis not worth training the output unit before all the training errors of thehidden units have been corrected. However, Marchand, Golea, and Rujan(1990) pointed out that it is not necessary to correct all the errors of thesuccessively trained hidden units. It is sufficient that the IRs be faithful andLS. If the output unit is trained immediately after each appended hiddenunit, the network may discover that the IRs are already faithful and stopadding units. This may be seen in Figure 1. None of the parity machineimplementations would find the solution represented on the figure, becauseeach of the three perceptrons systematically unlearns part of the patternslearned by the preceding one.To our knowledge, Sequential Learning (Marchand et al., 1990) is theonly incremental learning algorithm that might find a solution equivalent(although not the same) to the one of Figure 1. In this algorithm, the firstunit is trained to separate the training set keeping one pure half-spacecontaining patterns of only one class. Wrongly classified patterns, if any,must all lie in the other half-space. Each appended neuron is trained toseparate wrongly classified patterns with this constraint of always keepingone pure, error-free half-space. Thus, neurons must be appended in a preciseorder, making the algorithm difficult to implement in practice. For example,Sequential Learning applied to the problem of Figure 1 needs to impose thatE 3 , the only solution satisfying the puritythe first unit finds the weights wrestriction.Other proposed incremental learning algorithms strive to solve the problem with different architectures, and/or with real valued units. For example,in the algorithm Cascade Correlation (Fahlman &amp; Lebiere, 1990), each appended unit is selected among a pool of several real-valued neurons, trainedto learn the correlation between the targets and the training errors. The unitis then connected to the input units and to all the other hidden neuronsalready included in the network.Another approach to learning classification tasks is through the construction of decision trees (Breiman, Friedman, Olshen, &amp; Stone, 1984), which hierarchically partition the input space through successive dichotomies. Theneural networks implementations generate treelike architectures. Each neu-Classification Tasks with Binary Units1017ron of the tree introduces a dichotomy of the input space, which is treatedseparately by the children nodes, which eventually produce new splits. Besides the weights, the resulting networks need to store the decision path.The proposed heuristics (Sirat &amp; Nadal, 1990; Farrell &amp; Mammone, 1994;Knerr, Personnaz, &amp; Dreyfus, 1990) differ in the algorithm used to train eachnode and/or in the stopping criterion. In particular, Neural-Trees (Sirat &amp;Nadal, 1990) may be regarded as a generalization of Classification and Regression Trees (CART) (Breiman et al., 1984) in which the hyperplanes arenot constrained to be perpendicular to the coordinate axis. The heuristics ofthe Modified Neural Tree Network (MNTN) (Farrell &amp; Mammone, 1994),similar to Neural-Trees, includes a criterion of early stopping based on aconfidence measure of the partition. As NetLines considers the whole inputspace to train each hidden unit, it generates domain boundaries that maygreatly differ from the splits produced by trees. We are not aware of anysystematic study or theoretical comparison of both approaches.Other algorithms, like Restricted Coulomb Energy (RCE) (Reilly, Cooper,&amp; Elbaum, 1982), Grow and Learn (GAL) (Alpaydin, 1990), Glocal (Depenau, 1995), and Growing Cells (Fritzke, 1994), propose to cover or mask theinput space with hyperspheres of adaptive size containing patterns of thesame class. These approaches generally end up with a very large number ofunits. Covering Regions by the LP Method (Mukhopadhyay, Roy, Kim, &amp;Govil, 1993) is a trial-and-error procedure devised to select the most efficientmasks among hyperplanes, hyperspheres, and hyperellipsoids. The masksparameters are determined through linear programming.Many incremental strategies use the Pocket algorithm (Gallant, 1986)to train the appended units. Its main drawback is that it has no naturalstopping condition, which is left to the users patience. The proposed alternative algorithms (Frean, 1992; Bottou &amp; Vapnik, 1992) are not guaranteedto find the best solution to the problem of learning. The algorithm used bythe MNTN (Farrell &amp; Mammone, 1994) and the ITRULE (Goodman et al.,1992) minimize cost functions similar to equation 2.4, but using differentmisclassification measures at the place of our stability (see equation 2.3).The essential difference with Minimerror is that none of these algorithms isable to control which patterns contribute to learning, as Minimerror doeswith the temperature.4 Generalization to Multiclass ProblemsThe usual way to cope with problems having more than two classes is togenerate as many networks as classes. Each network is trained to separatepatterns of one class from all the others, and a winner-takes-all (WTA) strategy based on the value of the outputs weighted sum in equation 2.2 is usedto decide the class if more than one network recognizes the input pattern. Inour case, because we use normalized weights, the outputs weighted sumis merely the distance of the IR to the separating hyperplane. All the pat-1018J. Manuel Torres Moreno and Mirta B. Gordonterns mapped to the same IR are given the same outputs weighted sum,independent of the relative position of the pattern in input space. A strongweighted sum on the output neuron is not inconsistent with small weightedsums on the hidden neurons. Therefore, a naive WTA decision may not givegood results, as shown in the example in section 5.3.1.We now describe an implementation for the multiclass problem that results in a treelike architecture of networks. It is more involved than the naiveWTA and may be applied to any binary classifier. Suppose that we have aproblem with C classes. We must choose in which order the classes willbe learned, say (c1 , c2 , . . . , cC ). This order constitutes a particular learningsequence. Given a particular learning sequence, a first network is trainedto separate class c1 , which is given output target 1 = +1, from the others(which are given targets 1 = 1). The opposite convention is equivalentand could equally be used. After training, all the patterns of class c1 areeliminated from the training set, and we generate a second network trainedto separate patterns of class c2 from the remaining classes. The procedure,reiterated with training sets of decreasing size, generates C  1 hierarchically organized tree of networks (TON): the outputs are ordered sequencesE = (1 , 2 , . . . , C1 ). The predicted class of a pattern is ci , where i is thefirst network in the sequence having an output +1 (i = +1 and j = 1 forj &lt; i), the outputs of the networks with j &gt; i being irrelevant.The performance of the TON may depend on the chosen learning sequence. Therefore, it is convenient that an odd number of TONs, trainedwith different learning sequences, compete through a vote. We verified empirically, as is shown in section 5.3, that this vote improves the results obtained with each of the individual TONs participating in the vote. Noticethat our procedure is different from bagging (Breiman, 1994); all the networks of the TON are trained with the same training set, without the needof any resampling procedure.5 ApplicationsAlthough convergence proofs of learning algorithms are satisfactory on theoretical grounds, they are not a guarantee of good generalization. In fact,they demonstrate only that correct learning is possible; they do not addressthe problem of generalization. This last issue still remains quite empirical(Vapnik, 1992; Geman et al., 1992; Friedman, 1996), and the generalizationperformance of learning algorithms is usually tested on well-known benchmarks (Prechelt, 1994).We first tested the algorithm on learning the parity function of N bits for2  N  11. It is well known that the smallest network with the architectureconsidered here needs H = N hidden neurons. The optimal architecturewas found in all the cases. Although this is quite an unusual performance,the parity is not a representative problem: learning is exhaustive, and generalization cannot be tested. Another test, the classification of sonar signalsClassification Tasks with Binary Units1019(Gorman &amp; Sejnowski, 1988), revealed the quality of Minimerror, as it solvedthe problem without hidden units. In fact, we found that not only the training set of this benchmark is linearly separable, a result already reported(Hoehfeld &amp; Fahlman, 1991; Roy, Kim, &amp; Mukhopadhyay, 1993), but thatthe complete databasethe training and the test sets togetheris also linearly separable (Torres Moreno &amp; Gordon, 1998).We next present our results, generalization error g and number of weights,on several benchmarks corresponding to different kinds of problems: binaryclassification of binary input patterns, binary classification of real-valuedinput patterns, and multiclass problems. These benchmarks were chosenbecause they have already served as a test for many other algorithms, providing us with unbiased results for comparison. The generalization errorg of NetLines was estimated as usual, through the fraction of misclassifiedpatterns on a test set of data.The results are reported as a function of the training sets sizes P wheneverthese sizes are not specified by the benchmark. Besides the generalizationerror g , averaged over a (specified) number of classifiers trained with randomly selected training sets, we also present the number of weights of thecorresponding networks which is a measure of the classifiers complexity,as it corresponds to the number of its parameters.Training times are usually cited among the characteristics of the trainingalgorithms. Only the numbers of epochs used by backpropagation on twoof the studied benchmarks have been published; we restrict the comparisonto these cases. As NetLines updates only N weights per epoch, whereasbackpropagation updates all the networks weights, we compare the totalnumber of weights updates. They are of the same order of magnitude forboth algorithms. However, these comparisons should be taken with caution. NetLines is a deterministic algorithm; it learns the architecture andthe weights through a single run, whereas with backpropagation severalarchitectures must be previously investigated, and this time is not includedin the training time.The following notation is used: D is the total number of available patterns,P the number of training patterns, and G the number of test patterns.5.1 Binary Inputs. The case of binary input patterns has the property,not shared by real-valued inputs, that every pattern may be separated fromthe others by a single hyperplane. This solution, usually called grandmother,needs as many hidden units as patterns in the training set. In fact, the convergence proofs for incremental algorithms in the case of binary input patternsare based on this property.5.1.1 Monks Problem. This benchmark, thoroughly studied with manydifferent learning algorithms (Trhun et al., 1991), contains three distinctproblems. Each has an underlying logical proposition that depends on sixdiscrete variables, coded with N = 17 binary numbers. The total number of1020J. Manuel Torres Moreno and Mirta B. Gordonpossible input patterns is D = 432, and the targets correspond to the truth table of the corresponding proposition. Both NetLines and MonoPlane foundthe underlying logical proposition of the first two problems; they generalized correctly, giving g = 0. In fact, these are easy problems: all the neuralnetworkbased algorithms, and some nonneural learning algorithms werereported to generalize them correctly. In the third Monks problem, 6 patterns among the P3 = 122 examples are given wrong targets. The generalization error is calculated over the complete set of D = 432 patterns, that is,including the training patterns, but in the test set all the patterns are giventhe correct targets. Thus, any training method that learns the training setcorrectly will make at least 1.4% of generalization errors. Four algorithmsspecially adapted to noisy problems were reported to reach g = 0. However,none of them generalizes correctly the two other (noiseless) Monks problems. Besides them, the best performance, g = 0.0277, which correspondsto 12 misclassified patterns, is reached only by neural networks methods:backpropagation, backpropagation with weight decay, cascade correlation,and NetLines. The number of hidden units generated with NetLines (58weights) is intermediate between backpropagation with weight decay (39)and cascade correlation (75) or backpropagation (77). MonoPlane reached aslightly worse performance (g = 0.0416, or 18 misclassified patterns) withthe same number of weights as NetLines, showing that the parity machineencoding may not be optimal.5.1.2 Two or More Clumps. In this problem (Denker et al., 1987) the network has to discriminate if the number of clumps in a ring of N bits is strictlysmaller than 2 or not. One clump is a sequence of identical bits bounded bybits of the other kind. The patterns are generated through a Monte Carlomethod in which the mean number of clumps is controlled by a parameterk (Mezard &amp; Nadal, 1989). We generated training sets of P patterns withk = 3, corresponding to a mean number of clumps of  1.5, for rings ofN = 10 and N = 25 bits. The generalization error corresponding to several learning algorithms, estimated with independently generated testingsets of the same sizes as the training sets, G = P, are displayed in Figure 2as a function of P. Points with error bars correspond to averages over 25independent training sets. Points without error bars correspond to best results. NetLines, MonoPlane, and Upstart for N = 25 have nearly the sameperformances when trained to reach error-free learning.We tested the effect of early stopping by imposing on NetLines a maximalnumber of two hidden units (H = 2). The residual training error t is plottedon Figure 2, as a function of P. Note that early stopping does not help to decrease g . Overfitting, which arises when NetLines is applied until error-freetraining is reached, does not degrade the networks generalization performance. This behavior is very different from the one of networks trainedwith backpropagation. The latter reduces classification learning to a regression problem, in which the generalization error can be decomposed in twoClassification Tasks with Binary Units1021 RU PRUH FOXPSV1  RU PRUH FOXPSV1 7LOLQJ*URZWK%DFNSURS6WHSZLVHJ0RQR3ODQH1HW/LQHV + 1HW/LQHV8SVWDUW1HW/LQHV1HW/LQHVW33Figure 2: Two or more clumps for two ring sizes, N = 10 and N = 25. Generalization error g versus size of the training set P, for different algorithms.N = 10: backpropagation (Solla, 1989), Stepwise (Knerr et al., 1990). N = 25:Tiling (Mezard &amp; Nadal, 1989), Upstart (Frean, 1990). Results with the GrowthAlgorithm (Nadal, 1989) are indistinguishable from those of Tiling at the scaleof the figure. Points without error bars correspond to best results. Results ofMonoPlane and NetLines are averages over 25 tests.competing terms: bias and variance. With backpropagation, early stoppinghelps to decrease overfitting because some hidden neurons do not reachlarge enough weights to work in the nonlinear part of the sigmoidal transfer functions. All the neurons working in the linear part may be replaced bya single linear unit. Thus, with early stopping, the network is equivalent toa smaller one with all the units working in the nonlinear regime. Our resultsare consistent with recent theories (Friedman, 1996) showing that, contraryto regression, the bias and variance components of the generalization errorin classification combine in a highly nonlinear way.The number of weights used by the different algorithms is plotted on alogarithmic scale as a function of P in Figure 3. It turns out that the strategyof NetLines is slightly better than that of MonoPlane with respect to bothgeneralization performance and network size.5.2 Real Valued Inputs. We tested NetLines on two problems that havereal valued inputs (we include graded-valued inputs here).5.2.1 Wisconsin Breast Cancer Database. The input patterns of this benchmark (Wolberg &amp; Mangasarian, 1990) have N = 9 attributes characterizing1022J. Manuel Torres Moreno and Mirta B. Gordon1XPEHU RI ZHLJKWV RU PRUH FOXPSV1 %DFNSURSDJDWLRQ RU PRUH FOXPSV1 6WHSZLVH0RQR3ODQH1HW/LQHV8SVWDUW1HW/LQHV33Figure 3: Two or more clumps. Number of weights (logarithmic scale) versussize of the training set P, for N = 10 and N = 25. Results of MonoPlane andNetLines are averages over 25 tests. The references are the same as in Figure 2.samples of breast cytology, classified as benign or malignant. We excludedfrom the original database 16 patterns that have the attribute 6 (bare nuclei) missing. Among the remaining D = 683 patterns, the two classes areunevenly represented, 65.5% of the examples being benign. We studied thegeneralization performance of networks trained with sets of several sizes P.The P patterns for each learning test were selected at random. In Figure 4a,the generalization error at classifying the remaining G  D  P patterns isdisplayed as a function of the corresponding number of weights in a logarithmic scale. For comparison, we included in the same figure results of asingle perceptron trained with P = 75 patterns using Minimerror. The results, averaged values over 50 independent tests for each P, show that bothNetLines and MonoPlane have lower g and fewer parameters than otheralgorithms on this benchmark.The total number of weights updates needed by NetLines, including theweights of the dropped output units, is 7  104 ; backpropagation needed 104 (Prechelt, 1994).The trained network may be used to classify the patterns with missingattributes. The number of misclassified patterns among the 16 cases forwhich attribute 6 is missing is plotted as a function of the possible valuesof 6 on Figure 4b. For large values of 6 , there are discrepancies between themedical and the networks diagnosis on half the cases. This is an exampleof the kind of information that may be obtained in practical applications.Classification Tasks with Binary Units1023%UHDVW FDQFHU D%UHDVW FDQFHU E0LQLPHUURU 3 0RQR3ODQH1HW/LQHV 3 1HW/LQHVJ0RQR3ODQH 3 1HW/LQHV0RQR3ODQH1XPEHU RI ZHLJKWV3RVVLEOH YDOXHV RI DWWULEXWHFigure 4: Breast cancer classification. (a) Generalization error g versus number of weights (logarithmic scale), for P = 525. 13: Rprop with no shortcuts(Prechelt, 1994); 46: Rprop with shortcuts (Prechelt, 1994); 7: Cascade Correlation (Depenau, 1995). For comparison, results with smaller training sets, P = 75(single perceptron) and P = 160, are displayed. Results of MonoPlane and NetLines are averages over 50 tests. (b) Classification errors versus possible valuesof the missing attribute bare nuclei for the 16 incomplete patterns, averagedover 50 independently trained networks.5.2.2 Diabetes Diagnosis. This benchmark (Prechelt, 1994) contains D =768 patterns described by N = 8 real-valued attributes, corresponding to 35% of Pima women suffering from diabetes, 65% being healthy. Trainingsets of P = 576 patterns were selected at random, and generalization wastested on the remaining G = 192 patterns. The comparison with publishedresults obtained with other algorithms tested under the same conditions,presented in Figure 5, shows that NetLines reaches the best performancepublished so far on this benchmark, needing many fewer parameters. Training times of NetLines are of  105 updates. The numbers of updates neededby Rprop (Prechelt, 1994) range between 4  103 and 5  105 , depending onthe networks architecture.5.3 Multiclass Problems. We applied our learning algorithm to two different problems, both of three classes. We compare the results obtained witha WTA classification based on the results of three networks, each independently trained to separate one class from the two others, to the results ofthe TON architectures described in section 4. Because the number of classesis low, we determined the three TONs, corresponding to the three possible1024J. Manuel Torres Moreno and Mirta B. Gordon,QGLDQV 3LPD 'LDEHWHVJ1HW/LQHV1XPEHU RI ZHLJKWVFigure 5: Diabetes diagnosis: Generalization error g versus number of weights.Results of NetLines are averages over 50 tests. 13: Rprop no shortcuts, 46:Rprop with shortcuts (Prechelt, 1994).learning sequences. The vote of the three TONs improves the performances,as expected.5.3.1 Breimans Waveform Recognition Problem. This problem was introduced as a test for the algorithm CART (Breiman et al., 1984). The inputpatterns are defined by N = 21 real-valued amplitudes x(t) observed at regularly spaced intervals t = 1, 2, . . . , N. Each pattern is a noisy convex linearcombination of two among three elementary waves (triangular waves centered on three different values of t). There are three possible combinations,and the patterns class identifies from which combination it is issued.We trained the networks with the same 11 training sets of P = 300 examples, and generalization was tested on the same independent test setof G = 5000, as in Gascuel (1995). Our results are displayed in Figure 6,where only results of algorithms reaching g &lt; 0.25 in Gascuel (1995) areincluded. Although it is known that due to the noise, the classification errorhas a lower bound of  14% (Breiman et al., 1984), the results of NetLinesand MonoPlane presented here correspond to error-free training. The networks generated by NetLines have between three and six hidden neurons,depending on the training sets. The results obtained with a single perceptron trained with Minimerror and with the perceptron learning algorithm,which may be considered the extreme case of early stopping, are hardly improved by the more complex networks. Here again the overfitting producedby error-free learning with NetLines does not cause the generalization per-Classification Tasks with Binary Units1025%UHLPDQ V :DYHIRUPV0RQR3ODQH :7$J0LQLPHUURU1HW/LQHV 9RWH7KHRUHWLFDO OLPLW1XPEHU RI SDUDPHWHUVFigure 6: Breiman waveforms: Generalization error g averaged over 11 testsversus number of parameters. Error bars on the number of weights generatedby NetLines and MonoPlane are not visible at the scale of the figure. 1: linear discrimination; 2: perceptron; 3: backpropagation; 4: genetic algorithm; 5: quadraticdiscrimination; 6: Parzens kernel; 7: K-NN; 8: constraint (Gascuel, 1995).formance to deteriorate. The TONs vote reduces the variance but does notdecrease the average g .5.3.2 Fishers Iris Plants Database. In this classic three-class problem, onehas to determine the class of iris plants based on the values of N = 4 realvalued attributes. The database of D = 150 patterns contains 50 examplesof each class. Networks were trained with P = 149 patterns, and the generalization error is the mean value of all the 150 leave-one-out possible tests.Results of g are displayed as a function of the number of weights in Figure 7.Error bars are available for only our own results. In this difficult problem,the vote of the three possible TONs trained with the three possible classsequences (see section 4) improves the generalization performance.</corps><conclusion>6 ConclusionWe presented an incremental learning algorithm for classification, which wecall NetLines. It generates small feedforward neural networks with a singlehidden layer of binary units connected to a binary output neuron. NetLinesallows for an automatic adaptation of the neural network to the complexityof the particular task. This is achieved by coupling an error-correcting strategy for the successive addition of hidden neurons with Minimerror, a very1026J. Manuel Torres Moreno and Mirta B. Gordon,5,6 GDWDEDVH1HW/LQHV :7$J1HW/LQHV YRWH1XPEHU RI ZHLJKWVFigure 7: Iris database: Generalization error g versus number of parameters.1: offset, 2: backpropagation (Martinez &amp; Esteve, 1992); 4,5: backpropagation(Verma &amp; Mulawka, 1995); 3,6: gradient-descent orthogonalized training (Verma&amp; Mulawka, 1995).efficient perceptron training algorithm. Learning is fast not only becauseit reduces the problem to that of training single perceptrons, but mainlybecause there is no longer a need for the usual preliminary tests required todetermine the correct architecture for the particular application. Theoremsvalid for binary as well as for real-valued inputs guarantee the existence ofa solution with a bounded number of hidden neurons obeying the growthstrategy.The networks are composed of binary hidden units whose states constitute a faithful encoding of the input patterns. They implement a mappingfrom the input space to a discrete H-dimensional hidden space, H beingthe number of hidden neurons. Thus, each pattern is labeled with a binaryword of H bits. This encoding may be seen as a compression of the patternsinformation. The hidden neurons define linear boundaries, or portions ofboundaries, between classes in input space. The networks output may begiven a probabilistic interpretation based on the distance of the patterns tothese boundaries.Tests on several benchmarks showed that the networks generated by ourincremental strategy are small, in spite of the fact that the hidden neuronsare appended until error-free learning is reached. Even when the networksobtained with NetLines are larger than those used by other algorithms, itsgeneralization error remains among the smallest values reported. In noisyor difficult problems, it may be useful to stop the networks growth beforeClassification Tasks with Binary Units1027the condition of zero training errors is reached. This decreases overfitting, assmaller networks (with less parameters) are thus generated. However, theprediction quality (measured by the generalization error) of the classifiersgenerated with NetLines is not improved by early stopping.Our results were obtained without cross-validation or any data manipulation like boosting, bagging, or arcing (Breiman, 1994; Drucker, Schapire,&amp; Simard, 1993). Those costly procedures combine results of very largenumbers of classifiers, with the aim of improving the generalization performance through the reduction of the variance. Because NetLines is a stableclassifier, presenting small variance, we do not expect that such techniqueswould significantly improve our results.AppendixIn this appendix we exhibit a particular solution to the learning strategy ofNetLines. This solution is built in such a way that the cardinal of a convexsubset of well-learned patterns, Lh , grows monotonically upon the additionof hidden units. Because this cardinal cannot be larger than the total numberof training patterns, the algorithm must stop with a finite number of hiddenunits.Suppose that h hidden units have already been included and that theoutput neuron still makes classification errors on patterns of the training set,called training errors. Among these wrongly learned patterns, let  be theE h , called hyperplane-h hereafter.one closest to the hyperplane normal to wWe define Lh as the subset of (correctly learned) patterns lying closer tohyperplane-h than E  . Patterns in Lh have 0 &lt; h &lt; |h |. The subset Lh andat least pattern  are well learned if the next hidden unit, h + 1, has weights:E h  E  )e0 ,E h  (1  h )h (wE h+1 = h ww(A.1)where e0  (1, 0, . . . , 0). The conditions that both Lh and pattern  havepositive stabilities (are correctly learned) impose that0 &lt; h &lt; minLh|h |  h.|h |(A.2)The following weights between the hidden units and the output will givethe correct output to pattern  and to the patterns of Lh :W0 (h + 1) = W0 (h) +  Wi (h + 1) = Wi (h) for 1  i  hWh+1 (h + 1) =   .(A.3)(A.4)(A.5)Thus, card(Lh+1 )  card(Lh ) + 1. As the number of patterns in Lh increasesmonotonically with h, convergence is guaranteed with less than P hiddenunits.1028J. Manuel Torres Moreno and Mirta B. GordonAcknowledgmentsJ.M. thanks Consejo Nacional de Ciencia y Tecnologa and UniversidadAutonoma Metropolitana, Mexico, for financial support (grant 65659).ReferencesAlpaydin, E. A. I. (1990). Neural models of supervised and unsupervised learning. Unpublished doctoral dissertation, Ecole Polytechnique Federale de Lausanne,Switzerland.Biehl, M., &amp; Opper, M. (1991). Tilinglike learning in the parity machine. PhysicalReview A, 44, 6888.Bottou, L., &amp; Vapnik, V. (1992). Local learning algorithms. Neural Computation,4(6), 888900.Breiman, L. (1994). Bagging predictors (Tech. Rep. No. 421). Berkeley: Departmentof Statistics, University of California at Berkeley.Breiman, L., Friedman, J. H., Olshen, R. A., &amp; Stone, C. J. (1984). Classificationand regression trees. Monterey, CA: Wadsworth and Brooks/Cole.Denker, J., Schwartz, D., Wittner, B., Solla, S., Howard, R., Jackel, L., &amp; Hopfield, J.(1987). Large automatic learning, rule extraction, and generalization. ComplexSystems, 1, 877922.Depenau, J. (1995). Automated design of neural network architecture for classification.Unpublished doctoral dissertation, Computer Science Department, AarhusUniversity.Drucker, H., Schapire, R., &amp; Simard, P. (1993). Improving performance in neural networks using a boosting algorithm. In S. J. Hanson, J. D. Cowan, &amp;C. L. Giles (Eds.), Advances in neural information processing systems, 5 (pp. 4249). San Mateo, CA: Morgan Kaufmann.Fahlman, S. E., &amp; Lebiere, C. (1990). The cascade-correlation learning architecture. In D. S. Touretzky (Ed.), Advances in neural information processing systems,2 (pp. 524532). San Mateo: Morgan Kaufmann.Farrell, K. R., &amp; Mammone, R. J. (1994). Speaker recognition using neural treenetworks. In J. D. Cowan, G. Tesauro, &amp; J. Alspector (Eds.), Advances in NeuralInformation Processing Systems, 6 (pp. 10351042). San Mateo, CA: MorganKaufmann.Frean, M. (1990). The Upstart algorithm: A method for constructing and trainingfeedforward neural networks. Neural Computation, 2(2), 198209.Frean, M. (1992). A thermal perceptron learning rule. Neural Computation, 4(6),946957.Friedman, J. H. (1996). On bias, variance, 0/1-loss, and the curse-of-dimensionality(Tech. Rep.) Stanford, CA: Department of Statistics, Stanford University.Fritzke, B. (1994). Supervised learning with growing cell structures. InJ. D. Cowan, G. Tesauro, &amp; J. Alspector (Eds.), Advances in neural information processing systems, 6 (pp. 255262). San Mateo, CA: Morgan Kaufmann.Gallant, S. I. (1986). Optimal linear discriminants. In Proc. 8th. Conf. PatternRecognition, Oct. 2831, Paris, vol. 4.Classification Tasks with Binary Units1029Gascuel, O. (1995). Symenu. Collective Paper (Gascuel O. Coordinator) (Tech. Rep.).5emes Journees Nationales du PRC-IA Teknea, Nancy.Geman, S., Bienenstock, E., &amp; Doursat, R. (1992). Neural networks and thebias/variance dilemma. Neural Computation, 4(1), 158.Goodman, R. M., Smyth, P., Higgins, C. M., &amp; Miller, J. W. (1992). Rule-basedneural networks for classification and probability estimation. Neural Computation, 4(6), 781804.Gordon, M. B. (1996). A convergence theorem for incremental learning with realvalued inputs. In IEEE International Conference on Neural Networks, pp. 381386.Gordon, M. B., &amp; Berchier, D. (1993). Minimerror: A perceptron learning rulethat finds the optimal weights. In M. Verleysen (Ed.), European Symposium onArtificial Neural Networks (pp. 105110). Brussels: D Facto.Gordon, M. B., &amp; Grempel, D. (1995). Optimal learning with a temperaturedependent algorithm. Europhysics Letters, 29(3), 257262.Gordon, M. B., Peretto, P., &amp; Berchier, D. (1993). Learning algorithms for perceptrons from statistical physics. Journal of Physics I (France), 3, 377387.Gorman, R. P., &amp; Sejnowski, T. J. (1988). Analysis of hidden units in a layerednetwork trained to classify sonar targets. Neural Networks, 1, 7589.Gyorgyi, G., &amp; Tishby, N. (1990). Statistical theory of learning a rule. InW. K. Theumann &amp; R. Koeberle (Eds.), Neural networks and spin glasses. Singapore: World Scientific.Hoehfeld, M., &amp; Fahlman, S. (1991). Learning with limited numerical precision usingthe cascade correlation algorithm (Tech. Rep. No. CMU-CS-91-130). Pittsburgh:Carnegie Mellon University.Knerr, S., Personnaz, L., &amp; Dreyfus, G. (1990). Single-layer learning revisited: Astepwise procedure for building and training a neural network. In J. Herault&amp; F. Fogelman (Eds.), Neurocomputing, algorithms, architectures and applications(pp. 4150). Berlin: Springer-Verlag.Marchand, M., Golea, M., &amp; Rujan, P. (1990). A convergence theorem for sequential learning in two-layer perceptrons. Europhysics Letters, 11, 487492.Martinez, D., &amp; Esteve, D. (1992). The offset algorithm: Building and learningmethod for multilayer neural networks. Europhysics Letters, 18, 95100.Mezard, M., &amp; Nadal, J.-P. (1989). Learning in feedforward layered networks:The Tiling algorithm. J. Phys. A: Math. and Gen., 22, 21912203.Mukhopadhyay, S., Roy, A., Kim, L. S., &amp; Govil, S. (1993). A polynomial time algorithm for generating neural networks for pattern classification: Its stabilityproperties and some test results. Neural Computation, 5(2), 317330.Nadal, J.-P. (1989). Study of a growth algorithm for a feedforward neural network. Int. J. Neur. Syst., 1, 5559.Prechelt, L. (1994). PROBEN1A set of benchmarks and benchmarking rules for neural network training algorithms (Tech. Rep. No. 21/94). University of Karlsruhe,Faculty of Informatics.Raffin, B., &amp; Gordon, M. B. (1995). Learning and generalization with Minimerror,a temperature dependent learning algorithm. Neural Computation, 7(6), 12061224.1030J. Manuel Torres Moreno and Mirta B. GordonReilly, D. E, Cooper, L. N., &amp; Elbaum, C. (1982). A neural model for categorylearning. Biological Cybernetics, 45, 3541.Roy, A., Kim, L., &amp; Mukhopadhyay, S. (1993). A polynomial time algorithmfor the construction and training of a class of multilayer perceptron. NeuralNetworks, 6(1), 535545.Sirat, J. A., &amp; Nadal, J.-P. (1990). Neural trees: A new tool for classification.Network, 1, 423438.Solla, S. A. (1989). Learning and generalization in layered neural networks: Thecontiguity problem. In L. Personnaz &amp; G. Dreyfus (Eds.), Neural Networksfrom Models to Applications. Paris: I.D.S.E.T.Torres Moreno, J.-M., &amp; Gordon, M. B. (1995). An evolutive architecture coupledwith optimal perceptron learning for classification. In M. Verleysen (Ed.),European Symposium on Artificial Neural Networks. Brussels: D Facto.Torres Moreno, J.-M., &amp; Gordon, M. B. (1998). Characterization of the sonarsignals benchmark. Neural Proc. Letters, 7(1), 14.Trhun, S. B., et al. (1991). The monks problems: A performance comparison of differentlearning algorithms (Tech. Rep. No. CMU-CS-91-197). Pittsburgh: CarnegieMellon University.Vapnik, V. (1992). Principles of risk minimization for learning theory. InJ. E. Moody, S. J. Hanson, &amp; R. P. Lippmann (Eds.), Advances in neural information processing systems, 4 (pp. 831838). San Mateo, CA: Morgan Kaufmann.Verma, B. K., &amp; Mulawka, J. J. (1995). A new algorithm for feedforward neural networks. In M. Verleysen (Ed.), European Symposium on Artificial NeuralNetworks (pp. 359364). Brussels: D Facto.Wolberg, W. H., &amp; Mangasarian, O. L. (1990). Multisurface method of patternseparation for medical diagnosis applied to breast cytology. In Proceedings ofthe National Academy of Sciences, USA, 87, 91939196.Received February 13, 1997; accepted September 4, 1997.This article has been cited by:1. C. Citterio, A. Pelagotti, V. Piuri, L. Rocca. 1999. Function approximation-fast-convergence neural approach based on spectralanalysis. IEEE Transactions on Neural Networks 10, 725-740. [CrossRef]2. Andrea Pelagotti, Vincenzo Piuri. 1997. Entropic Analysis and Incremental Synthesis of Multilayered Feedforward NeuralNetworks. International Journal of Neural Systems 08, 647-659. [CrossRef]</conclusion><discussion></discussion><biblio>ReferencesAlpaydin, E. A. I. (1990). Neural models of supervised and unsupervised learning. Unpublished doctoral dissertation, Ecole Polytechnique Federale de Lausanne,Switzerland.Biehl, M., &amp; Opper, M. (1991). Tilinglike learning in the parity machine. PhysicalReview A, 44, 6888.Bottou, L., &amp; Vapnik, V. (1992). Local learning algorithms. Neural Computation,4(6), 888900.Breiman, L. (1994). Bagging predictors (Tech. Rep. No. 421). Berkeley: Departmentof Statistics, University of California at Berkeley.Breiman, L., Friedman, J. H., Olshen, R. A., &amp; Stone, C. J. (1984). Classificationand regression trees. Monterey, CA: Wadsworth and Brooks/Cole.Denker, J., Schwartz, D., Wittner, B., Solla, S., Howard, R., Jackel, L., &amp; Hopfield, J.(1987). Large automatic learning, rule extraction, and generalization. ComplexSystems, 1, 877922.Depenau, J. (1995). Automated design of neural network architecture for classification.Unpublished doctoral dissertation, Computer Science Department, AarhusUniversity.Drucker, H., Schapire, R., &amp; Simard, P. (1993). Improving performance in neural networks using a boosting algorithm. In S. J. Hanson, J. D. Cowan, &amp;C. L. Giles (Eds.), Advances in neural information processing systems, 5 (pp. 4249). San Mateo, CA: Morgan Kaufmann.Fahlman, S. E., &amp; Lebiere, C. (1990). The cascade-correlation learning architecture. In D. S. Touretzky (Ed.), Advances in neural information processing systems,2 (pp. 524532). San Mateo: Morgan Kaufmann.Farrell, K. R., &amp; Mammone, R. J. (1994). Speaker recognition using neural treenetworks. In J. D. Cowan, G. Tesauro, &amp; J. Alspector (Eds.), Advances in NeuralInformation Processing Systems, 6 (pp. 10351042). San Mateo, CA: MorganKaufmann.Frean, M. (1990). The Upstart algorithm: A method for constructing and trainingfeedforward neural networks. Neural Computation, 2(2), 198209.Frean, M. (1992). A thermal perceptron learning rule. Neural Computation, 4(6),946957.Friedman, J. H. (1996). On bias, variance, 0/1-loss, and the curse-of-dimensionality(Tech. Rep.) Stanford, CA: Department of Statistics, Stanford University.Fritzke, B. (1994). Supervised learning with growing cell structures. InJ. D. Cowan, G. Tesauro, &amp; J. Alspector (Eds.), Advances in neural information processing systems, 6 (pp. 255262). San Mateo, CA: Morgan Kaufmann.Gallant, S. I. (1986). Optimal linear discriminants. In Proc. 8th. Conf. PatternRecognition, Oct. 2831, Paris, vol. 4.Classification Tasks with Binary Units1029Gascuel, O. (1995). Symenu. Collective Paper (Gascuel O. Coordinator) (Tech. Rep.).5emes Journees Nationales du PRC-IA Teknea, Nancy.Geman, S., Bienenstock, E., &amp; Doursat, R. (1992). Neural networks and thebias/variance dilemma. Neural Computation, 4(1), 158.Goodman, R. M., Smyth, P., Higgins, C. M., &amp; Miller, J. W. (1992). Rule-basedneural networks for classification and probability estimation. Neural Computation, 4(6), 781804.Gordon, M. B. (1996). A convergence theorem for incremental learning with realvalued inputs. In IEEE International Conference on Neural Networks, pp. 381386.Gordon, M. B., &amp; Berchier, D. (1993). Minimerror: A perceptron learning rulethat finds the optimal weights. In M. Verleysen (Ed.), European Symposium onArtificial Neural Networks (pp. 105110). Brussels: D Facto.Gordon, M. B., &amp; Grempel, D. (1995). Optimal learning with a temperaturedependent algorithm. Europhysics Letters, 29(3), 257262.Gordon, M. B., Peretto, P., &amp; Berchier, D. (1993). Learning algorithms for perceptrons from statistical physics. Journal of Physics I (France), 3, 377387.Gorman, R. P., &amp; Sejnowski, T. J. (1988). Analysis of hidden units in a layerednetwork trained to classify sonar targets. Neural Networks, 1, 7589.Gyorgyi, G., &amp; Tishby, N. (1990). Statistical theory of learning a rule. InW. K. Theumann &amp; R. Koeberle (Eds.), Neural networks and spin glasses. Singapore: World Scientific.Hoehfeld, M., &amp; Fahlman, S. (1991). Learning with limited numerical precision usingthe cascade correlation algorithm (Tech. Rep. No. CMU-CS-91-130). Pittsburgh:Carnegie Mellon University.Knerr, S., Personnaz, L., &amp; Dreyfus, G. (1990). Single-layer learning revisited: Astepwise procedure for building and training a neural network. In J. Herault&amp; F. Fogelman (Eds.), Neurocomputing, algorithms, architectures and applications(pp. 4150). Berlin: Springer-Verlag.Marchand, M., Golea, M., &amp; Rujan, P. (1990). A convergence theorem for sequential learning in two-layer perceptrons. Europhysics Letters, 11, 487492.Martinez, D., &amp; Esteve, D. (1992). The offset algorithm: Building and learningmethod for multilayer neural networks. Europhysics Letters, 18, 95100.Mezard, M., &amp; Nadal, J.-P. (1989). Learning in feedforward layered networks:The Tiling algorithm. J. Phys. A: Math. and Gen., 22, 21912203.Mukhopadhyay, S., Roy, A., Kim, L. S., &amp; Govil, S. (1993). A polynomial time algorithm for generating neural networks for pattern classification: Its stabilityproperties and some test results. Neural Computation, 5(2), 317330.Nadal, J.-P. (1989). Study of a growth algorithm for a feedforward neural network. Int. J. Neur. Syst., 1, 5559.Prechelt, L. (1994). PROBEN1A set of benchmarks and benchmarking rules for neural network training algorithms (Tech. Rep. No. 21/94). University of Karlsruhe,Faculty of Informatics.Raffin, B., &amp; Gordon, M. B. (1995). Learning and generalization with Minimerror,a temperature dependent learning algorithm. Neural Computation, 7(6), 12061224.1030J. Manuel Torres Moreno and Mirta B. GordonReilly, D. E, Cooper, L. N., &amp; Elbaum, C. (1982). A neural model for categorylearning. Biological Cybernetics, 45, 3541.Roy, A., Kim, L., &amp; Mukhopadhyay, S. (1993). A polynomial time algorithmfor the construction and training of a class of multilayer perceptron. NeuralNetworks, 6(1), 535545.Sirat, J. A., &amp; Nadal, J.-P. (1990). Neural trees: A new tool for classification.Network, 1, 423438.Solla, S. A. (1989). Learning and generalization in layered neural networks: Thecontiguity problem. In L. Personnaz &amp; G. Dreyfus (Eds.), Neural Networksfrom Models to Applications. Paris: I.D.S.E.T.Torres Moreno, J.-M., &amp; Gordon, M. B. (1995). An evolutive architecture coupledwith optimal perceptron learning for classification. In M. Verleysen (Ed.),European Symposium on Artificial Neural Networks. Brussels: D Facto.Torres Moreno, J.-M., &amp; Gordon, M. B. (1998). Characterization of the sonarsignals benchmark. Neural Proc. Letters, 7(1), 14.Trhun, S. B., et al. (1991). The monks problems: A performance comparison of differentlearning algorithms (Tech. Rep. No. CMU-CS-91-197). Pittsburgh: CarnegieMellon University.Vapnik, V. (1992). Principles of risk minimization for learning theory. InJ. E. Moody, S. J. Hanson, &amp; R. P. Lippmann (Eds.), Advances in neural information processing systems, 4 (pp. 831838). San Mateo, CA: Morgan Kaufmann.Verma, B. K., &amp; Mulawka, J. J. (1995). A new algorithm for feedforward neural networks. In M. Verleysen (Ed.), European Symposium on Artificial NeuralNetworks (pp. 359364). Brussels: D Facto.Wolberg, W. H., &amp; Mangasarian, O. L. (1990). Multisurface method of patternseparation for medical diagnosis applied to breast cytology. In Proceedings ofthe National Academy of Sciences, USA, 87, 91939196.Received February 13, 1997; accepted September 4, 1997.This article has been cited by:1. C. Citterio, A. Pelagotti, V. Piuri, L. Rocca. 1999. Function approximation-fast-convergence neural approach based on spectralanalysis. IEEE Transactions on Neural Networks 10, 725-740. [CrossRef]2. Andrea Pelagotti, Vincenzo Piuri. 1997. Entropic Analysis and Incremental Synthesis of Multilayered Feedforward NeuralNetworks. International Journal of Neural Systems 08, 647-659. [CrossRef]</biblio></article></xml>