<xml><article><preambule>Boudin-Torres-2006.pdf</preambule><titre>A Scalable MMR Approach to Sentence Scoring
</titre><auteur>Florian Boudin \ and Marc El-Be&#768;ze \
</auteur><abstract>Abstract

</abstract><introduction>1IntroductionExtensive experiments on query-oriented multidocument summarization have been carried outover the past few years. Most of the strategiesto produce summaries are based on an extraction method, which identifies salient textual segments, most often sentences, in documents. Sentences containing the most salient concepts are selected, ordered and assembled according to theirrelevance to produce summaries (also called extracts) (Mani and Maybury, 1999).Recently emerged from the Document Understanding Conference (DUC) 20071 , update summarization attempts to enhance summarizationwhen more information about knowledge acquiredby the user is available. It asks the following question: has the user already read documents on thetopic? In the case of a positive answer, producingan extract focusing on only new facts is of interest. In this way, an important issue is introduced:c 2008.Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.1Document Understanding Conferences are conductedsince 2000 by the National Institute of Standards and Technology (NIST), http://www-nlpir.nist.gov23Coling 2008: Companion volume  Posters and Demonstrations, pages 2326Manchester, August 2008introduces our proposed sentence scoring methodand Section 3 presents experiments and evaluatesour approach.sentence s and the query Q:1 XJ We (s, Q) =max J W(q, m)mS 0|Q|</introduction><corps>2where S 0 is the term set of s in which the termsm that already have maximized J W(q, m) are removed. The use of J We smooths normalization andmisspelling errors. Each sentence s is scored usingthe linear combination:qQMethodThe underlying idea of our method is that as thenumber of sentences in the history increases, thelikelihood to have redundant information withincandidate sentences also increases. We proposea scalable sentence scoring method derived fromMMR that, as the size of the history increases,gives more importance to non-redundancy that toquery relevance. We define H to represent the previously read documents (history), Q to representthe query and s the candidate sentence. The following subsections formally define the similaritymeasures and the scalable MMR scoring method.2.1(1)~Sim1 (s, Q) =   cosine(~s, Q)+ (1  )  J We (s, Q)(2)where  = 0.7, optimally tuned on the past DUCsdata (2005 and 2006). The system produces a listof ranked sentences from which the summary isconstructed by arranging the high scored sentencesuntil the desired size is reached.2.2A scalable MMR approachMMR re-ranking algorithm has been successfullyused in query-oriented summarization (Ye et al.,2005). It strives to reduce redundancy while maintaining query relevance in selected sentences. Thesummary is constructed incrementally from a listof ranked sentences, at each iteration the sentencewhich maximizes MMR is chosen:A query-oriented multi-documentsummarizerWe have first started by implementing a simplesummarizer for which the task is to produce queryfocused summaries from clusters of documents.Each document is pre-processed: documents aresegmented into sentences, sentences are filtered(words which do not carry meaning are removedsuch as functional words or common words) andnormalized using a lemmas database (i.e. inflectedforms go, goes, went, gone... are replacedby go). An N -dimensional term-space  , whereN is the number of different terms found in thecluster, is constructed. Sentences are representedin  by vectors in which each component is theterm frequency within the sentence. Sentence scoring can be seen as a passage retrieval task in Information Retrieval (IR). Each sentence s is scored bycomputing a combination of two similarity measures between the sentence and the query. The firstmeasure is the well known cosine angle (Salton etal., 1975) between the sentence and the query vectorial representations in  (denoted respectively ~s~ The second similarity measure is basedand Q).on the Jaro-Winkler distance (Winkler, 1999). Theoriginal Jaro-Winkler measure, denoted J W, usesthe number of matching characters and transpositions to compute a similarity score between twoterms, giving more favourable ratings to terms thatmatch from the beginning. We have extended thismeasure to calculate the similarity between theMMR = arg max [   Sim1 (s, Q)sS (1  )  max Sim2 (s, sj ) ]sj E(3)where S is the set of candidates sentences and Eis the set of selected sentences.  represents aninterpolation coefficient between sentences relevance and non-redundancy. Sim2 (s, sj ) is a normalized Longest Common Substring (LCS) measure between sentences s and sj . Detecting sentence rehearsals, LCS is well adapted for redundancy removal.We propose an interpretation of MMR to tacklethe update summarization issue. Since Sim1 andSim2 are ranged in [0, 1], they can be seen as probabilities even though they are not. Just as rewriting(3) as (NR stands for Novelty Relevance):NR = arg max [   Sim1 (s, Q)sS+ (1  )  (1  max Sim2 (s, sh )) ] (4)sh HWe can understand that (4) equates to an OR combination. But as we are looking for a more intuitive AND and since the similarities are independent, we have to use the product combination. The243. An update summary of documents in C, under the assumption that the reader has alreadyread documents in A and B.scoring method defined in (2) is modified into adouble maximization criterion in which the bestranked sentence will be the most relevant to thequery AND the most different to the sentences inH.Within a topic, the document clusters must be processed in chronological order. Our system generates a summary for each cluster by arranging thehigh ranked sentences until the limit of 100 wordsis reached.S MMR(s) = Sim1 (s, Q)f (H) 1  max Sim2 (s, sh )(5)sh H3.2Decreasing  in (3) with the length of the summary was suggested by (Murray et al., 2005) andsuccessfully used in the DUC 2005 by (Hacheyet al., 2005), thereby emphasizing the relevanceat the outset but increasingly prioritizing redundancy removal as the process continues. Similarly, we propose to follow this assumption inS MMR using a function denoted f that as theamount of data in history increases, prioritize nonredundancy (f (H)  0).3Most existing automated evaluation methods workby comparing the generated summaries to one ormore reference summaries (ideally, produced byhumans). To evaluate the quality of our generatedsummaries, we choose to use the ROUGE3 (Lin,2004) evaluation toolkit, that has been found to behighly correlated with human judgments. ROUGE N is a n-gram recall measure calculated betweena candidate summary and a set of reference summaries. In our experiments ROUGE -1, ROUGE -2and ROUGE - SU 4 will be computed.Experiments3.3The method described in the previous section hasbeen implemented and evaluated by using theDUC 2007 update corpus2 . The following subsections present details of the different experimentswe have conducted.3.1EvaluationResultsTable 1 reports the results obtained on the DUC2007 update data set for different sentence scoring methods. cosine + J We stands for the scoring method defined in (2) and NR improves itwith sentence re-ranking defined in equation (4).S MMR is the combined adaptation we have proposed in (5). The function f (H) used in S MMR isthe simple rational function H1 , where H increaseswith the number of previous clusters (f (H) = 1for cluster A, 12 for cluster B and 31 for cluster C).This function allows to simply test the assumptionthat non-redundancy have to be favoured as thesize of history grows. Baseline results are obtainedon summaries generated by taking the leading sentences of the most recent documents of the cluster,up to 100 words (official baseline of DUC). Thetable also lists the three top performing systems atDUC 2007 and the lowest scored human reference.As we can see from these results, S MMR outperforms the other sentence scoring methods. Byways of comparison our system would have beenranked second at the DUC 2007 update competition. Moreover, no post-processing was applied tothe selected sentences leaving an important marginof progress. Another interesting result is the highperformance of the non-update specific method(cosine + J We ) that could be due to the small sizeThe DUC 2007 update corpusWe used for our experiments the DUC 2007 update competition data set. The corpus is composedof 10 topics, with 25 documents per topic. The update task goal was to produce short (100 words)multi-document update summaries of newswire articles under the assumption that the user has already read a set of earlier articles. The purposeof each update summary will be to inform thereader of new information about a particular topic.Given a DUC topic and its 3 document clusters: A(10 documents), B (8 documents) and C (7 documents), the task is to create from the documentsthree brief, fluent summaries that contribute to satisfying the information need expressed in the topicstatement.1. A summary of documents in cluster A.2. An update summary of documents in B, under the assumption that the reader has alreadyread documents in A.2More information about the DUC 2007 corpus is available at http://duc.nist.gov/.325ROUGE is available at http://haydn.isi.edu/ROUGE/.of the corpus (little redundancy between clusters).Baseline3rd system2nd systemcosine + J WeNRS MMR1st systemWorst humanROUGE -1ROUGE -2ROUGE - SU 40.262320.357150.369650.359050.362070.363230.370320.404970.045430.096220.098510.101610.100420.102230.111890.105110.082470.132450.135090.137010.137810.138860.143060.14779Hickl, A., K. Roberts, and F. Lacatusu. 2007. LCCsGISTexter at DUC 2007: Machine Reading for Update Summarization. In Document UnderstandingConference (DUC).Lin, Z., T.S. Chua, M.Y. Kan, W.S. Lee, L. Qiu, andS. Ye. 2007. NUS at DUC 2007: Using Evolutionary Models of Text. In Document UnderstandingConference (DUC).Lin, C.Y. 2004. Rouge: A Package for AutomaticEvaluation of Summaries. In Workshop on Text Summarization Branches Out, pages 2526.Table 1: ROUGE average recall scores computedon the DUC 2007 update corpus.4Hachey, B., G. Murray, and D. Reitter. 2005. TheEmbra System at DUC 2005: Query-oriented Multidocument Summarization with a Very Large LatentSemantic Space. In Document Understanding Conference (DUC).Mani, I. and M.T. Maybury. 1999. Advances in Automatic Text Summarization. MIT Press.Discussion and Future WorkMani, I. and G. Wilson. 2000. Robust temporal processing of news. In 38th Annual Meeting on Association for Computational Linguistics, pages 6976.Association for Computational Linguistics Morristown, NJ, USA.In this paper we have described S MMR, a scalable sentence scoring method based on MMR thatachieves very promising results. An important aspect of our sentence scoring method is that it doesnot requires re-ranking nor linguistic knowledge,which makes it a simple and fast approach to theissue of update summarization. It was pointed outat the DUC 2007 workshop that Question Answering and query-oriented summarization have beenconverging on a common task. The value addedby summarization lies in the linguistic quality. Approaches mixing IR techniques are well suited forquery-oriented summarization but they require intensive work for making the summary fluent andcoherent. Among the others, this is a point that wethink is worthy of further investigation.Murray, G., S. Renals, and J. Carletta. 2005. ExtractiveSummarization of Meeting Recordings. In Ninth European Conference on Speech Communication andTechnology. ISCA.Salton, G., A. Wong, and C. S. Yang. 1975. A vectorspace model for automatic indexing. Communications of the ACM, 18(11):613620.Swan, R. and J. Allan. 2000. Automatic generationof overview timelines. In 23rd annual internationalACM SIGIR conference on Research and development in information retrieval, pages 4956.AcknowledgmentsWinkler, W. E. 1999. The state of record linkage andcurrent research problems. In Survey Methods Section, pages 7379.This work was supported by the Agence Nationalede la Recherche, France, project RPM2.Witte, R., R. Krestel, and S. Bergler. 2007. Generating Update Summaries for DUC 2007. In DocumentUnderstanding Conference (DUC).ReferencesYe, S., L. Qiu, T.S. Chua, and M.Y. Kan. 2005. NUSat DUC 2005: Understanding documents via concept links. In Document Understanding Conference(DUC).Boudin, F. and J.M. Torres-Moreno. 2007. A Cosine Maximization-Minimization approach for UserOriented Multi-Document Update Summarization.In Recent Advances in Natural Language Processing(RANLP), pages 8187.Carbonell, J. and J. Goldstein. 1998. The use of MMR,diversity-based reranking for reordering documentsand producing summaries. In 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 335336.ACM Press New York, NY, USA.26</corps><conclusion></conclusion><discussion></discussion><biblio>ReferencesYe, S., L. Qiu, T.S. Chua, and M.Y. Kan. 2005. NUSat DUC 2005: Understanding documents via concept links. In Document Understanding Conference(DUC).Boudin, F. and J.M. Torres-Moreno. 2007. A Cosine Maximization-Minimization approach for UserOriented Multi-Document Update Summarization.In Recent Advances in Natural Language Processing(RANLP), pages 8187.Carbonell, J. and J. Goldstein. 1998. The use of MMR,diversity-based reranking for reordering documentsand producing summaries. In 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 335336.ACM Press New York, NY, USA.26</biblio></article></xml>