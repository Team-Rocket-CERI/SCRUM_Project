<xml><article><preambule>Gonzalez_2018_Wisebe.pdf</preambule><titre>WiSeBE: Window-Based Sentence
</titre><auteur></auteur><abstract>Abstract. Sentence Boundary Detection (SBD) has been a major
research topic since Automatic Speech Recognition transcripts have been
used for further Natural Language Processing tasks like Part of Speech
Tagging, Question Answering or Automatic Summarization. But what
about evaluation? Do standard evaluation metrics like precision, recall,
F-score or classi&#64257;cation error; and more important, evaluating an automatic system against a unique reference is enough to conclude how well
a SBD system is performing given the &#64257;nal application of the transcript?
In this paper we propose Window-based Sentence Boundary Evaluation
(WiSeBE), a semi-supervised metric for evaluating Sentence Boundary
Detection systems based on multi-reference (dis)agreement. We evaluate and compare the performance of di&#64256;erent SBD systems over a set
of Youtube transcripts using WiSeBE and standard metrics. This double evaluation gives an understanding of how WiSeBE is a more reliable
metric for the SBD task.
Keywords: Sentence Boundary Detection
Transcripts &#183; Human judgment

</abstract><introduction>1 EvaluationIntroductionThe goal of Automatic Speech Recognition (ASR) is to transform spoken datainto a written representation, thus enabling natural human-machine interaction[33] with further Natural Language Processing (NLP) tasks. Machine translation, question answering, semantic parsing, POS tagging, sentiment analysis andautomatic text summarization; originally developed to work with formal written texts, can be applied over the transcripts made by ASR systems [2,25,31].However, before applying any of these NLP tasks a segmentation process calledSentence Boundary Detection (SBD) should be performed over ASR transcriptsto reach a minimal syntactic information in the text.To measure the performance of a SBD system, the automatically segmentedtranscript is evaluated against a single reference normally done by a human. Butc Springer Nature Switzerland AG 2018I. Batyrshin et al. (Eds.): MICAI 2018, LNAI 11289, pp. 119131, 2018.https://doi.org/10.1007/978-3-030-04497-8_10120C.-E. Gonzalez-Gallardo and J.-M. Torres-Morenogiven a transcript, does it exist a unique reference? Or, is it possible that thesame transcript could be segmented in ve dierent ways by ve dierent peoplein the same conditions? If so, which one is correct; and more important, howto fairly evaluate the automatically segmented transcript? These questions arethe foundations of Window-based Sentence Boundary Evaluation (WiSeBE), anew semi-supervised metric for evaluating SBD systems based on multi-reference(dis)agreement.The rest of this article is organized as follows. In Sect. 2 we set the frame ofSBD and how it is normally evaluated. WiSeBE is formally described in Sect. 3,followed by a multi-reference evaluation in Sect. 4. Further analysis of WiSeBEand discussion over the method and alternative multi-reference evaluation ispresented in Sect. 5. Finally, Sect. 6 concludes the paper.</introduction><corps>2Sentence Boundary DetectionSentence Boundary Detection (SBD) has been a major research topic scienceASR moved to more general domains as conversational speech [17,24,26]. Performance of ASR systems has improved over the years with the inclusion andcombination of new Deep Neural Networks methods [5,9,33]. As a general rule,the output of ASR systems lacks of any syntactic information such as capitalization and sentence boundaries, showing the interest of ASR systems to obtainthe correct sequence of words with almost no concern of the overall structure ofthe document [8].Similar to SBD is the Punctuation Marks Disambiguation (PMD) or SentenceBoundary Disambiguation. This task aims to segment a formal written text intowell formed sentences based on the existent punctuation marks [11,19,20,29]. Inthis context a sentence is dened (for English) by the Cambridge Dictionary1as:a group of words, usually containing a verb, that expresses a thought inthe form of a statement, question, instruction, or exclamation and startswith a capital letter when written.PMD carries certain complications, some given the ambiguity of punctuationmarks within a sentence. A period can denote an acronym, an abbreviation, theend of the sentence or a combination of them as in the following example:The U.S. president, Mr. Donald Trump, is meeting with the F.B.I.director Christopher A. Wray next Thursday at 8 p.m.However its diculties, DPM prots of morphological and lexical informationto achieve a correct sentence segmentation. By contrast, segmenting an ASRtranscript should be done without any (or almost any) lexical information anda urry denition of sentence.1https://dictionary.cambridge.org/.WiSeBE: Window-Based Sentence Boundary Evaluation121The obvious division in spoken language may be considered speaker utterances. However, in a normal conversation or even in a monologue, the way ideasare organized diers largely from written text. This dierences, added to disuencies like revisions, repetitions, restarts, interruptions and hesitations make thedenition of a sentence unclear thus complicating the segmentation task [27].Table 1 exemplies some of the diculties that are present when working withspoken language.Table 1. Sentence Boundary Detection exampleSpeech transcriptSBD applied to transcripttwo two women can look out aftera kid so bad as a man and awoman can so you can have a youcan have a mother and a fatherthat that still dont do right withthe kid and you can have to menthat can so as long as the loveeach other as long as they loveeach other it doesnt mattertwo // two women can look outafter a kid so bad as a man and awoman can // so you can have a// you can have a mother and afather that // that still dont doright with the kid and you canhave to men that can // so aslong as the love each other // aslong as they love each other itdoesnt matter//Stolcke and Shriberg [26] considered a set of linguistic structures as segmentsincluding the following list:Complete sentencesStand-alone sentencesDisuent sentences aborted in mid-utteranceInterjectionsBack-channel responses.In [17], Meteer and Iyer divided speaker utterances into segments, consistingeach of a single independent clause. A segment was considered to begin eitherat the beginning of an utterance, or after the end of the preceding segment. Anydysuency between the end of the previous segments and the begging of currentone was considered part of the current segments.Rott and Cerva [23] aimed to summarize news delivered orally segmenting thetranscripts into something that is similar to sentences. They used a syntacticanalyzer to identify the phrases within the text.A wide study focused in unbalanced data for the SBD task was performedby Liu et al. [15]. During this study they followed the segmentation scheme proposed by the Linguistic Data Consortium2 on the Simple Metadata AnnotationSpecication V5.0 guideline (SimpleMDE V5.0) [27], dividing the transcripts inSemantic Units.2https://www.ldc.upenn.edu/.122C.-E. Gonzalez-Gallardo and J.-M. Torres-MorenoA Semantic Unit (SU) is considered to be an atomic element of the transcriptthat manages to express a complete thought or idea on the part of the speaker[27]. Sometimes a SU corresponds to the equivalent of a sentence in written text,but other times (the most part of them) a SU corresponds to a phrase or a singleword.SUs seem to be an inclusive conception of a segment, they embrace dierentprevious segment denitions and are exible enough to deal with the majorityof spoken language troubles. For these reasons we will adopt SUs as our segmentdenition.2.1Sentence Boundary EvaluationSBD research has been focused on two dierent aspects; features and methods.Regarding the features, some work focused on acoustic elements like pausesduration, fundamental frequencies, energy, rate of speech, volume change andspeaker turn [10,12,14].The other kind of features used in SBD are textual or lexical features. Theyrely on the transcript content to extract features like bag-of-word, POS tags orword embeddings [7,12,16,18,23,26,30]. Mixture of acoustic and lexical featureshave also been explored [1,13,14,32], which is advantageous when both audiosignal and transcript are available.With respect to the methods used for SBD, they mostly rely on statistical/neural machine translation [12,22], language models [8,15,18,26], conditionalrandom elds [16,30] and deep neural networks [3,7,29].Despite their dierences in features and/or methodology, almost all previouscited research share a common element; the evaluation methodology. Metrics asPrecision, Recall, F1-score, Classication Error Rate and Slot Error Rate (SER)are used to evaluate the proposed system against one reference. As discussedin Sect. 1, further NLP tasks rely on the result of SBD, meaning that is crucialto have a good segmentation. But comparing the output of a system against aunique reference will provide a reliable score to decide if the system is good orbad?Bohac et al. [1] compared the human ability to punctuate recognized spontaneous speech. They asked 10 people (correctors) to punctuate about 30 min ofASR transcripts in Czech. For an average of 3,962 words, the punctuation marksplaced by correctors varied between 557 and 801; this means a dierence of 244segments for the same transcript. Over all correctors, the absolute consensus forperiod (.) was only 4.6% caused by the replacement of other punctuation marksas semicolons (;) and exclamation marks (!). These results are understandable ifwe consider the diculties presented previously in this section.To our knowledge, the amount of studies that have tried to target the sentenceboundary evaluation with a multi-reference approach is very small. In [1], Bohacet al. evaluated the overall punctuation accuracy for Czech in a straightforwardmulti-reference framework. They considered a period (.) valid if at least ve oftheir 10 correctors agreed on its position.WiSeBE: Window-Based Sentence Boundary Evaluation123Kolar and Lamel [13] considered two independent references to evaluate theirsystem and proposed two approaches. The st one was to calculate the SER foreach of one the two available references and then compute their mean. Theyfound this approach to be very strict because for those boundaries where noagreement between references existed, the system was going to be partially wrongeven the fact that it has correctly predicted the boundary. Their second approach tried to moderate the number of unjust penalizations. For this case, aclassication was considered incorrect only if it didnt match either of the tworeferences.These two examples exemplify the real need and some straightforward solutions for multi-reference evaluation metrics. However, we think that it is possibleto consider in a more inclusive approach the similarities and dierences that multiple references could provide into a sentence boundary evaluation protocol.3Window-Based Sentence Boundary EvaluationWindow-Based Sentence Boundary Evaluation (WiSeBE) is a semi-automaticmulti-reference sentence boundary evaluation protocol which considers the performance of a candidate segmentation over a set of segmentation references andthe agreement between those references.Let R = {R1 , R2 , ..., Rm } be the set of all available references given a transcript T = {t1 , t2 , ..., tn }, where tj is the j th word in the transcript; a referenceRi is dened as a binary vector in terms of the existent SU boundaries in T .Ri = {b1 , b2 , ..., bn }wherebj =(1)1 if tj is a boundary0 otherwiseGiven a transcript T , the candidate segmentation CT is dened similar to Ri .CT = {b1 , b2 , ..., bn }wherebj =3.1(2)1 if tj is a boundary0 otherwiseGeneral Reference and Agreement RatioA General Reference (RG ) is then constructed to calculate the agreement ratiobetween all references in. It is dened by the boundary frequencies of each reference Ri  R.RG = {d1 , d2 , ..., dn }where(3)124C.-E. Gonzalez-Gallardo and J.-M. Torres-Morenodj =mtijtj  T,dj = [0, m](4)i=1The Agreement Ratio (RGAR ) is needed to get a numerical value of the distribution of SU boundaries over R. A value of RGAR close to 0 means a lowagreement between references in R, while RGAR = 1 means a perfect agreement(Ri  R, Ri = Ri+1 |i = 1, ..., m  1) in R.RGAR =R GP BRGHA(5)In the equation above, RGP B corresponds to the ponderated common boundariesof RG and RGHA to its hypothetical maximum agreement.R GP B =ndj [dj  2](6)j=1RGHA = m 1 [dj = 0](7)dj RG3.2Window-Boundaries ReferenceIn Sect. 2 we discussed about how disuencies complicate SU segmentation. In amulti-reference environment this causes disagreement between references arounda same SU boundary. The way WiSeBE handle disagreements produced by disuencies is with a Window-boundaries Reference (RW ) dened as:RW = {w1 , w2 , ..., wp }(8)where each window wk considers one or more boundaries dj from RG with awindow separation limit equal to RWl .wk = {dj , dj+1 , dj+2 , ...}3.3(9)W iSeBEWiSeBE is a normalized score dependent of (1) the performance of CT over RWand (2) the agreement between all references in R. It is dened as:W iSeBE = F 1RW  RGARW iSeBE = [0, 1](10)where F 1RW corresponds to the harmonic mean of precision and recall of CTwith respect to RW (Eq. 11), while RGAR is the agreement ratio dened in (5).RGAR can be interpreted as a scaling factor; a low value will penalize the overallWiSeBE score given the low agreement between references. By contrast, for ahigh agreement in R (RGAR  1), W iSeBE  F 1RW .WiSeBE: Window-Based Sentence Boundary Evaluation125precisionRW  recallRWprecisionRW + recallRW(11)F 1RW = 2 precisionRW =bj CT1 [bj = 1, bj  w w  RW ]bj CT 1 [bj = 1](12)1 [wk  b b  CT ](13)pEquations 12 and 13 describe precision and recall of CT with respect to RW .Precision is the number of boundaries bj inside any window wk from RW dividedby the total number of boundaries bj in CT . Recall corresponds to the numberof windows w with at least one boundary b divided by the number of windowsw in RW .recallRW =4wk RWEvaluating with W iSeBETo exemplify the W iSeBE score we evaluated and compared the performanceof two dierent SBD systems over a set of YouTube videos in a multi-referenceenvironment. The rst system (S1) employs a Convolutional Neural Network todetermine if the middle word of a sliding window corresponds to a SU boundary or not [6]. The second approach (S2) by contrast, introduces a bidirectionalRecurrent Neural Network model with attention mechanism for boundary detection [28].In a rst glance we performed the evaluation of the systems against eachone of the references independently. Then, we implemented a multi-referenceevaluation with W iSeBE.4.1DatasetWe focused evaluation over a small but diversied dataset composed by 10YouTube videos in the English language in the news context. The selected videoscover dierent topics like technology, human rights, terrorism and politics witha length variation between 2 and 10 min. To encourage the diversity of contentformat we included newscasts, interviews, reports and round tables.During the transcription phase we opted for a manual transcription processbecause we observed that using transcripts from an ASR system will dicultin a large degree the manual segmentation process. The number of words pertranscript oscilate between 271 and 1,602 with a total number of 8,080.We gave clear instructions to three evaluators (ref1 , ref2 , ref3 ) of how segmentation was needed to be perform, including the SU concept and how punctuation marks were going to be taken into account. Periods (.), question marks (?),exclamation marks (!) and semicolons (;) were considered SU delimiters (boundaries) while colons (:) and commas (,) were considered as internal SU marks.The number of segments per transcript and reference can be seen in Table 2. Aninteresting remark is that ref3 assigns about 43% less boundaries than the meanof the other two references.126C.-E. Gonzalez-Gallardo and J.-M. Torres-MorenoTable 2. Manual dataset segmentationReference v1 v2 v3 v4 v5 v6 v74.2v8 v9 v10 Totalref138 42 17 11 55 87 109 72 55 16502ref233 42 16 14 54 9892 65 51 20485ref323 20 1076 30 292816 39 399EvaluationWe ran both systems (S1 &amp; S2) over the manually transcribed videos obtainingthe number of boundaries shown in Table 3. In general, it can be seen that S1predicts 27% more segments than S2. This dierence can aect the performanceof S1, increasing its probabilities of false positives.Table 3. Automatic dataset segmentationSystem v1 v2 v3 v4 v5 v6v7v8 v9 v10 TotalS153 38 15 13 54 108 106 70 71 11539S238 37 12 11 364249286 46 53 13Table 4 condenses the performance of both systems evaluated against eachone of the references independently. If we focus on F1 scores, performance of bothsystems varies depending of the reference. For ref1 , S1 was better in 5 occasionswith respect of S2; S1 was better in 2 occasions only for ref2 ; S1 overperformedS2 in 3 occasions concerning ref3 and in 4 occasions for mean (bold).Also from Table 4 we can observe that ref1 has a bigger similarity to S1 in5 occasions compared to other two references, while ref2 is more similar to S2in 7 transcripts (underline).After computing the mean F1 scores over the transcripts, it can be concludedthat in average S2 had a better performance segmenting the dataset comparedto S1, obtaining a F1 score equal to 0.510. But... What about the complexity ofthe dataset? Regardless all references have been considered, nor agreement ordisagreement between them has been taken into account.All values related to the W iSeBE score are displayed in Table 5. The Agreement Ratio (RGAR ) between references oscillates between 0.525 for v8 and 0.767for v5 . The lower the RGAR , the bigger the penalization W iSeBE will give tothe nal score. A good example is S2 for transcript v4 where F 1RW reaches avalue of 0.800, but after considering RGAR the W iSeBE score falls to 0.462.It is feasible to think that if all references are taken into account at the sametime during evaluation (F 1RW ), the score will be bigger compared to an averageof independent evaluations (F 1mean ); however this is not always true. That isthe case of S1 in v10, which present a slight decrease for F 1RW compared toF 1mean .WiSeBE: Window-Based Sentence Boundary Evaluation127Table 4. Independent multi-reference evaluationTranscriptSystem ref1Pv1v2v3v4v5v6v7v8v9v10ref2RF1Pref3RF1PM eanRF1PRF1S10.396 0.553 0.462 0.377 0.606 0.465 0.264 0.609 0.368 0.346 0.589 0.432S20.474 0.474 0.474 0.474 0.545 0.507 0.368 0.6087 0.459 0.439 0.543 0.480S10.605 0.548 0.575 0.711 0.643 0.675 0.368 0.700 0.483 0.561 0.630 0.578S20.595 0.524 0.557 0.676 0.595 0.633 0.351 0.650 0.456 0.541 0.590 0.549S10.333 0.294 0.313 0.267 0.250 0.258 0.200 0.300 0.240 0.267 0.281 0.270S20.417 0.294 0.345 0.417 0.313 0.357 0.250 0.300 0.273 0.361 0.302 0.325S10.615 0.571 0.593 0.462 0.545 0.500 0.308 0.667 0.421 0.462 0.595 0.505S20.909 0.714 0.800 0.818 0.818 0.818 0.455 0.833 0.588 0.727 0.789 0.735S10.630 0.618 0.624 0.593 0.593 0.593 0.481 0.667 0.560 0.568 0.626 0.592S20.667 0.436 0.527 0.611 0.407 0.489 0.500 0.462 0.480 0.593 0.435 0.499S10.491 0.541 0.515 0.454 0.563 0.503 0.213 0.590 0.313 0.386 0.565 0.443S20.500 0.469 0.484 0.522 0.552 0.536 0.250 0.590 0.351 0.4234 0.537 0.457S10.594 0.578 0.586 0.462 0.533 0.495 0.406 0.566 0.473 0.487 0.559 0.518S20.663 0.523 0.585 0.558 0.522 0.539 0.465 0.526 0.494 0.562 0.524 0.539S10.443 0.477 0.459 0.514 0.500 0.507 0.229 0.533 0.320 0.395 0.503 0.429S20.609 0.431 0.505 0.652 0.417 0.508 0.370 0.567 0.447 0.543 0.471 0.487S10.437 0.564 0.492 0.451 0.627 0.525 0.254 0.621 0.360 0.380 0.603 0.459S20.623 0.600 0.611 0.585 0.608 0.596 0.321 0.586 0.414 0.509 0.598 0.541S10.818 0.450 0.581 0.818 0.450 0.581 0.455 0.556 0.500 0.697 0.523 0.582S20.692 0.450 0.545 0.615 0.500 0.552 0.308 0.444 0.364 0.538 0.4645 0.487Mean scores S10.5200.5100.4040.481S20.5430.5540.4330.510An important remark is the behavior of S1 and S2 concerning v6 . If evaluated without considering any (dis)agreement between references (F 1mean ), S2overperforms S1; this is inverted once the systems are evaluated with W iSeBE.55.1DiscussionRG A R and Fleiss Kappa correlationIn Sect. 3 we described the W iSeBE score and how it relies on the RGAR valueto scale the performance of CT over RW . RGAR can intuitively be consider anagreement value over all elements of R. To test this hypothesis, we computedthe Pearson correlation coecient (P CC) [21] between RGAR and the FleissKappa [4] of each video in the dataset (R ).A linear correlation between RGAR and R can be observed in Table 6. Thisis conrmed by a P CC value equal to 0.890, which means a very strong positivelinear correlation between them.5.2F 1m ean vs. W iSeBEResults form Table 5 may give an idea that W iSeBE is just an scaled F 1mean .While it is true that they show a linear correlation, W iSeBE may produce a128C.-E. Gonzalez-Gallardo and J.-M. Torres-MorenoTable 5. W iSeBE evaluationTranscriptSystem F 1mean F 1RW RGAR W iSeBEv1S1S20.4320.4800.4950.5130.6910.3420.354v2S1S20.5780.5490.6590.5950.6880.4530.409v3S1S20.2700.3250.3030.4000.6840.2070.274v4S1S20.5050.7350.5930.8000.5780.3420.462v5S1S20.5920.4990.6140.5000.7670.4710.383v6S1S20.4430.4570.5500.5350.5410.2980.289v7S1S20.5180.5390.5920.6060.6170.3660.374v8S1S20.4290.4870.4940.5080.5250.2590.267v9S1S20.4590.5410.5690.6670.6040.3440.403v10S1S20.5820.4870.5810.5450.6190.3590.338Mean scores S1S20.4810.5100.5450.5670.6310.3440.355Table 6. Agreement within datasetAgreement metric v1v2v3v4v5v6v7v8v9v10RGAR0.691 0.688 0.684 0.578 0.767 0.541 0.617 0.525 0.604 0.619R0.776 0.697 0.757 0.696 0.839 0.630 0.743 0.655 0.704 0.718dierent system ranking than F 1mean given the integral multi-reference principleit follows. However, what we consider the most protable about W iSeBE is thetwofold inclusion of all available references it performs. First, the construction ofRW to provide a more inclusive reference against to whom be evaluated and then,the computation of RGAR , which scales the result depending of the agreementbetween references.WiSeBE: Window-Based Sentence Boundary Evaluation6129</corps><conclusion>ConclusionsIn this paper we presented WiSeBE, a semi-automatic multi-reference sentenceboundary evaluation protocol based on the necessity of having a more reliableway for evaluating the SBD task. We showed how W iSeBE is an inclusive metricwhich not only evaluates the performance of a system against all references, butalso takes into account the agreement between them. According to your pointof view, this inclusivity is very important given the diculties that are presentwhen working with spoken language and the possible disagreements that a tasklike SBD could provoke.W iSeBE shows to be correlated with standard SBD metrics, however wewant to measure its correlation with extrinsic evaluations techniques like automatic summarization and machine translation.Acknowledgments. We would like to acknowledge the support of CHIST-ERA forfunding this work through the Access Multilingual Information opinionS (AMIS),(France - Europe) project.We also like to acknowledge the support given by the Prof. Hanifa Boucheneb fromVERIFORM Laboratory (Ecole Polytechnique de Montreal).References1. Bohac, M., Blavka, K., Kucharova, M., Skodova, S.: Post-processing of the recognized speech for web presentation of large audio archive. In: 2012 35th InternationalConference on Telecommunications and Signal Processing (TSP), pp. 441445.IEEE (2012)2. Brum, H., Araujo, F., Kepler, F.: Sentiment analysis for Brazilian portuguese overa skewed class corpora. In: Silva, J., Ribeiro, R., Quaresma, P., Adami, A., Branco,A. (eds.) PROPOR 2016. LNCS (LNAI), vol. 9727, pp. 134138. Springer, Cham(2016). https://doi.org/10.1007/978-3-319-41552-9 143. Che, X., Wang, C., Yang, H., Meinel, C.: Punctuation prediction for unsegmentedtranscript based on word vector. In: LREC (2016)4. Fleiss, J.L.: Measuring nominal scale agreement among many raters. Psychol. Bull.76(5), 378 (1971)5. Fohr, D., Mella, O., Illina, I.: New paradigm in speech recognition: deep neural networks. In: IEEE International Conference on Information Systems and EconomicIntelligence (2017)6. Gonzalez-Gallardo, C.E., Hajjem, M., SanJuan, E., Torres-Moreno, J.M.: Transcripts informativeness study: an approach based on automatic summarization. In:Conference en Recherche dInformation et Applications (CORIA), Rennes, France,May (2018)7. Gonzalez-Gallardo, C.E., Torres-Moreno, J.M.: Sentence boundary detection forFrench with subword-level information vectors and convolutional neural networks.arXiv preprint arXiv:1802.04559 (2018)8. Gotoh, Y., Renals, S.: Sentence boundary detection in broadcast speech transcripts.In: ASR2000-Automatic Speech Recognition: Challenges for the new MilleniumISCA Tutorial and Research Workshop (ITRW) (2000)130C.-E. Gonzalez-Gallardo and J.-M. Torres-Moreno9. Hinton, G., et al.: Deep neural networks for acoustic modeling in speech recognition: the shared views of four research groups. IEEE Signal Process. Mag. 29(6),8297 (2012)10. Jamil, N., Ramli, M.I., Seman, N.: Sentence boundary detection without speechrecognition: a case of an under-resourced language. J. Electr. Syst. 11(3), 308318(2015)11. Kiss, T., Strunk, J.: Unsupervised multilingual sentence boundary detection. Comput. Linguist. 32(4), 485525 (2006)12. Klejch, O., Bell, P., Renals, S.: Punctuated transcription of multi-genre broadcastsusing acoustic and lexical approaches. In: 2016 IEEE Spoken Language TechnologyWorkshop (SLT), pp. 433440. IEEE (2016)13. Kolar, J., Lamel, L.: Development and evaluation of automatic punctuation forFrench and english speech-to-text. In: Thirteenth Annual Conference of the International Speech Communication Association (2012)14. Kolar, J., Svec, J., Psutka, J.: Automatic punctuation annotation in Czech broadcast news speech. In: SPECOM 2004 (2004)15. Liu, Y., Chawla, N.V., Harper, M.P., Shriberg, E., Stolcke, A.: A study in machinelearning from imbalanced data for sentence boundary detection in speech. Comput.Speech Lang. 20(4), 468494 (2006)16. Lu, W., Ng, H.T.: Better punctuation prediction with dynamic conditional random elds. In: Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing. pp. 177186. Association for Computational Linguistics(2010)17. Meteer, M., Iyer, R.: Modeling conversational speech for speech recognition. In:Conference on Empirical Methods in Natural Language Processing (1996)18. Mrozinski, J., Whittaker, E.W., Chatain, P., Furui, S.: Automatic sentence segmentation of speech for automatic summarization. In: 2006 IEEE InternationalConference on Acoustics Speech and Signal Processing Proceedings, vol. 1, p. I.IEEE (2006)19. Palmer, D.D., Hearst, M.A.: Adaptive sentence boundary disambiguation. In: Proceedings of the Fourth Conference on Applied Natural Language Processing, pp.7883. ANLC 1994. Association for Computational Linguistics, Stroudsburg, PA,USA (1994)20. Palmer, D.D., Hearst, M.A.: Adaptive multilingual sentence boundary disambiguation. Comput. Linguist. 23(2), 241267 (1997)21. Pearson, K.: Note on regression and inheritance in the case of two parents. Proc.R. Soc. Lond. 58, 240242 (1895)22. Peitz, S., Freitag, M., Ney, H.: Better punctuation prediction with hierarchicalphrase-based translation. In: Proceedings of the International Workshop on SpokenLanguage Translation (IWSLT), South Lake Tahoe, CA, USA (2014)23. Rott, M., Cerva, P.: Speech-to-text summarization using automatic phrase extraction from recognized text. In: Sojka, P., Horak, A., Kopecek, I., Pala, K. (eds.) TSD2016. LNCS (LNAI), vol. 9924, pp. 101108. Springer, Cham (2016). https://doi.org/10.1007/978-3-319-45510-5 1224. Shriberg, E., Stolcke, A.: Word predictability after hesitations: a corpus-basedstudy. In: Proceedings of the Fourth International Conference on Spoken Language,1996. ICSLP 1996, vol. 3, pp. 18681871. IEEE (1996)25. Stevenson, M., Gaizauskas, R.: Experiments on sentence boundary detection. In:Proceedings of the sixth conference on Applied natural language processing, pp.8489. Association for Computational Linguistics (2000)WiSeBE: Window-Based Sentence Boundary Evaluation13126. Stolcke, A., Shriberg, E.: Automatic linguistic segmentation of conversationalspeech. In: Proceedings of the Fourth International Conference on Spoken Language, 1996. ICSLP 1996, vol. 2, pp. 10051008. IEEE (1996)27. Strassel, S.: Simple metadata annotation specication v5. 0, linguistic data consortium (2003). http://www.ldc.upenn.edu/projects/MDE/Guidelines/SimpleMDEV5.0.pdf28. Tilk, O., Alumae, T.: Bidirectional recurrent neural network with attention mechanism for punctuation restoration. In: Interspeech 2016 (2016)29. Treviso, M.V., Shulby, C.D., Aluisio, S.M.: Evaluating word embeddings for sentence boundary detection in speech transcripts. arXiv preprint arXiv:1708.04704(2017)30. Ueng, N., Bisani, M., Vozila, P.: Improved models for automatic punctuationprediction for spoken and written text. In: Interspeech, pp. 30973101 (2013)31. Wang, W., Tur, G., Zheng, J., Ayan, N.F.: Automatic disuency removal forimproving spoken language translation. In: 2010 IEEE International Conference onAcoustics Speech and Signal Processing (ICASSP), pp. 52145217. IEEE (2010)32. Xu, C., Xie, L., Huang, G., Xiao, X., Chng, E.S., Li, H.: A deep neural networkapproach for sentence boundary detection in broadcast news. In: Fifteenth AnnualConference of the International Speech Communication Association (2014)33. Yu, D., Deng, L.: Automatic Speech Recognition. Springer, London (2015). https://doi.org/10.1007/978-1-4471-5779-3</conclusion><discussion></discussion><biblio>References1. Bohac, M., Blavka, K., Kucharova, M., Skodova, S.: Post-processing of the recognized speech for web presentation of large audio archive. In: 2012 35th InternationalConference on Telecommunications and Signal Processing (TSP), pp. 441445.IEEE (2012)2. Brum, H., Araujo, F., Kepler, F.: Sentiment analysis for Brazilian portuguese overa skewed class corpora. In: Silva, J., Ribeiro, R., Quaresma, P., Adami, A., Branco,A. (eds.) PROPOR 2016. LNCS (LNAI), vol. 9727, pp. 134138. Springer, Cham(2016). https://doi.org/10.1007/978-3-319-41552-9 143. Che, X., Wang, C., Yang, H., Meinel, C.: Punctuation prediction for unsegmentedtranscript based on word vector. In: LREC (2016)4. Fleiss, J.L.: Measuring nominal scale agreement among many raters. Psychol. Bull.76(5), 378 (1971)5. Fohr, D., Mella, O., Illina, I.: New paradigm in speech recognition: deep neural networks. In: IEEE International Conference on Information Systems and EconomicIntelligence (2017)6. Gonzalez-Gallardo, C.E., Hajjem, M., SanJuan, E., Torres-Moreno, J.M.: Transcripts informativeness study: an approach based on automatic summarization. In:Conference en Recherche dInformation et Applications (CORIA), Rennes, France,May (2018)7. Gonzalez-Gallardo, C.E., Torres-Moreno, J.M.: Sentence boundary detection forFrench with subword-level information vectors and convolutional neural networks.arXiv preprint arXiv:1802.04559 (2018)8. Gotoh, Y., Renals, S.: Sentence boundary detection in broadcast speech transcripts.In: ASR2000-Automatic Speech Recognition: Challenges for the new MilleniumISCA Tutorial and Research Workshop (ITRW) (2000)130C.-E. Gonzalez-Gallardo and J.-M. Torres-Moreno9. Hinton, G., et al.: Deep neural networks for acoustic modeling in speech recognition: the shared views of four research groups. IEEE Signal Process. Mag. 29(6),8297 (2012)10. Jamil, N., Ramli, M.I., Seman, N.: Sentence boundary detection without speechrecognition: a case of an under-resourced language. J. Electr. Syst. 11(3), 308318(2015)11. Kiss, T., Strunk, J.: Unsupervised multilingual sentence boundary detection. Comput. Linguist. 32(4), 485525 (2006)12. Klejch, O., Bell, P., Renals, S.: Punctuated transcription of multi-genre broadcastsusing acoustic and lexical approaches. In: 2016 IEEE Spoken Language TechnologyWorkshop (SLT), pp. 433440. IEEE (2016)13. Kolar, J., Lamel, L.: Development and evaluation of automatic punctuation forFrench and english speech-to-text. In: Thirteenth Annual Conference of the International Speech Communication Association (2012)14. Kolar, J., Svec, J., Psutka, J.: Automatic punctuation annotation in Czech broadcast news speech. In: SPECOM 2004 (2004)15. Liu, Y., Chawla, N.V., Harper, M.P., Shriberg, E., Stolcke, A.: A study in machinelearning from imbalanced data for sentence boundary detection in speech. Comput.Speech Lang. 20(4), 468494 (2006)16. Lu, W., Ng, H.T.: Better punctuation prediction with dynamic conditional random elds. In: Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing. pp. 177186. Association for Computational Linguistics(2010)17. Meteer, M., Iyer, R.: Modeling conversational speech for speech recognition. In:Conference on Empirical Methods in Natural Language Processing (1996)18. Mrozinski, J., Whittaker, E.W., Chatain, P., Furui, S.: Automatic sentence segmentation of speech for automatic summarization. In: 2006 IEEE InternationalConference on Acoustics Speech and Signal Processing Proceedings, vol. 1, p. I.IEEE (2006)19. Palmer, D.D., Hearst, M.A.: Adaptive sentence boundary disambiguation. In: Proceedings of the Fourth Conference on Applied Natural Language Processing, pp.7883. ANLC 1994. Association for Computational Linguistics, Stroudsburg, PA,USA (1994)20. Palmer, D.D., Hearst, M.A.: Adaptive multilingual sentence boundary disambiguation. Comput. Linguist. 23(2), 241267 (1997)21. Pearson, K.: Note on regression and inheritance in the case of two parents. Proc.R. Soc. Lond. 58, 240242 (1895)22. Peitz, S., Freitag, M., Ney, H.: Better punctuation prediction with hierarchicalphrase-based translation. In: Proceedings of the International Workshop on SpokenLanguage Translation (IWSLT), South Lake Tahoe, CA, USA (2014)23. Rott, M., Cerva, P.: Speech-to-text summarization using automatic phrase extraction from recognized text. In: Sojka, P., Horak, A., Kopecek, I., Pala, K. (eds.) TSD2016. LNCS (LNAI), vol. 9924, pp. 101108. Springer, Cham (2016). https://doi.org/10.1007/978-3-319-45510-5 1224. Shriberg, E., Stolcke, A.: Word predictability after hesitations: a corpus-basedstudy. In: Proceedings of the Fourth International Conference on Spoken Language,1996. ICSLP 1996, vol. 3, pp. 18681871. IEEE (1996)25. Stevenson, M., Gaizauskas, R.: Experiments on sentence boundary detection. In:Proceedings of the sixth conference on Applied natural language processing, pp.8489. Association for Computational Linguistics (2000)WiSeBE: Window-Based Sentence Boundary Evaluation13126. Stolcke, A., Shriberg, E.: Automatic linguistic segmentation of conversationalspeech. In: Proceedings of the Fourth International Conference on Spoken Language, 1996. ICSLP 1996, vol. 2, pp. 10051008. IEEE (1996)27. Strassel, S.: Simple metadata annotation specication v5. 0, linguistic data consortium (2003). http://www.ldc.upenn.edu/projects/MDE/Guidelines/SimpleMDEV5.0.pdf28. Tilk, O., Alumae, T.: Bidirectional recurrent neural network with attention mechanism for punctuation restoration. In: Interspeech 2016 (2016)29. Treviso, M.V., Shulby, C.D., Aluisio, S.M.: Evaluating word embeddings for sentence boundary detection in speech transcripts. arXiv preprint arXiv:1708.04704(2017)30. Ueng, N., Bisani, M., Vozila, P.: Improved models for automatic punctuationprediction for spoken and written text. In: Interspeech, pp. 30973101 (2013)31. Wang, W., Tur, G., Zheng, J., Ayan, N.F.: Automatic disuency removal forimproving spoken language translation. In: 2010 IEEE International Conference onAcoustics Speech and Signal Processing (ICASSP), pp. 52145217. IEEE (2010)32. Xu, C., Xie, L., Huang, G., Xiao, X., Chng, E.S., Li, H.: A deep neural networkapproach for sentence boundary detection in broadcast news. In: Fifteenth AnnualConference of the International Speech Communication Association (2014)33. Yu, D., Deng, L.: Automatic Speech Recognition. Springer, London (2015). https://doi.org/10.1007/978-1-4471-5779-3</biblio></article></xml>