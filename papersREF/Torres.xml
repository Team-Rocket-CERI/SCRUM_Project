<xml><article><preambule>Torres.pdf</preambule><titre>Summary Evaluation
</titre><auteur>Juan-Manuel Torres-Moreno, Horacio Saggion, Iria da Cunha, Eric SanJuan, and Patricia Vela&#769;zquez-Morales
</auteur><abstract>Abstract&#8212;We study a new content-based method for
the evaluation of text summarization systems without
human models which is used to produce system rankings.
The research is carried out using a new content-based
evaluation framework called F RESA to compute a variety of
divergences among probability distributions. We apply our
comparison framework to various well-established content-based
evaluation measures in text summarization such as C OVERAGE,
R ESPONSIVENESS, P YRAMIDS and ROUGE studying their
associations in various text summarization tasks including
generic multi-document summarization in English and French,
focus-based multi-document summarization in English and
generic single-document summarization in French and Spanish.
Index Terms&#8212;Text summarization evaluation, content-based
evaluation measures, divergences.

</abstract><introduction>I. I NTRODUCTIONTEXT summarization evaluation has always been acomplex and controversial issue in computationallinguistics. In the last decade, significant advances have beenmade in this field as well as various evaluation measures havebeen designed. Two evaluation campaigns have been led bythe U.S. agence DARPA. The first one, SUMMAC, ran from1996 to 1998 under the auspices of the Tipster program [1],and the second one, entitled DUC (Document UnderstandingConference) [2], was the main evaluation forum from 2000until 2007. Nowadays, the Text Analysis Conference (TAC)[3] provides a forum for assessment of different informationaccess technologies including text summarization.Evaluation in text summarization can be extrinsic orintrinsic [4]. In an extrinsic evaluation, the summaries areassessed in the context of an specific task carried out by ahuman or a machine. In an intrinsic evaluation, the summariesare evaluated in reference to some ideal model. SUMMACwas mainly extrinsic while DUC and TAC followed anintrinsic evaluation paradigm. In an intrinsic evaluation, anManuscript received June 8, 2010. Manuscript accepted for publication July25, 2010.Juan-Manuel Torres-Moreno is with LIA/Universite dAvignon,FranceandEcolePolytechniquedeMontreal,Canada(juan-manuel.torres@univ-avignon.fr).EricSanJuaniswithLIA/UniversitedAvignon,France(eric.sanjuan@univ-avignon.fr).Horacio Saggion is with DTIC/Universitat Pompeu Fabra, Spain(horacio.saggion@upf.edu).Iria da Cunha is with IULA/Universitat Pompeu Fabra, Spain;LIA/Universite dAvignon, France and Instituto de Ingeniera/UNAM, Mexico(iria.dacunha@upf.edu).PatriciaVelazquez-MoralesiswithVMLabs,France(patricia velazquez@yahoo.com).13Polibits (42) 2010Juan-Manuel Torres-Moreno, Horacio Saggion, Iria da Cunha, Eric SanJuan, and Patricia Velzquez-Moralesof models and the identification, matching, and weighting ofSCUs in both: models and peers.[12] evaluated the effectiveness of the Jensen-Shannon(J S) [13] theoretic measure in predicting systems ranksin two summarization tasks: query-focused and updatesummarization. They have shown that ranks producedby P YRAMIDS and those produced by J S measurecorrelate. However, they did not investigate the effectof the measure in summarization tasks such as genericmulti-document summarization (DUC 2004 Task 2),biographical summarization (DUC 2004 Task 5), opinionsummarization (TAC 2008 OS), and summarization inlanguages other than English.In this paper we present a series of experiments aimed ata better understanding of the value of the J S divergencefor ranking summarization systems. We have carried outexperimentation with the proposed measure and we haveverified that in certain tasks (such as those studied by[12]) there is a strong correlation among P YRAMIDS,R ESPONSIVENESS and the J S divergence, but as we willshow in this paper, there are datasets in which the correlationis not so strong. We also present experiments in Spanishand French showing positive correlation between the J Sand ROUGE which is the de facto evaluation measure usedin evaluation of non-English summarization. To the best ofour knowledge this is the more extensive set of experimentsinterpreting the value of evaluation without human models.The rest of the paper is organized in the following way:First in Section II we introduce related work in the area ofcontent-based evaluation identifying the departing point forour inquiry; then in Section III we explain the methodologyadopted in our work and the tools and resources used forexperimentation. In Section IV we present the experimentscarried out together with the results. Section V discusses theresults and Section VI concludes the paper and identifies futurework.non-random systems, no clear conclusion was reached on thevalue of each of the studied measures.Nowadays, a widespread summarization evaluationframework is ROUGE [14], which offers a set of statisticsthat compare peer summaries with models. It countsco-occurrences of n-grams in peer and models to derive ascore. There are several statistics depending on the usedn-grams and the text processing applied to the input texts(e.g., lemmatization, stop-word removal).[15] proposed a method of evaluation based on theuse of distances or divergences between two probabilitydistributions (the distribution of units in the automaticsummary and the distribution of units in the modelsummary). They studied two different Information Theoreticmeasures of divergence: the Kullback-Leibler (KL) [16] andJensen-Shannon (J S) [13] divergences. KL computes thedivergence between probability distributions P and Q in thefollowing way:Pw1XPw log2(1)DKL (P ||Q) =</introduction><corps>2 wQwWhile J S divergence is defined as follows:1X2Pw2QwDJ S (P ||Q) =Pw log2+ Qw log22 wPw + QwPw + Qw(2)These measures can be applied to the distribution of units insystem summaries P and reference summaries Q. The valueobtained may be used as a score for the system summary. Themethod has been tested by [15] over the DUC 2002 corpus forsingle and multi-document summarization tasks showing goodcorrelation among divergence measures and both coverage andROUGE rankings.[12] went even further and, as in [5], they proposed tocompare directly the distribution of words in full documentswith the distribution of words in automatic summaries toderive a content-based evaluation measure. They found ahigh correlation between rankings produced using modelsand rankings produced without models. This last work is thedeparting point for our inquiry into the value of measures thatdo not rely on human models.II. R ELATED W ORKOne of the first works to use content-based measures intext summarization evaluation is due to [5], who presented anevaluation framework to compare rankings of summarizationsystems produced by recall and cosine-based measures. Theyshowed that there was weak correlation among rankingsproduced by recall, but that content-based measures producerankings which were strongly correlated. This put forwardthe idea of using directly the full document for comparisonpurposes in text summarization evaluation. [6] presented aset of evaluation measures based on the notion of vocabularyoverlap including n-gram overlap, cosine similarity, andlongest common subsequence, and they applied them tomulti-document summarization in English and Chinese.However, they did not evaluate the performance of themeasures in different summarization tasks. [7] also comparedvarious evaluation measures based on vocabulary overlap.Although these measures were able to separate random fromPolibits (42) 2010III. M ETHODOLOGYThe followed methodology in this paper mirrors the oneadopted in past work (e.g. [5], [7], [12]). Given a particularsummarization task T , p data points to be summarizedp1with input material {Ii }i=0(e.g. document(s), question(s),s1topic(s)), s peer summaries {SUMi,k }k=0for input i, andm1m model summaries {MODELi,j }j=0 for input i, we willcompare rankings of the s peer summaries produced by variousevaluation measures. Some measures that we use comparesummaries with n of the m models:MEASUREM (SUMi,k , {MODELi,j }n1j=0 )14(3)Summary Evaluation with and without Referenceswhile other measures compare peers with all or some of theinput material:MEASUREM (SUMi,k , Ii0 )3) Update-summarization task that consists of creating asummary out of a cluster of documents and a topic. Twosub-tasks are considered here: A) an initial summary hasto be produced based on an initial set of documents andtopic; B) an update summary has to be produced froma different (but related) cluster assuming documentsused in A) are known. The English TAC08 UpdateSummarization dataset is used, which consists of 48topics with 20 documents each  36,911 words.4) Opinion summarization where systems have to analyzea set of blog articles and summarize the opinionsabout a target in the articles. The TAC08 OpinionSummarization in English4 data set (taken from theBlogs06 Text Collection) is used: 25 clusters and targets(i.e., target entity and questions) were used  1,167,735words.5) Generic single-document summarization in Spanishusing the Medicina Clnica5 corpus, which is composedof 50 medical articles in Spanish, each one with itscorresponding author abstract  124,929 words.6) Generic single document summarization in French usingthe Canadien French Sociological Articles corpusfrom the journal Perspectives interdisciplinaires sur letravail et la sante (PISTES)6 . It contains 50 sociologicalarticles in French, each one with its correspondingauthor abstract  381,039 words.7) Generic multi-document-summarization in French usingdata from the RPM27 corpus [18], 20 different themesconsisting of 10 articles and 4 abstracts by referencethematic  185,223 words.(4)where Ii0 is some subset of input Ii . The values producedby the measures for each summary SUMi,k are averagedfor each system k = 0, . . . , s  1 and these averages areused to produce a ranking. Rankings are then comparedusing Spearman Rank correlation [17] which is used tomeasure the degree of association between two variableswhose values are used to rank objects. We have chosento use this correlation to compare directly results to thosepresented in [12]. Computation of correlations is done usingthe Statistics-RankCorrelation-0.12 package1 , which computesthe rank correlation between two vectors. We also verifiedthe good conformity of the results with the correlation testof Kendall  calculated with the statistical software R. Thetwo nonparametric tests of Spearman and Kendall do notreally stand out as the treatment of ex-quo. The goodcorrespondence between the two tests shows that they do notintroduce bias in our analysis. Subsequently will mention onlythe  of Sperman more widely used in this field.A. ToolsWe carry out experimentation using a new summarizationevaluation framework: F RESA FRamework for EvaluatingSummaries Automatically, which includes document-basedsummary evaluation measures based on probabilitiesdistribution2 . As in the ROUGE package, F RESA supportsdifferent n-grams and skip n-grams probability distributions.The F RESA environment can be used in the evaluation ofsummaries in English, French, Spanish and Catalan, and itintegrates filtering and lemmatization in the treatment ofsummaries and documents. It is developed in Perl and willbe made publicly available. We also use the ROUGE package[10] to compute various ROUGE statistics in new datasets.For experimentation in the TAC and the DUC datasets we usedirectly the peer summaries produced by systems participatingin the evaluations. For experimentation in Spanish and French(single and multi-document summarization) we have createdsummaries at a similar ratio to those of reference using thefollowing systems: ENERTEX [19], a summarizer based on a theory oftextual energy; CORTEX [20], a single-document sentence extractionsystem for Spanish and French that combines variousstatistical measures of relevance (angle between sentenceand topic, various Hamming weights for sentences, etc.)and applies an optimal decision algorithm for sentenceselection; SUMMTERM [21], a terminology-based summarizer thatis used for summarization of medical articles anduses specialized terminology for scoring and rankingsentences; REG [22], summarization system based on an greedyalgorithm;B. Summarization Tasks and Data SetsWe have conducted our experimentation with the followingsummarization tasks and data sets:1) Generic multi-document-summarization in English(production of a short summary of a cluster of relateddocuments) using data from DUC043 , task 2: 50clusters, 10 documents each  294,636 words.2) Focused-based summarization in English (production ofa short focused multi-document summary focused on thequestion who is X?, where X is a persons name) usingdata from the DUC04 task 5: 50 clusters, 10 documentseach plus a target person name  284,440 words.4 http://www.nist.gov/tac/data/index.html1 http://search.cpan.org/gene/Statistics-RankCorrelation-0.12/5 http://www.elsevier.es/revistas/ctl2 F RESAis available at: http://lia.univavignon.fr/fileadmin/axes/TALNE/Ressources.html3 http://www-nlpir.nist.gov/projects/duc/guidelines/2004.htmlservlet? f=7032&amp;revistaid=26 http://www.pistes.uqam.ca/7 http://www-labs.sinequa.com/rpm215Polibits (42) 2010Juan-Manuel Torres-Moreno, Horacio Saggion, Iria da Cunha, Eric SanJuan, and Patricia Velzquez-Morales J S summarizer, a summarization system that scoresand ranks sentences according to their Jensen-Shannondivergence to the source document; a lead-based summarization system that selects the leadsentences of the document; a random-based summarization system that selectssentences at random; Open Text Summarizer [23], a multi-lingual summarizerbased on the frequency and commercial systems: Word, SSSummarizer8 , Pertinence9and Copernic10 .presented here we used uni-grams, 2-grams, and the skip2-grams with maximum skip distance of 4 (ROUGE-1,ROUGE-2 and ROUGE-SU4). ROUGE is used to comparea peer summary to a set of model summaries in ourframework (as indicated in equation 3). Jensen-Shannon divergence formula given in Equation 2is implemented in our F RESA package with the followingspecification (Equation 6) for the probability distributionof words w.CTPw = wN(SCwifwSNSQw =(6)TCw+otherwiseN +BC. Evaluation MeasuresThe following measures derived from human assessment ofthe content of the summaries are used in our experiments: C OVERAGE is understood as the degree to which onepeer summary conveys the same information as a modelsummary [2]. C OVERAGE was used in DUC evaluations.This measure is used as indicated in equation 3 usinghuman references or models. R ESPONSIVENESS ranks summaries in a 5-point scaleindicating how well the summary satisfied a giveninformation need [2]. It is used in focused-basedsummarization tasks. This measure is used as indicatedin equation 4 since a human judges the summarywith respect to a given input user need (e.g., aquestion). R ESPONSIVENESS was used in DUC and TACevaluations. P YRAMIDS [11] is a content assessment measure whichcompares content units in a peer summary to weightedcontent units in a set of model summaries. Thismeasure is used as indicated in equation 3 using humanreferences or models. P YRAMIDS is the adopted metricfor content-based evaluation in the TAC evaluations.For DUC and TAC datasets the values of these measures areavailable and we used them directly. We used the followingautomatic evaluation measures in our experiments: ROUGE [14], which is a recall metric that takes intoaccount n-grams as units of content for comparing peerand model summaries. The ROUGE formula specified in[10] is as follows:PmROUGE-n(R, M ) =P M ngramP countmatch (n  gram)PPcount(n-gram)m MWhere P is the probability distribution of words w intext T and Q is the probability distribution of words win summary S; N is the number of words in text andTsummary N = NT +NS , B = 1.5|V |, Cwis the numberSof words in the text and Cw is the number of words inthe summary. For smoothing the summarys probabilitieswe have used  = 0.005. We have also implementedother smoothing approaches (e.g. Good-Turing [24], thatuses the CPAN Perls Statistics-Smoothing-SGT-2.1.2package11 ) in F RESA, but we do not use them inthe experiments reported here. Following the ROUGEapproach, in addition to word uni-grams we use 2-gramsand skip n-grams computing divergences such as J S(using uni-grams) J S 2 (using 2-grams), J S 4 (using theskip n-grams of ROUGE-SU4), and J S M which is anaverage of the J S i . J Ss measures are used to compare apeer summary to its source document(s) in our framework(as indicated in equation 4). In the case of summarizationof multiple documents, these are concatenated (in thegiven input order) to form a single input from whichprobabilities are computed.IV. E XPERIMENTS AND R ESULTSWe first replicated the experiments presented in [12] toverify that our implementation of J S produced correlationresults compatible with that work. We used the TAC08Update Summarization data set and computed J S andROUGE measures for each peer summary. We producedtwo system rankings (one for each measure), which werecompared to rankings produced using the manual P YRAMIDSand R ESPONSIVENESS scores. Spearman correlations werecomputed among the different rankings. The results arepresented in Table I. These results confirm a high correlationamong P YRAMIDS, R ESPONSIVENESS and J S. We alsoverified high correlation between J S and ROUGE-2 (0.83Spearman correlation, not shown in the table) in this task anddataset.Then, we experimented with data from DUC04, TAC08Opinion Summarization pilot task as well as single and(5)where R is the summary to be evaluated, M is the set ofmodel (human) summaries, countmatch is the number ofcommon n-grams in m and P , and count is the numberof n-grams in the model summaries. For the experiments8 http://www.kryltech.com/summarizer.htm9 http://www.pertinence.net11 http://search.cpan.org/bjoernw/Statistics-Smoothing-SGT-2.1.2/10 http://www.copernic.com/en/products/summarizerPolibits (42) 201016Summary Evaluation with and without ReferencesTABLE IS PEARMANMesureROUGE-2JSCORRELATION OF CONTENT- BASED MEASURES INU PDATE S UMMARIZATION TASKP YRAMIDS0.960.85p-valuep &lt; 0.005p &lt; 0.005R ESPONSIVENESS0.920.74evaluation metrics that do not rely on human models but thatcompare summary content to input content directly [12]. Wehave some positive and some negative results regarding thedirect use of the full document in content-based evaluation.We have verified that in both generic muti-documentsummarizationandintopic-basedmulti-documentsummarization in English correlation among measuresthat use human models (P YRAMIDS, R ESPONSIVENESSand ROUGE) and a measure that does not use models(J S divergence) is strong. We have found that correlationamong the same measures is weak for summarization ofbiographical information and summarization of opinions inblogs. We believe that in these cases content-based measuresshould be considered, in addition to the input document, thesummarization task (i.e. text-based representation, description)to better assess the content of the peers [25], the task being adeterminant factor in the selection of content for the summary.Our multi-lingual experiments in generic single-documentsummarization confirm a strong correlation among theJ S divergence and ROUGE measures. It is worth notingthat ROUGE is in general the chosen framework forpresenting content-based evaluation results in non-Englishsummarization.For the experiments in Spanish, we are conscious that weonly have one model summary to compare with the peers.Nevertheless, these models are the corresponding abstractswritten by the authors. As the experiments in [26] show, theprofessionals of a specialized domain (as, for example, themedical domain) adopt similar strategies to summarize theirtexts and they tend to choose roughly the same content chunksfor their summaries. Previous studies have shown that authorabstracts are able to reformulate content with fidelity [27] andthese abstracts are ideal candidates for comparison purposes.Because of this, the summary of the author of a medical articlecan be taken as reference for summaries evaluation. It is worthnoting that there is still debate on the number of models to beused in summarization evaluation [28]. In the French corpusPISTES, we suspect the situation is similar to the Spanishcase.TAC08p-valuep &lt; 0.005p &lt; 0.005multi-document summarization in Spanish and French. In spiteof the fact that the experiments for French and Spanish corporause less data points (i.e., less summarizers per task) thanfor English, results are still quite significant. For DUC04,we computed the J S measure for each peer summary intasks 2 and 5 and we used J S, ROUGE, C OVERAGE andR ESPONSIVENESS scores to produce systems rankings. Thevarious Spearmans rank correlation values for DUC04 arepresented in Tables II (for task 2) and III (for task 5).For task 2, we have verified a strong correlation betweenJ S and C OVERAGE. For task 5, the correlation betweenJ S and C OVERAGE is weak, and that between J S andR ESPONSIVENESS is weak and negative.Although the Opinion Summarization (OS) task is a newtype of summarization task and its evaluation is a complicatedissue, we have decided to compare J S rankings with thoseobtained using P YRAMIDS and R ESPONSIVENESS in TAC08.Spearmans correlation values are listed in Table IV. As it canbe seen, there is weak and negative correlation of J S withboth P YRAMIDS and R ESPONSIVENESS. Correlation betweenP YRAMIDS and R ESPONSIVENESS rankings is high for thistask (0.71 Spearmans correlation value).For experimentation in mono-document summarizationin Spanish and French, we have run 11 multi-lingualsummarization systems; for experimentation in French, wehave run 12 systems. In both cases, we have producedsummaries at a compression rate close to the compression rateof the authors provided abstracts. We have then computed J Sand ROUGE measures for each summary and we have averagedthe measures values for each system. These averages wereused to produce rankings per each measure. We computedSpearmans correlations for all pairs of rankings.Results are presented in Tables V, VI and VII. All resultsshow medium to strong correlation between the J S measuresand ROUGE measures. However the J S measure based onuni-grams has lower correlation than J Ss which use n-gramsof higher order. Note that table VII presents results forgeneric multi-document summarization in French, in thiscase correlation scores are lower than correlation scores forsingle-document summarization in French, a result which maybe expected given the diversity of input in multi-documentsummarization.VI. C ONCLUSIONS AND F UTURE W ORKThis paper has presented a series of experiments incontent-based measures that do not rely on the use of modelsummaries for comparison purposes. We have carried outextensive experimentation with different summarization tasksdrawing a clearer picture of tasks where the measures couldbe applied. This paper makes the following contributions: We have shown that if we are only interested in rankingsummarization systems according to the content of theirautomatic summaries, there are tasks were models couldbe subtituted by the full document in the computation ofthe J S measure obtaining reliable rankings. However,we have also found that the substitution of modelsby full-documents is not always advisable. We haveV. D ISCUSSIONThe departing point for our inquiry into text summarizationevaluation has been recent work on the use of content-based17Polibits (42) 2010Juan-Manuel Torres-Moreno, Horacio Saggion, Iria da Cunha, Eric SanJuan, and Patricia Velzquez-MoralesTABLE IIS PEARMAN  OF CONTENT- BASED MEASURES WITH C OVERAGE IN DUC04 TASK 2MesureROUGE-2JSC OVERAGE0.790.68p-valuep &lt; 0.0050p &lt; 0.0025TABLE IIIS PEARMAN  OF CONTENT- BASED MEASURES IN DUC04 TASK 5MesureROUGE-2JSC OVERAGE0.780.40p-valuep &lt; 0.001p &lt; 0.050R ESPONSIVENESS0.44-0.18p-valuep &lt; 0.05p &lt; 0.25TABLE IVS PEARMAN  OF CONTENT- BASED MEASURES IN TAC08 OSMesureJSP YRAMIDS-0.13p-valuep &lt; 0.25R ESPONSIVENESS-0.14TASKp-valuep &lt; 0.25TABLE VS PEARMAN  OF CONTENT- BASED MEASURES WITH ROUGE IN THE Medicina Clnica C ORPUS (S PANISH )MesureJSJ S2J S4J SMROUGE -10.560.880.880.82p-valuep &lt; 0.100p &lt; 0.001p &lt; 0.001p &lt; 0.005ROUGE -20.460.800.800.71ROUGE -SU40.450.810.810.71p-valuep &lt; 0.200p &lt; 0.005p &lt; 0.005p &lt; 0.010a representation of the task/topic in the calculation ofmeasures. To carry out these comparisons, however, we aredependent on the existence of references.F RESA will also be used in the new question-answer taskcampaign INEX2010 (http://www.inex.otago.ac.nz/tracks/qa/qa.asp) for the evaluation of long answers. This task aimsto answer a question by extraction and agglomeration ofsentences in Wikipedia. This kind of task correspondsto those for which we have found a high correlationamong the measures J S and evaluation methods withhuman intervention. Moreover, the J S calculation will beamong the summaries produced and a representative set ofrelevant passages from Wikipedia. F RESA will be used tocompare three types of systems, although different tasks: themulti-document summarizer guided by a query, the searchsystems targeted information (focused IR) and the questionanswering systems.found weak correlation among different rankings incomplex summarization tasks such as the summarizationof biographical information and the summarization ofopinions. We have also carried out large-scale experiments inSpanish and French which show positive medium tostrong correlation among systems ranks produced byROUGE and divergence measures that do not use themodel summaries. We have also presented a new framework, F RESA, forthe computation of measures based on J S divergence.Following the ROUGE approach, F RESA package useword uni-grams, 2-grams and skip n-grams computingdivergences. This framework will be available to thecommunity for research purposes.Although we have made a number of contributions, this paperleaves many open questions than need to be addressed. Inorder to verify correlation between ROUGE and J S, in theshort term we intend to extend our investigation to otherlanguages such as Portuguese and Chinesse for which wehave access to data and summarization technology. We alsoplan to apply F RESA to the rest of the DUC and TACsummarization tasks, by using several smoothing techniques.As a novel idea, we contemplate the possibility of adaptingthe evaluation framework for the phrase compression task[29], which, to our knowledge, does not have an efficientevaluation measure. The main idea is to calculate J S froman automatically-compressed sentence taking the completesentence by reference. In the long term, we plan to incorporatePolibits (42) 2010p-valuep &lt; 0.100p &lt; 0.002p &lt; 0.002p &lt; 0.020ACKNOWLEDGMENTWe are grateful to the Programa Ramon y Cajal fromMinisterio de Ciencia e Innovacion, Spain. This work ispartially supported by: a postdoctoral grant from the NationalProgram for Mobility of Research Human Resources (NationalPlan of Scientific Research, Development and Innovation2008-2011, Ministerio de Ciencia e Innovacion, Spain); theresearch project CONACyT, number 82050, and the researchproject PAPIIT-DGAPA (Universidad Nacional Autonoma deMexico), number IN403108.18Summary Evaluation with and without ReferencesTABLE VIS PEARMAN  OF CONTENT- BASED MEASURES WITH ROUGE IN THE PISTES C ORPUS (F RENCH )MesureJSJ S2J S4J SMROUGE -10.700.930.830.88p-valuep &lt; 0.050p &lt; 0.002p &lt; 0.020p &lt; 0.010ROUGE -20.730.860.760.83p-valuep &lt; 0.05p &lt; 0.01p &lt; 0.05p &lt; 0.02ROUGE -SU40.730.860.760.83p-valuep &lt; 0.500p &lt; 0.005p &lt; 0.050p &lt; 0.010TABLE VIIS PEARMAN  OF CONTENT- BASED MEASURES WITH ROUGE IN THE RPM2 C ORPUS (F RENCH )MeasureJSJ S2J S4J SMROUGE -10.8300.8000.7500.850p-valuep &lt; 0.002p &lt; 0.005p &lt; 0.010p &lt; 0.002ROUGE -20.6600.5900.5200.640R EFERENCESp-valuep &lt; 0.05p &lt; 0.05p &lt; 0.10p &lt; 0.05ROUGE -SU40.7410.6800.6200.740p-valuep &lt; 0.01p &lt; 0.02p &lt; 0.05p &lt; 0.01[18] C. de Loupy, M. Guegan, C. Ayache, S. Seng, and J.-M. Torres-Moreno,A French Human Reference Corpus for multi-documentssummarization and sentence compression, in LREC10, vol. 2,Malta, 2010, p. In press.[19] S. Fernandez, E. SanJuan, and J.-M. Torres-Moreno, Textual Energyof Associative Memories: performants applications of Enertex algorithmin text summarization and topic segmentation, in MICAI07, 2007, pp.861871.[20] J.-M. Torres-Moreno, P. Velazquez-Morales, and J.-G. Meunier,Condenses de textes par des methodes numeriques, in JADT02, vol. 2,St Malo, France, 2002, pp. 723734.[21] J. Vivaldi, I. da Cunha, J.-M. Torres-Moreno, and P. Velazquez-Morales,Automatic summarization using terminological and semanticresources, in LREC10, vol. 2, Malta, 2010, p. In press.[22] J.-M. Torres-Moreno and J. Ramirez, REG : un algorithme gloutonapplique au resume automatique de texte, in JADT10. Rome, 2010,p. In press.[23] V. Yatsko and T. Vishnyakov, A method for evaluating modernsystems of automatic text summarization, Automatic Documentationand Mathematical Linguistics, vol. 41, no. 3, pp. 93103, 2007.[24] C. D. Manning and H. Schutze, Foundations of Statistical NaturalLanguage Processing.Cambridge, Massachusetts: The MIT Press,1999.[25] K. Sparck Jones, Automatic summarising: The state of the art, IPM,vol. 43, no. 6, pp. 14491481, 2007.[26] I. da Cunha, L. Wanner, and M. T. Cabre, Summarization of specializeddiscourse: The case of medical articles in spanish, Terminology, vol. 13,no. 2, pp. 249286, 2007.[27] C.-K. Chuah, Types of lexical substitution in abstracting, in ACLStudent Research Workshop.Toulouse, France: Association forComputational Linguistics, 9-11 July 2001 2001, pp. 4954.[28] K. Owkzarzak and H. T. Dang, Evaluation of automatic summaries:Metrics under varying data conditions, in UCNLG+Sum09, Suntec,Singapore, August 2009, pp. 2330.[29] K. Knight and D. Marcu, Statistics-based summarization-step one:Sentence compression, in Proceedings of the National Conference onArtificial Intelligence. Menlo Park, CA; Cambridge, MA; London;AAAI Press; MIT Press; 1999, 2000, pp. 703710.[1] I. Mani, G. Klein, D. House, L. Hirschman, T. Firmin, andB. Sundheim, Summac: a text summarization evaluation, NaturalLanguage Engineering, vol. 8, no. 1, pp. 4368, 2002.[2] P. Over, H. Dang, and D. Harman, DUC in context, IPM, vol. 43,no. 6, pp. 15061520, 2007.[3] Proceedings of the Text Analysis Conference. Gaithesburg, Maryland,USA: NIST, November 17-19 2008.[4] K. Sparck Jones and J. Galliers, Evaluating Natural LanguageProcessing Systems, An Analysis and Review, ser. Lecture Notes inComputer Science. Springer, 1996, vol. 1083.[5] R. L. Donaway, K. W. Drummey, and L. A. Mather, A comparison ofrankings produced by summarization evaluation measures, in NAACLWorkshop on Automatic Summarization, 2000, pp. 6978.[6] H. Saggion, D. Radev, S. Teufel, and W. Lam, Meta-evaluationof Summaries in a Cross-lingual Environment using Content-basedMetrics, in COLING 2002, Taipei, Taiwan, August 2002, pp. 849855.[7] D. R. Radev, S. Teufel, H. Saggion, W. Lam, J. Blitzer, H. Qi, A. Celebi,D. Liu, and E. Drabek, Evaluation challenges in large-scale documentsummarization, in ACL03, 2003, pp. 375382.[8] K. Papineni, S. Roukos, T. Ward, , and W. J. Zhu, BLEU: a methodfor automatic evaluation of machine translation, in ACL02, 2002, pp.311318.[9] K. Pastra and H. Saggion, Colouring summaries BLEU, in EvaluationInitiatives in Natural Language Processing. Budapest, Hungary: EACL,14 April 2003.[10] C.-Y. Lin, ROUGE: A Package for Automatic Evaluation ofSummaries, in Text Summarization Branches Out: ACL-04 Workshop,M.-F. Moens and S. Szpakowicz, Eds., Barcelona, July 2004, pp. 7481.[11] A. Nenkova and R. J. Passonneau, Evaluating Content Selection inSummarization: The Pyramid Method, in HLT-NAACL, 2004, pp.145152.[12] A. Louis and A. Nenkova, Automatically Evaluating Content Selectionin Summarization without Human Models, in Empirical Methods inNatural Language Processing, Singapore, August 2009, pp. 306314.[Online]. Available: http://www.aclweb.org/anthology/D/D09/D09-1032[13] J. Lin, Divergence Measures based on the Shannon Entropy, IEEETransactions on Information Theory, vol. 37, no. 145-151, 1991.[14] C.-Y. Lin and E. Hovy, Automatic Evaluation of Summaries UsingN-gram Co-occurrence Statistics, in HLT-NAACL. Morristown, NJ,USA: Association for Computational Linguistics, 2003, pp. 7178.[15] C.-Y. Lin, G. Cao, J. Gao, and J.-Y. Nie, An information-theoreticapproach to automatic evaluation of summaries, in HLT-NAACL,Morristown, USA, 2006, pp. 463470.[16] S. Kullback and R. Leibler, On information and sufficiency, Ann. ofMath. Stat., vol. 22, no. 1, pp. 7986, 1951.[17] S. Siegel and N. Castellan, Nonparametric Statistics for the BehavioralSciences. McGraw-Hill, 1998.19Polibits (42) 2010</corps><conclusion></conclusion><discussion></discussion><biblio>with and without ReferencesJuan-Manuel Torres-Moreno, Horacio Saggion, Iria da Cunha, Eric SanJuan, and Patricia Velazquez-Moralesautomatically generated summary (peer) has to be comparedwith one or more reference summaries (models). DUC usedan interface called SEE to allow human judges to comparea peer with a model. Thus, judges give a C OVERAGE scoreto each peer produced by a system and the final systemC OVERAGE score is the average of the C OVERAGEs scoresasigned. These systems C OVERAGE scores can then be usedto rank summarization systems. In the case of query-focusedsummarization (e.g. when the summary should answer aquestion or series of questions) a R ESPONSIVENESS scoreis also assigned to each summary, which indicates howresponsive the summary is to the question(s).Because manual comparison of peer summaries with modelsummaries is an arduous and costly process, a body ofresearch has been produced in the last decade on automaticcontent-based evaluation procedures. Early studies used textsimilarity measures such as cosine similarity (with or withoutweighting schema) to compare peer and model summaries[5]. Various vocabulary overlap measures such as n-gramsoverlap or longest common subsequence between peer andmodel have also been proposed [6], [7]. The B LEU machinetranslation evaluation measure [8] has also been tested insummarization [9]. The DUC conferences adopted the ROUGEpackage for content-based evaluation [10]. ROUGE implementsa series of recall measures based on n-gram co-occurrencebetween a peer summary and a set of model summaries. Thesemeasures are used to produce systems rank. It has been shownthat system rankings, produced by some ROUGE measures(e.g., ROUGE-2, which uses 2-grams), have a correlation withrankings produced using C OVERAGE.In recent years the P YRAMIDS evaluation method [11] hasbeen introduced. It is based on the distribution of contentof a set of model summaries. Summary Content Units (SCUs)are first identified in the model summaries, then each SCUreceives a weight which is the number of models containingor expressing the same unit. Peer SCUs are identified in thepeer, matched against model SCUs, and weighted accordingly.The P YRAMIDS score given to a peer is the ratio of the sumof the weights of its units and the sum of the weights of thebest possible ideal summary with the same number of SCUs asthe peer. The P YRAMIDS scores can be also used for rankingsummarization systems. [11] showed that P YRAMIDS scoresproduced reliable system rankings when multiple (4 or more)models were used and that P YRAMIDS rankings correlate withrankings produced by ROUGE-2 and ROUGE-SU2 (i.e. ROUGEwith skip 2-grams). However, this method requires the creationAbstractWe study a new content-based method forthe evaluation of text summarization systems withouthuman models which is used to produce system rankings.The research is carried out using a new content-basedevaluation framework called F RESA to compute a variety ofdivergences among probability distributions. We apply ourcomparison framework to various well-established content-basedevaluation measures in text summarization such as C OVERAGE,R ESPONSIVENESS, P YRAMIDS and ROUGE studying theirassociations in various text summarization tasks includinggeneric multi-document summarization in English and French,focus-based multi-document summarization in English andgeneric single-document summarization in French and Spanish.Index TermsText summarization evaluation, content-basedevaluation measures, divergences.I. I NTRODUCTIONTEXT summarization evaluation has always been acomplex and controversial issue in computationallinguistics. In the last decade, significant advances have beenmade in this field as well as various evaluation measures havebeen designed. Two evaluation campaigns have been led bythe U.S. agence DARPA. The first one, SUMMAC, ran from1996 to 1998 under the auspices of the Tipster program [1],and the second one, entitled DUC (Document UnderstandingConference) [2], was the main evaluation forum from 2000until 2007. Nowadays, the Text Analysis Conference (TAC)[3] provides a forum for assessment of different informationaccess technologies including text summarization.Evaluation in text summarization can be extrinsic orintrinsic [4]. In an extrinsic evaluation, the summaries areassessed in the context of an specific task carried out by ahuman or a machine. In an intrinsic evaluation, the summariesare evaluated in reference to some ideal model. SUMMACwas mainly extrinsic while DUC and TAC followed anintrinsic evaluation paradigm. In an intrinsic evaluation, anManuscript received June 8, 2010. Manuscript accepted for publication July25, 2010.Juan-Manuel Torres-Moreno is with LIA/Universite dAvignon,FranceandEcolePolytechniquedeMontreal,Canada(juan-manuel.torres@univ-avignon.fr).EricSanJuaniswithLIA/UniversitedAvignon,France(eric.sanjuan@univ-avignon.fr).Horacio Saggion is with DTIC/Universitat Pompeu Fabra, Spain(horacio.saggion@upf.edu).Iria da Cunha is with IULA/Universitat Pompeu Fabra, Spain;LIA/Universite dAvignon, France and Instituto de Ingeniera/UNAM, Mexico(iria.dacunha@upf.edu).PatriciaVelazquez-MoralesiswithVMLabs,France(patricia velazquez@yahoo.com).13Polibits (42) 2010Juan-Manuel Torres-Moreno, Horacio Saggion, Iria da Cunha, Eric SanJuan, and Patricia Velzquez-Moralesof models and the identification, matching, and weighting ofSCUs in both: models and peers.[12] evaluated the effectiveness of the Jensen-Shannon(J S) [13] theoretic measure in predicting systems ranksin two summarization tasks: query-focused and updatesummarization. They have shown that ranks producedby P YRAMIDS and those produced by J S measurecorrelate. However, they did not investigate the effectof the measure in summarization tasks such as genericmulti-document summarization (DUC 2004 Task 2),biographical summarization (DUC 2004 Task 5), opinionsummarization (TAC 2008 OS), and summarization inlanguages other than English.In this paper we present a series of experiments aimed ata better understanding of the value of the J S divergencefor ranking summarization systems. We have carried outexperimentation with the proposed measure and we haveverified that in certain tasks (such as those studied by[12]) there is a strong correlation among P YRAMIDS,R ESPONSIVENESS and the J S divergence, but as we willshow in this paper, there are datasets in which the correlationis not so strong. We also present experiments in Spanishand French showing positive correlation between the J Sand ROUGE which is the de facto evaluation measure usedin evaluation of non-English summarization. To the best ofour knowledge this is the more extensive set of experimentsinterpreting the value of evaluation without human models.The rest of the paper is organized in the following way:First in Section II we introduce related work in the area ofcontent-based evaluation identifying the departing point forour inquiry; then in Section III we explain the methodologyadopted in our work and the tools and resources used forexperimentation. In Section IV we present the experimentscarried out together with the results. Section V discusses theresults and Section VI concludes the paper and identifies futurework.non-random systems, no clear conclusion was reached on thevalue of each of the studied measures.Nowadays, a widespread summarization evaluationframework is ROUGE [14], which offers a set of statisticsthat compare peer summaries with models. It countsco-occurrences of n-grams in peer and models to derive ascore. There are several statistics depending on the usedn-grams and the text processing applied to the input texts(e.g., lemmatization, stop-word removal).[15] proposed a method of evaluation based on theuse of distances or divergences between two probabilitydistributions (the distribution of units in the automaticsummary and the distribution of units in the modelsummary). They studied two different Information Theoreticmeasures of divergence: the Kullback-Leibler (KL) [16] andJensen-Shannon (J S) [13] divergences. KL computes thedivergence between probability distributions P and Q in thefollowing way:Pw1XPw log2(1)DKL (P ||Q) =2 wQwWhile J S divergence is defined as follows:1X2Pw2QwDJ S (P ||Q) =Pw log2+ Qw log22 wPw + QwPw + Qw(2)These measures can be applied to the distribution of units insystem summaries P and reference summaries Q. The valueobtained may be used as a score for the system summary. Themethod has been tested by [15] over the DUC 2002 corpus forsingle and multi-document summarization tasks showing goodcorrelation among divergence measures and both coverage andROUGE rankings.[12] went even further and, as in [5], they proposed tocompare directly the distribution of words in full documentswith the distribution of words in automatic summaries toderive a content-based evaluation measure. They found ahigh correlation between rankings produced using modelsand rankings produced without models. This last work is thedeparting point for our inquiry into the value of measures thatdo not rely on human models.II. R ELATED W ORKOne of the first works to use content-based measures intext summarization evaluation is due to [5], who presented anevaluation framework to compare rankings of summarizationsystems produced by recall and cosine-based measures. Theyshowed that there was weak correlation among rankingsproduced by recall, but that content-based measures producerankings which were strongly correlated. This put forwardthe idea of using directly the full document for comparisonpurposes in text summarization evaluation. [6] presented aset of evaluation measures based on the notion of vocabularyoverlap including n-gram overlap, cosine similarity, andlongest common subsequence, and they applied them tomulti-document summarization in English and Chinese.However, they did not evaluate the performance of themeasures in different summarization tasks. [7] also comparedvarious evaluation measures based on vocabulary overlap.Although these measures were able to separate random fromPolibits (42) 2010III. M ETHODOLOGYThe followed methodology in this paper mirrors the oneadopted in past work (e.g. [5], [7], [12]). Given a particularsummarization task T , p data points to be summarizedp1with input material {Ii }i=0(e.g. document(s), question(s),s1topic(s)), s peer summaries {SUMi,k }k=0for input i, andm1m model summaries {MODELi,j }j=0 for input i, we willcompare rankings of the s peer summaries produced by variousevaluation measures. Some measures that we use comparesummaries with n of the m models:MEASUREM (SUMi,k , {MODELi,j }n1j=0 )14(3)Summary Evaluation with and without Referenceswhile other measures compare peers with all or some of theinput material:MEASUREM (SUMi,k , Ii0 )3) Update-summarization task that consists of creating asummary out of a cluster of documents and a topic. Twosub-tasks are considered here: A) an initial summary hasto be produced based on an initial set of documents andtopic; B) an update summary has to be produced froma different (but related) cluster assuming documentsused in A) are known. The English TAC08 UpdateSummarization dataset is used, which consists of 48topics with 20 documents each  36,911 words.4) Opinion summarization where systems have to analyzea set of blog articles and summarize the opinionsabout a target in the articles. The TAC08 OpinionSummarization in English4 data set (taken from theBlogs06 Text Collection) is used: 25 clusters and targets(i.e., target entity and questions) were used  1,167,735words.5) Generic single-document summarization in Spanishusing the Medicina Clnica5 corpus, which is composedof 50 medical articles in Spanish, each one with itscorresponding author abstract  124,929 words.6) Generic single document summarization in French usingthe Canadien French Sociological Articles corpusfrom the journal Perspectives interdisciplinaires sur letravail et la sante (PISTES)6 . It contains 50 sociologicalarticles in French, each one with its correspondingauthor abstract  381,039 words.7) Generic multi-document-summarization in French usingdata from the RPM27 corpus [18], 20 different themesconsisting of 10 articles and 4 abstracts by referencethematic  185,223 words.(4)where Ii0 is some subset of input Ii . The values producedby the measures for each summary SUMi,k are averagedfor each system k = 0, . . . , s  1 and these averages areused to produce a ranking. Rankings are then comparedusing Spearman Rank correlation [17] which is used tomeasure the degree of association between two variableswhose values are used to rank objects. We have chosento use this correlation to compare directly results to thosepresented in [12]. Computation of correlations is done usingthe Statistics-RankCorrelation-0.12 package1 , which computesthe rank correlation between two vectors. We also verifiedthe good conformity of the results with the correlation testof Kendall  calculated with the statistical software R. Thetwo nonparametric tests of Spearman and Kendall do notreally stand out as the treatment of ex-quo. The goodcorrespondence between the two tests shows that they do notintroduce bias in our analysis. Subsequently will mention onlythe  of Sperman more widely used in this field.A. ToolsWe carry out experimentation using a new summarizationevaluation framework: F RESA FRamework for EvaluatingSummaries Automatically, which includes document-basedsummary evaluation measures based on probabilitiesdistribution2 . As in the ROUGE package, F RESA supportsdifferent n-grams and skip n-grams probability distributions.The F RESA environment can be used in the evaluation ofsummaries in English, French, Spanish and Catalan, and itintegrates filtering and lemmatization in the treatment ofsummaries and documents. It is developed in Perl and willbe made publicly available. We also use the ROUGE package[10] to compute various ROUGE statistics in new datasets.For experimentation in the TAC and the DUC datasets we usedirectly the peer summaries produced by systems participatingin the evaluations. For experimentation in Spanish and French(single and multi-document summarization) we have createdsummaries at a similar ratio to those of reference using thefollowing systems: ENERTEX [19], a summarizer based on a theory oftextual energy; CORTEX [20], a single-document sentence extractionsystem for Spanish and French that combines variousstatistical measures of relevance (angle between sentenceand topic, various Hamming weights for sentences, etc.)and applies an optimal decision algorithm for sentenceselection; SUMMTERM [21], a terminology-based summarizer thatis used for summarization of medical articles anduses specialized terminology for scoring and rankingsentences; REG [22], summarization system based on an greedyalgorithm;B. Summarization Tasks and Data SetsWe have conducted our experimentation with the followingsummarization tasks and data sets:1) Generic multi-document-summarization in English(production of a short summary of a cluster of relateddocuments) using data from DUC043 , task 2: 50clusters, 10 documents each  294,636 words.2) Focused-based summarization in English (production ofa short focused multi-document summary focused on thequestion who is X?, where X is a persons name) usingdata from the DUC04 task 5: 50 clusters, 10 documentseach plus a target person name  284,440 words.4 http://www.nist.gov/tac/data/index.html1 http://search.cpan.org/gene/Statistics-RankCorrelation-0.12/5 http://www.elsevier.es/revistas/ctl2 F RESAis available at: http://lia.univavignon.fr/fileadmin/axes/TALNE/Ressources.html3 http://www-nlpir.nist.gov/projects/duc/guidelines/2004.htmlservlet? f=7032&amp;revistaid=26 http://www.pistes.uqam.ca/7 http://www-labs.sinequa.com/rpm215Polibits (42) 2010Juan-Manuel Torres-Moreno, Horacio Saggion, Iria da Cunha, Eric SanJuan, and Patricia Velzquez-Morales J S summarizer, a summarization system that scoresand ranks sentences according to their Jensen-Shannondivergence to the source document; a lead-based summarization system that selects the leadsentences of the document; a random-based summarization system that selectssentences at random; Open Text Summarizer [23], a multi-lingual summarizerbased on the frequency and commercial systems: Word, SSSummarizer8 , Pertinence9and Copernic10 .presented here we used uni-grams, 2-grams, and the skip2-grams with maximum skip distance of 4 (ROUGE-1,ROUGE-2 and ROUGE-SU4). ROUGE is used to comparea peer summary to a set of model summaries in ourframework (as indicated in equation 3). Jensen-Shannon divergence formula given in Equation 2is implemented in our F RESA package with the followingspecification (Equation 6) for the probability distributionof words w.CTPw = wN(SCwifwSNSQw =(6)TCw+otherwiseN +BC. Evaluation MeasuresThe following measures derived from human assessment ofthe content of the summaries are used in our experiments: C OVERAGE is understood as the degree to which onepeer summary conveys the same information as a modelsummary [2]. C OVERAGE was used in DUC evaluations.This measure is used as indicated in equation 3 usinghuman references or models. R ESPONSIVENESS ranks summaries in a 5-point scaleindicating how well the summary satisfied a giveninformation need [2]. It is used in focused-basedsummarization tasks. This measure is used as indicatedin equation 4 since a human judges the summarywith respect to a given input user need (e.g., aquestion). R ESPONSIVENESS was used in DUC and TACevaluations. P YRAMIDS [11] is a content assessment measure whichcompares content units in a peer summary to weightedcontent units in a set of model summaries. Thismeasure is used as indicated in equation 3 using humanreferences or models. P YRAMIDS is the adopted metricfor content-based evaluation in the TAC evaluations.For DUC and TAC datasets the values of these measures areavailable and we used them directly. We used the followingautomatic evaluation measures in our experiments: ROUGE [14], which is a recall metric that takes intoaccount n-grams as units of content for comparing peerand model summaries. The ROUGE formula specified in[10] is as follows:PmROUGE-n(R, M ) =P M ngramP countmatch (n  gram)PPcount(n-gram)m MWhere P is the probability distribution of words w intext T and Q is the probability distribution of words win summary S; N is the number of words in text andTsummary N = NT +NS , B = 1.5|V |, Cwis the numberSof words in the text and Cw is the number of words inthe summary. For smoothing the summarys probabilitieswe have used  = 0.005. We have also implementedother smoothing approaches (e.g. Good-Turing [24], thatuses the CPAN Perls Statistics-Smoothing-SGT-2.1.2package11 ) in F RESA, but we do not use them inthe experiments reported here. Following the ROUGEapproach, in addition to word uni-grams we use 2-gramsand skip n-grams computing divergences such as J S(using uni-grams) J S 2 (using 2-grams), J S 4 (using theskip n-grams of ROUGE-SU4), and J S M which is anaverage of the J S i . J Ss measures are used to compare apeer summary to its source document(s) in our framework(as indicated in equation 4). In the case of summarizationof multiple documents, these are concatenated (in thegiven input order) to form a single input from whichprobabilities are computed.IV. E XPERIMENTS AND R ESULTSWe first replicated the experiments presented in [12] toverify that our implementation of J S produced correlationresults compatible with that work. We used the TAC08Update Summarization data set and computed J S andROUGE measures for each peer summary. We producedtwo system rankings (one for each measure), which werecompared to rankings produced using the manual P YRAMIDSand R ESPONSIVENESS scores. Spearman correlations werecomputed among the different rankings. The results arepresented in Table I. These results confirm a high correlationamong P YRAMIDS, R ESPONSIVENESS and J S. We alsoverified high correlation between J S and ROUGE-2 (0.83Spearman correlation, not shown in the table) in this task anddataset.Then, we experimented with data from DUC04, TAC08Opinion Summarization pilot task as well as single and(5)where R is the summary to be evaluated, M is the set ofmodel (human) summaries, countmatch is the number ofcommon n-grams in m and P , and count is the numberof n-grams in the model summaries. For the experiments8 http://www.kryltech.com/summarizer.htm9 http://www.pertinence.net11 http://search.cpan.org/bjoernw/Statistics-Smoothing-SGT-2.1.2/10 http://www.copernic.com/en/products/summarizerPolibits (42) 201016Summary Evaluation with and without ReferencesTABLE IS PEARMANMesureROUGE-2JSCORRELATION OF CONTENT- BASED MEASURES INU PDATE S UMMARIZATION TASKP YRAMIDS0.960.85p-valuep &lt; 0.005p &lt; 0.005R ESPONSIVENESS0.920.74evaluation metrics that do not rely on human models but thatcompare summary content to input content directly [12]. Wehave some positive and some negative results regarding thedirect use of the full document in content-based evaluation.We have verified that in both generic muti-documentsummarizationandintopic-basedmulti-documentsummarization in English correlation among measuresthat use human models (P YRAMIDS, R ESPONSIVENESSand ROUGE) and a measure that does not use models(J S divergence) is strong. We have found that correlationamong the same measures is weak for summarization ofbiographical information and summarization of opinions inblogs. We believe that in these cases content-based measuresshould be considered, in addition to the input document, thesummarization task (i.e. text-based representation, description)to better assess the content of the peers [25], the task being adeterminant factor in the selection of content for the summary.Our multi-lingual experiments in generic single-documentsummarization confirm a strong correlation among theJ S divergence and ROUGE measures. It is worth notingthat ROUGE is in general the chosen framework forpresenting content-based evaluation results in non-Englishsummarization.For the experiments in Spanish, we are conscious that weonly have one model summary to compare with the peers.Nevertheless, these models are the corresponding abstractswritten by the authors. As the experiments in [26] show, theprofessionals of a specialized domain (as, for example, themedical domain) adopt similar strategies to summarize theirtexts and they tend to choose roughly the same content chunksfor their summaries. Previous studies have shown that authorabstracts are able to reformulate content with fidelity [27] andthese abstracts are ideal candidates for comparison purposes.Because of this, the summary of the author of a medical articlecan be taken as reference for summaries evaluation. It is worthnoting that there is still debate on the number of models to beused in summarization evaluation [28]. In the French corpusPISTES, we suspect the situation is similar to the Spanishcase.TAC08p-valuep &lt; 0.005p &lt; 0.005multi-document summarization in Spanish and French. In spiteof the fact that the experiments for French and Spanish corporause less data points (i.e., less summarizers per task) thanfor English, results are still quite significant. For DUC04,we computed the J S measure for each peer summary intasks 2 and 5 and we used J S, ROUGE, C OVERAGE andR ESPONSIVENESS scores to produce systems rankings. Thevarious Spearmans rank correlation values for DUC04 arepresented in Tables II (for task 2) and III (for task 5).For task 2, we have verified a strong correlation betweenJ S and C OVERAGE. For task 5, the correlation betweenJ S and C OVERAGE is weak, and that between J S andR ESPONSIVENESS is weak and negative.Although the Opinion Summarization (OS) task is a newtype of summarization task and its evaluation is a complicatedissue, we have decided to compare J S rankings with thoseobtained using P YRAMIDS and R ESPONSIVENESS in TAC08.Spearmans correlation values are listed in Table IV. As it canbe seen, there is weak and negative correlation of J S withboth P YRAMIDS and R ESPONSIVENESS. Correlation betweenP YRAMIDS and R ESPONSIVENESS rankings is high for thistask (0.71 Spearmans correlation value).For experimentation in mono-document summarizationin Spanish and French, we have run 11 multi-lingualsummarization systems; for experimentation in French, wehave run 12 systems. In both cases, we have producedsummaries at a compression rate close to the compression rateof the authors provided abstracts. We have then computed J Sand ROUGE measures for each summary and we have averagedthe measures values for each system. These averages wereused to produce rankings per each measure. We computedSpearmans correlations for all pairs of rankings.Results are presented in Tables V, VI and VII. All resultsshow medium to strong correlation between the J S measuresand ROUGE measures. However the J S measure based onuni-grams has lower correlation than J Ss which use n-gramsof higher order. Note that table VII presents results forgeneric multi-document summarization in French, in thiscase correlation scores are lower than correlation scores forsingle-document summarization in French, a result which maybe expected given the diversity of input in multi-documentsummarization.VI. C ONCLUSIONS AND F UTURE W ORKThis paper has presented a series of experiments incontent-based measures that do not rely on the use of modelsummaries for comparison purposes. We have carried outextensive experimentation with different summarization tasksdrawing a clearer picture of tasks where the measures couldbe applied. This paper makes the following contributions: We have shown that if we are only interested in rankingsummarization systems according to the content of theirautomatic summaries, there are tasks were models couldbe subtituted by the full document in the computation ofthe J S measure obtaining reliable rankings. However,we have also found that the substitution of modelsby full-documents is not always advisable. We haveV. D ISCUSSIONThe departing point for our inquiry into text summarizationevaluation has been recent work on the use of content-based17Polibits (42) 2010Juan-Manuel Torres-Moreno, Horacio Saggion, Iria da Cunha, Eric SanJuan, and Patricia Velzquez-MoralesTABLE IIS PEARMAN  OF CONTENT- BASED MEASURES WITH C OVERAGE IN DUC04 TASK 2MesureROUGE-2JSC OVERAGE0.790.68p-valuep &lt; 0.0050p &lt; 0.0025TABLE IIIS PEARMAN  OF CONTENT- BASED MEASURES IN DUC04 TASK 5MesureROUGE-2JSC OVERAGE0.780.40p-valuep &lt; 0.001p &lt; 0.050R ESPONSIVENESS0.44-0.18p-valuep &lt; 0.05p &lt; 0.25TABLE IVS PEARMAN  OF CONTENT- BASED MEASURES IN TAC08 OSMesureJSP YRAMIDS-0.13p-valuep &lt; 0.25R ESPONSIVENESS-0.14TASKp-valuep &lt; 0.25TABLE VS PEARMAN  OF CONTENT- BASED MEASURES WITH ROUGE IN THE Medicina Clnica C ORPUS (S PANISH )MesureJSJ S2J S4J SMROUGE -10.560.880.880.82p-valuep &lt; 0.100p &lt; 0.001p &lt; 0.001p &lt; 0.005ROUGE -20.460.800.800.71ROUGE -SU40.450.810.810.71p-valuep &lt; 0.200p &lt; 0.005p &lt; 0.005p &lt; 0.010a representation of the task/topic in the calculation ofmeasures. To carry out these comparisons, however, we aredependent on the existence of references.F RESA will also be used in the new question-answer taskcampaign INEX2010 (http://www.inex.otago.ac.nz/tracks/qa/qa.asp) for the evaluation of long answers. This task aimsto answer a question by extraction and agglomeration ofsentences in Wikipedia. This kind of task correspondsto those for which we have found a high correlationamong the measures J S and evaluation methods withhuman intervention. Moreover, the J S calculation will beamong the summaries produced and a representative set ofrelevant passages from Wikipedia. F RESA will be used tocompare three types of systems, although different tasks: themulti-document summarizer guided by a query, the searchsystems targeted information (focused IR) and the questionanswering systems.found weak correlation among different rankings incomplex summarization tasks such as the summarizationof biographical information and the summarization ofopinions. We have also carried out large-scale experiments inSpanish and French which show positive medium tostrong correlation among systems ranks produced byROUGE and divergence measures that do not use themodel summaries. We have also presented a new framework, F RESA, forthe computation of measures based on J S divergence.Following the ROUGE approach, F RESA package useword uni-grams, 2-grams and skip n-grams computingdivergences. This framework will be available to thecommunity for research purposes.Although we have made a number of contributions, this paperleaves many open questions than need to be addressed. Inorder to verify correlation between ROUGE and J S, in theshort term we intend to extend our investigation to otherlanguages such as Portuguese and Chinesse for which wehave access to data and summarization technology. We alsoplan to apply F RESA to the rest of the DUC and TACsummarization tasks, by using several smoothing techniques.As a novel idea, we contemplate the possibility of adaptingthe evaluation framework for the phrase compression task[29], which, to our knowledge, does not have an efficientevaluation measure. The main idea is to calculate J S froman automatically-compressed sentence taking the completesentence by reference. In the long term, we plan to incorporatePolibits (42) 2010p-valuep &lt; 0.100p &lt; 0.002p &lt; 0.002p &lt; 0.020ACKNOWLEDGMENTWe are grateful to the Programa Ramon y Cajal fromMinisterio de Ciencia e Innovacion, Spain. This work ispartially supported by: a postdoctoral grant from the NationalProgram for Mobility of Research Human Resources (NationalPlan of Scientific Research, Development and Innovation2008-2011, Ministerio de Ciencia e Innovacion, Spain); theresearch project CONACyT, number 82050, and the researchproject PAPIIT-DGAPA (Universidad Nacional Autonoma deMexico), number IN403108.18Summary Evaluation with and without ReferencesTABLE VIS PEARMAN  OF CONTENT- BASED MEASURES WITH ROUGE IN THE PISTES C ORPUS (F RENCH )MesureJSJ S2J S4J SMROUGE -10.700.930.830.88p-valuep &lt; 0.050p &lt; 0.002p &lt; 0.020p &lt; 0.010ROUGE -20.730.860.760.83p-valuep &lt; 0.05p &lt; 0.01p &lt; 0.05p &lt; 0.02ROUGE -SU40.730.860.760.83p-valuep &lt; 0.500p &lt; 0.005p &lt; 0.050p &lt; 0.010TABLE VIIS PEARMAN  OF CONTENT- BASED MEASURES WITH ROUGE IN THE RPM2 C ORPUS (F RENCH )MeasureJSJ S2J S4J SMROUGE -10.8300.8000.7500.850p-valuep &lt; 0.002p &lt; 0.005p &lt; 0.010p &lt; 0.002ROUGE -20.6600.5900.5200.640R EFERENCESp-valuep &lt; 0.05p &lt; 0.05p &lt; 0.10p &lt; 0.05ROUGE -SU40.7410.6800.6200.740p-valuep &lt; 0.01p &lt; 0.02p &lt; 0.05p &lt; 0.01[18] C. de Loupy, M. Guegan, C. Ayache, S. Seng, and J.-M. Torres-Moreno,A French Human Reference Corpus for multi-documentssummarization and sentence compression, in LREC10, vol. 2,Malta, 2010, p. In press.[19] S. Fernandez, E. SanJuan, and J.-M. Torres-Moreno, Textual Energyof Associative Memories: performants applications of Enertex algorithmin text summarization and topic segmentation, in MICAI07, 2007, pp.861871.[20] J.-M. Torres-Moreno, P. Velazquez-Morales, and J.-G. Meunier,Condenses de textes par des methodes numeriques, in JADT02, vol. 2,St Malo, France, 2002, pp. 723734.[21] J. Vivaldi, I. da Cunha, J.-M. Torres-Moreno, and P. Velazquez-Morales,Automatic summarization using terminological and semanticresources, in LREC10, vol. 2, Malta, 2010, p. In press.[22] J.-M. Torres-Moreno and J. Ramirez, REG : un algorithme gloutonapplique au resume automatique de texte, in JADT10. Rome, 2010,p. In press.[23] V. Yatsko and T. Vishnyakov, A method for evaluating modernsystems of automatic text summarization, Automatic Documentationand Mathematical Linguistics, vol. 41, no. 3, pp. 93103, 2007.[24] C. D. Manning and H. Schutze, Foundations of Statistical NaturalLanguage Processing.Cambridge, Massachusetts: The MIT Press,1999.[25] K. Sparck Jones, Automatic summarising: The state of the art, IPM,vol. 43, no. 6, pp. 14491481, 2007.[26] I. da Cunha, L. Wanner, and M. T. Cabre, Summarization of specializeddiscourse: The case of medical articles in spanish, Terminology, vol. 13,no. 2, pp. 249286, 2007.[27] C.-K. Chuah, Types of lexical substitution in abstracting, in ACLStudent Research Workshop.Toulouse, France: Association forComputational Linguistics, 9-11 July 2001 2001, pp. 4954.[28] K. Owkzarzak and H. T. Dang, Evaluation of automatic summaries:Metrics under varying data conditions, in UCNLG+Sum09, Suntec,Singapore, August 2009, pp. 2330.[29] K. Knight and D. Marcu, Statistics-based summarization-step one:Sentence compression, in Proceedings of the National Conference onArtificial Intelligence. Menlo Park, CA; Cambridge, MA; London;AAAI Press; MIT Press; 1999, 2000, pp. 703710.[1] I. Mani, G. Klein, D. House, L. Hirschman, T. Firmin, andB. Sundheim, Summac: a text summarization evaluation, NaturalLanguage Engineering, vol. 8, no. 1, pp. 4368, 2002.[2] P. Over, H. Dang, and D. Harman, DUC in context, IPM, vol. 43,no. 6, pp. 15061520, 2007.[3] Proceedings of the Text Analysis Conference. Gaithesburg, Maryland,USA: NIST, November 17-19 2008.[4] K. Sparck Jones and J. Galliers, Evaluating Natural LanguageProcessing Systems, An Analysis and Review, ser. Lecture Notes inComputer Science. Springer, 1996, vol. 1083.[5] R. L. Donaway, K. W. Drummey, and L. A. Mather, A comparison ofrankings produced by summarization evaluation measures, in NAACLWorkshop on Automatic Summarization, 2000, pp. 6978.[6] H. Saggion, D. Radev, S. Teufel, and W. Lam, Meta-evaluationof Summaries in a Cross-lingual Environment using Content-basedMetrics, in COLING 2002, Taipei, Taiwan, August 2002, pp. 849855.[7] D. R. Radev, S. Teufel, H. Saggion, W. Lam, J. Blitzer, H. Qi, A. Celebi,D. Liu, and E. Drabek, Evaluation challenges in large-scale documentsummarization, in ACL03, 2003, pp. 375382.[8] K. Papineni, S. Roukos, T. Ward, , and W. J. Zhu, BLEU: a methodfor automatic evaluation of machine translation, in ACL02, 2002, pp.311318.[9] K. Pastra and H. Saggion, Colouring summaries BLEU, in EvaluationInitiatives in Natural Language Processing. Budapest, Hungary: EACL,14 April 2003.[10] C.-Y. Lin, ROUGE: A Package for Automatic Evaluation ofSummaries, in Text Summarization Branches Out: ACL-04 Workshop,M.-F. Moens and S. Szpakowicz, Eds., Barcelona, July 2004, pp. 7481.[11] A. Nenkova and R. J. Passonneau, Evaluating Content Selection inSummarization: The Pyramid Method, in HLT-NAACL, 2004, pp.145152.[12] A. Louis and A. Nenkova, Automatically Evaluating Content Selectionin Summarization without Human Models, in Empirical Methods inNatural Language Processing, Singapore, August 2009, pp. 306314.[Online]. Available: http://www.aclweb.org/anthology/D/D09/D09-1032[13] J. Lin, Divergence Measures based on the Shannon Entropy, IEEETransactions on Information Theory, vol. 37, no. 145-151, 1991.[14] C.-Y. Lin and E. Hovy, Automatic Evaluation of Summaries UsingN-gram Co-occurrence Statistics, in HLT-NAACL. Morristown, NJ,USA: Association for Computational Linguistics, 2003, pp. 7178.[15] C.-Y. Lin, G. Cao, J. Gao, and J.-Y. Nie, An information-theoreticapproach to automatic evaluation of summaries, in HLT-NAACL,Morristown, USA, 2006, pp. 463470.[16] S. Kullback and R. Leibler, On information and sufficiency, Ann. ofMath. Stat., vol. 22, no. 1, pp. 7986, 1951.[17] S. Siegel and N. Castellan, Nonparametric Statistics for the BehavioralSciences. McGraw-Hill, 1998.19Polibits (42) 2010</biblio></article></xml>