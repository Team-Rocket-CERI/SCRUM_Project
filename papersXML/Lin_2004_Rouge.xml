<xml><article><preambule>Lin_2004_Rouge.pdf</preambule><titre>Rouge</titre><auteur>Lin</auteur><abstract>Abstract
ROUGE stands for Recall-Oriented Understudy for
Gisting Evaluation. It includes measures to automatically determine the quality of a summary by
comparing it to other (ideal) summaries created by
humans. The measures count the number of overlapping units such as n-gram, word sequences, and
word pairs between the computer-generated summary to be evaluated and the ideal summaries created by humans. This paper introduces four different
ROUGE measures: ROUGE-N, ROUGE-L, ROUGE-W,
and ROUGE-S included in the ROUGE summarization evaluation package and their evaluatio ns. Three
of them have been used in the Document Understanding Conference (DUC) 2004, a large-scale
summarization evaluation sponsored by NIST.
1

</abstract><introduction>1

Introduction

Traditionally evaluation of summarization involves
human judgments of different quality metrics, for
example, coherence, conciseness, grammaticality,
readability, and content (Mani, 2001). However,
even simple manual evaluation of summaries on a
large scale over a few linguistic quality questions
and content coverage as in the Document Understanding Conference (DUC) (Over and Yen, 2003)
would require over 3,000 hours of human efforts.
This is very expensive and difficult to conduct in a
frequent basis. Therefore, how to evaluate summaries automatically has drawn a lot of attention in the
summarization research community in recent years.
For example, Saggion et al. (2002) proposed three
content-based evaluation methods that measure
similarity between summaries. These methods are:
cosine similarity, unit overlap (i.e. unigram or bigram), and longest common subsequence. However,
they did not show how the results of these automatic
evaluation methods correlate to human judgments.
Following the successful applic ation of automatic
evaluation methods, such as BLEU (Papineni et al.,
2001), in machine translation evaluation, Lin and
Hovy (2003) showed that methods similar to BLEU ,

i.e. n-gram co-occurrence statistics, could be applied
to evaluate summaries. In this paper, we introduce a
package, ROUGE, for automatic evaluation of summaries and its evaluations. ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation. It
includes several automatic evaluation methods that
measure the similarity between summaries. We describe ROUGE-N in Section 2, ROUGE-L in Section
3, ROUGE-W in Section 4, and ROUGE-S in Section
5. Section 6 shows how these measures correlate
with human judgments using DUC 2001, 2002, and
2003 data. Section 7 concludes this paper and discusses future directions.
</introduction><corps>2ROUGE-N: N-gram Co-Occurrence StatisticsFormally, ROUGE-N is an n-gram recall between acandidate summary and a set of reference summaries. ROUGE-N is computed as follows:ROUGE-N= CountS { ReferemceSummaries} gramn  Smatch( gramn ) Count (gram )(1)nS { ReferenceSummaries} gramn  SWhere n stands for the length of the n-gram,gramn , and Countmatch(gramn ) is the maximum number of n-grams co-occurring in a candidate summaryand a set of reference summaries.It is clear that ROUGE-N is a recall-related measure because the denominator of the equation is thetotal sum of the number of n-grams occurring at thereference summary side. A closely related measure,BLEU , used in automatic evaluation of machinetranslation, is a precision-based measure. BLEUmeasures how well a candidate translation matchesa set of reference translations by counting the percentage of n-grams in the candidate translation overlapping wit h the references. Please see Papineni etal. (2001) for details about BLEU .Note that the number of n-grams in the denominator of the ROUGE-N formula increases as we addmore references. This is intuitive and reasonablebecause there might exist multiple good summaries.Every time we add a reference into the pool, we expand the space of alternative summaries. By controlling what types of references we add to thereference pool, we can design evaluations that focuson different aspects of summarization. Also notethat the numerator sums over all reference summaries. This effectively gives more weight to matchingn-grams occurring in multiple references. Thereforea candidate summary that contains words shared bymore references is favored by the ROUGE-N measure. This is again very intuitive and reasonable because we normally prefer a candidate summary thatis more similar to consensus among reference summaries.2.1Multiple ReferencesSo far, we only demonstrated how to computeROUGE-N using a single reference. When multiplereferences are used, we compute pairwise summarylevel ROUGE-N between a candidate summary s andevery reference, ri , in the reference set. We thentake the maximum of pairwise summary-levelROUGE-N scores as the final multiple referenceROUGE-N score. This can be written as follows:ROUGE-Nmulti = argmaxi ROUGE-N(ri ,s)This procedure is also applied to computation ofROUGE-L (Section 3), ROUGE-W (Section 4) , andROUGE-S (Section 5). In the implementation, we usea Jackknifing procedure. Given M references, wecompute the best score over M sets of M-1 references. The final ROUGE-N score is the average ofthe M ROUGE-N scores using different M-1 references. The Jackknifing procedure is adopted sincewe often need to compare system and human performance and the reference summaries are usuallythe only human summaries available. Using thisprocedure, we are able to estimate average humanperformance by averaging M ROUGE-N scores ofone reference vs. the rest M-1 references. Althoughthe Jackknif ing procedure is not necessary when wejust want to compute ROUGE scores using multiplereferences, it is applied in all ROUGE score computations in the ROUGE evaluation package.In the next section, we describe a ROUGE measurebased on longest common subsequences betweentwo summaries.3ROUGE-L: Longest Common Subs equenceA sequence Z = [z1 , z2, ..., zn ] is a subsequence ofanother sequence X = [x1 , x2 , ..., x m ], if there exists astrict increasing sequence [i1 , i2 , ..., ik] of indices ofX such that for all j = 1, 2, ..., k, we have xij = zj(Cormen et al., 1989). Given two sequences X andY, the longest common subsequence (LCS) of X andY is a common subsequence with maximum length.LCS has been used in identifying cognate candidates during construction of N-best translation lexicon from parallel text. Melamed (1995) used theratio (LCSR) between the length of the LCS of twowords and the length of the longer word of the twowords to measure the cognateness between them.He used LCS as an approximate string matchingalgorithm. Saggion et al. (2002) used normalizedpairwise LCS to compare simila rity between twotexts in automatic summarization evaluation.3.1Sentence-Level LCSTo apply LCS in summarization evaluation, weview a summary sentence as a sequence of words.The intuition is that the longer the LCS of twosummary sentences is, the more similar the twosummaries are. We propose using LCS-based Fmeasure to estimate the similarity between twosummaries X of length m and Y of length n, assuming X is a reference summary sentence and Y is acandidate summary sentence, as follows:LCS ( X , Y )(2)mLCS ( X , Y )Plcs =(3)n(1 +  2 ) Rlcs PlcsFlcs =(4)Rlcs +  2PlcsRlcs =Where LCS(X,Y) is the length of a longest common subsequence of X and Y, and  = Plcs/Rlcs when?Flcs/?Rlcs_ =_ ?Flcs/?Plcs . In DUC,  is set to a verybig number (? 8 ). Therefore, only Rlcs is considered. We call the LCS-based F-measure, i.e. Equation 4, ROUGE-L. Notice that ROUGE-L is 1 when X= Y; while ROUGE-L is zero when LCS(X,Y) = 0, i.e.there is nothing in common between X and Y. Fmeasure or its equivalents has been shown to havemet several theoretical criteria in measuring accuracy involving more than one factor (Van Rijsbergen, 1979). The composite factors are LCS-basedrecall and precision in this case. Melamed et al.(2003) used unigram F-measure to estimate machinetranslation quality and showed that unigram Fmeasure was as good as BLEU .One advantage of using LCS is that it does not require consecutive matches but in-sequence matchesthat reflect sentence level word order as n-grams.The other advantage is that it automatically includeslongest in-sequence common n-grams, therefore nopredefined n-gram length is necessary.ROUGE-L as defined in Equation 4 has the property that its value is less than or equal to the min imum of unigram F-measure of X and Y. Unigramrecall reflects the proportion of words in X (reference summary sentence) that are also present in Y(candidate summary sentence); while unigram precision is the proportion of words in Y that are also inX. Unigram recall and precision count all cooccurring words regardless their orders; whileROUGE-L counts only in-sequence co-occurrences.By only awarding credit to in-sequence unigrammatches, ROUGE-L also captures sentence levelstructure in a natural way. Consider the followingexample:S1. police killed the gunmanS2. police kill the gunmanS3. the gunman kill policeWe only consider ROUGE-2, i.e. N=2, for the purpose of explanation. Using S1 as the reference andS2 and S3 as the candidate summary sentences, S2and S3 would have the same ROUGE-2 score, sincethey both have one bigram, i.e. the gunman. However, S2 and S3 have very different meanings. In thecase of ROUGE-L, S2 has a score of 3/4 = 0.75 andS3 has a score of 2/4 = 0.5, with  = 1. Therefore S2is better than S3 according to ROUGE-L. This example also illustrated that ROUGE-L can work reliablyat sentence level.However, LCS suffers one disadvantage that itonly counts the main in-sequence words; therefore,other alternative LCSes and shorter sequences arenot reflected in the final score. For example, giventhe following candidate sentence:S4. the gunman police killedUsing S1 as its reference, LCS counts either thegunman or police killed, but not both; therefore,S4 has the same ROUGE-L score as S3. ROUGE-2would prefer S4 than S3.3.2Previous section described how to compute sentence-level LCS-based F-measure score. When applying to summary-level, we take the union LCSmatches between a reference summary sentence, ri ,and every candidate summary sentence, cj . Given areference summary of u sentences containing a totalof m words and a candidate summary of v sentencescontaining a total of n words, the summary-levelLCS-based F-measure can be computed as follows:uRlcs = LCS ( ri , C )i =1m(5)uPlcs = LCS ( ri , C )i =1n(6)(7)Again  is set to a very big number (? 8 ) inDUC, i.e. only Rlcs is considered. LCS  ( ri , C) is theLCS score of the union longest common subsequence between reference sentence ri and candidatesummary C. For example, if ri = w1 w2 w3 w4 w5 , andC contains two sentences: c1 = w1 w2 w6 w7 w8 and c2= w1 w3 w8 w9 w5 , then the longest common subsequence of ri and c1 is w1 w2  and the longest common subsequence of ri and c2 is w1 w3 w5 . Theunion longest common subsequence of ri , c1 , and c2is w1 w2 w3 w5 and LCS  ( ri , C) = 4/5.3.3ROUGE-L vs. Normalized Pairwise LCSThe normalized pairwise LCS proposed by Radev etal. (page 51, 2002) between two summaries S1 andS2, LCS(S1 ,S 2 )MEAD , is written as follows:s i S 1length( s ) + max s j S 2 LCS (si , s j ) +s i S 1is j S2s j S 2max s i S 1 LCS (s i , s j )(8)length (s j )Assuming S1 has m words and S2 has n words,Equation 8 can be rewritten as Equation 9 due tosymmetry:2*s i S1max s j S2 LCS ( s i , s j )m+n(9)We then define MEAD LCS recall (Rlcs-MEAD) andMEAD LCS precision (Plcs-MEAD ) as follows:Rlcs-MEAD =Summary-Level LCS(1 +  2 ) Rlcs PlcsRlcs +  2PlcsFlcs =Plcs-MEAD =siS1max s jS2 LCS ( si , s j )msiS1max s jS2 LCS ( si , s j )n(10)(11)We can rewrite Equation (9) in terms of Rlcs-MEADand Plcs-MEAD with a constant parameter  = 1 as follows:LCS(S1 ,S 2 )MEAD =(1 +  2 ) Rlcs  MEAD Plcs  MEAD(12)Rlcs  MEAD +  2Plcs  MEADEquation 12 shows that normalized pairwise LCSas defined in Radev et al. (2002) and implementedin MEAD is also a F-measure with  = 1. Sentencelevel normalized pairwise LCS is the same asROUGE-L with  = 1. Besides setting  = 1, summary-level normalized pairwise LCS is differentfrom ROUGE-L in how a sentence gets its LCS scorefrom its references. Normalized pairwise LCS takesthe best LCS score while ROUGE-L takes the unionLCS score.4ROUGE-W: Weighted Longest Common SubsequenceLCS has many nice properties as we have describedin the previous sections. Unfortunately, the basicLCS also has a problem that it does not differentiateLCSes of different spatial relations within their embedding sequences. For example, given a referencesequence X and two candidate sequences Y1 and Y2as follows:X:Y1 :Y2 :[A B C D E F G][A B C D H I K][A H B K C I D]Y1 and Y2 have the same ROUGE-L score. However, in this case, Y1 should be the better choice thanY2 because Y1 has consecutive matches. To improvethe basic LCS method, we can simply remember thelength of consecutive matches encountered so far toa regular two dimensional dynamic program tablecomputing LCS. We call this weighted LCS(WLCS) and use k to indicate the length of the current consecutive matches ending at words xi and yj.Given two sentences X and Y, the WLCS score of Xand Y can be computed using the following dynamicprogramming procedure:(1) For (i = 0; i &lt;=m; i++)c(i,j) = 0 // initialize c-tablew(i,j) = 0 // initialize w-table(2) For (i = 1; i &lt;= m; i++)For (j = 1; j &lt;= n; j++)If xi = yj Then// the length of consecutive matches at// position i-1 and j -1k = w(i-1,j-1)c(i,j) = c(i-1,j-1) + f(k+1 )  f(k)// remember the length of consecutive// matches at position i, jw(i,j) = k+1OtherwiseIf c(i-1,j) &gt; c(i,j-1) Thenc(i,j) = c(i-1,j)w(i,j) = 0// no match at i, jElse c(i,j) = c(i,j-1)w(i,j) = 0// no match at i, j(3) WLCS(X,Y) = c(m,n)Where c is the dynamic programming table, c(i,j)stores the WLCS score ending at word xi of X and yjof Y, w is the table storing the length of consecutivematches ended at c table position i and j, and f is afunction of consecutive matches at the table posi-tion, c(i,j). Notice that by providing differentweighting function f, we can parameterize theWLCS algorithm to assign different credit to consecutive in-sequence matches.The weighting function f must have the propertythat f(x+y) &gt; f(x) + f(y) for any positive integers xand y. In other words, consecutive matches areawarded more scores than non-consecutive matches.For example, f(k)-=-k   when k &gt;= 0, and ,  &gt;0. This function charges a gap penalty of  foreach non-consecutive n-gram sequences. Anotherpossible function family is the polynomial family ofthe form k  where - &gt; 1. However, in order tonorma lize the final ROUGE-W score, we also preferto have a function that has a close form inversefunction. For example, f(k)-=-k 2 has a close forminverse function f -1 (k)-=-k 1/2. F-measure based onWLCS can be computed as follows, given two sequences X of length m and Y of length n: WLCS ( X , Y ) f ( m) WLCS ( X , Y ) = f 1 f (n )Rwlcs = fPwlcsFwlcs1(1 +  2 ) Rwlcs Pwlcs=Rwlcs +  2 Pwlcs(13)(14)(15)Where f -1 is the inverse function of f. In DUC,  isset to a very big number (? 8 ). Therefore, onlyRwlcs is considered. We call the WLCS-based Fmeasure, i.e. Equation 15, ROUGE-W. Using Equation 15 and f(k)-=-k2 as the weighting function, theROUGE-W scores for sequences Y1 and Y2 are 0.571and 0.286 respectively. Therefore, Y1 would beranked higher than Y2 using WLCS. We use thepolynomial function of the form k  in the ROUGEevaluation package. In the next section, we introduce the skip-bigram co-occurrence statistics.5ROUGE-S: Skip-Bigram Co-Occurrence StatisticsSkip-bigram is any pair of words in their sentenceorder, allowing for arbitrary gaps. Skip-bigram cooccurrence statistics measure the overlap of skipbigrams between a candidate translation and a set ofreference translations. Using the example given inSection 3.1:S1.S2.S3.S4.police killed the gunmanpolice kill the gunmanthe gunman kill policethe gunman police killedeach sentence has C(4,2) 1 = 6 skip-bigrams. For example, S1 has the following skip-bigrams:(police killed, police the, police gunman,killed the, killed gunman, the gunman)S2 has three skip-bigram matches with S1 (police the, police gunman, the gunman), S3 hasone skip-bigram match with S1 (the gunman), andS4 has two skip-bigram matches with S1 (policekille d, the gunman). Given translations X oflength m and Y of length n, assuming X is a reference translation and Y is a candidate translation, wecompute skip-bigram-based F-measure as follows:SKIP 2( X , Y )(16)C ( m, 2)SKIP 2( X , Y )Pskip2 =(17)C ( n,2)(1 +  2 ) Rskip2 Pskip2Fskip2 =(18)Rskip2 +  2 Pskip2Rskip2 =Where SKIP2(X,Y) is the number of skip-bigrammatches between X and Y,  controlling the relativeimportance of Pskip2 and Rskip2 , and C is the combination function. We call the skip-bigram-based Fmeasure, i.e. Equation 18, ROUGE-S.Using Equation 18 with  = 1 and S1 as the reference, S2s ROUGE-S score is 0.5, S3 is 0.167, andS4 is 0.333. Therefore, S2 is better than S3 and S4,and S4 is better than S3. This result is more intuitivethan using BLEU -2 and ROUGE-L. One advantage ofskip-bigram vs. BLEU is that it does not require consecutive matches but is still sensitive to word order.Comparing skip-bigram with LCS, skip-bigramcounts all in-order matching word pairs while LCSonly counts one longest common subsequence.Applying skip-bigram without any constraint onthe distance between the words, spurious matchessuch as the the or of in might be counted asvalid matches. To reduce these spurious matches,we can limit the maximum skip distance, d skip , between two in-order words that is allowed to form askip-bigram. For example, if we set d skip to 0 thenROUGE-S is equivalent to bigram overlap Fmeasure. If we set d skip to 4 then only word pairs ofat most 4 words apart can form skip-bigrams.Adjusting Equations 16, 17, and 18 to use maximum skip distance limit is straightforward: we onlycount the skip-bigram matches, SKIP2 (X,Y), withinthe maximum skip distance and replace denominators of Equations 16, C(m,2), and 17, C(n,2), withthe actual numbers of within distance skip-bigramsfrom the reference and the candidate respectively.5.1One potential problem for ROUGE-S is that it doesnot give any credit to a candidate sentence if thesentence does not have any word pair co-occurringwith its references. For example, the following sentence has a ROUGE-S score of zero:S5. gunman the killed policeS5 is the exact reverse of S1 and there is no skipbigram match between them. However, we wouldlike to differentiate sentences similar to S5 fromsentences that do not have single word cooccurrence with S1. To achieve this, we extendROUGE-S with the addition of unigram as countingunit. The extended version is called ROUGE-SU. Wecan also obtain ROUGE-SU from ROUGE-S by adding a begin-of-sentence marker at the beginning ofcandidate and reference sentences.6Evaluations of ROUGETo assess the effectiveness of ROUGE measures, wecompute the correlation between ROUGE assignedsummary scores and human assigned summaryscores. The intuition is that a good evaluation measure should assign a good score to a good summaryand a bad score to a bad summary. The ground truthis based on human assigned scores. Acquiring human judgments are usually very expensive; fortunately, we have DUC 2001, 2002, and 2003evaluation data that include human judgments forthe following: Single document summaries of about 100words: 12 systems 2 for DUC 2001 and 14 systems for 2002. 149 single document summarieswere judged per system in DUC 2001 and 295were judged in DUC 2002. Single document very short summaries of about10 words (headline-like, keywords, or phrases):14 systems for DUC 2003. 624 very short summaries were judged per system in DUC 2003. Multi-document summaries of about 10 words:6 systems for DUC 2002; 50 words: 14 systemsfor DUC 2001 and 10 systems for DUC 2002;100 words: 14 systems for DUC 2001, 10 systems for DUC 2002, and 18 systems for DUC2003; 200 words: 14 systems for DUC 2001 and10 systems for DUC 2002; 400 words: 14 systems for DUC 2001. 29 summaries were judgedper system per summary size in DUC 2001, 59were judged in DUC 2002, and 30 were judgedin DUC 2003.21C(4,2) = 4!/(2!*2!) = 6.ROUGE-SU: Extension of ROUGE-SAll systems include 1 or 2 baselines. Please see DUCwebsite for details.DUC 2001 100 WORDS SINGLE DOCDUC 2002 100 WORDS SINGLE DOC1 REF3 REFS1 REF2 REFSMethod CASE STEM STOP CASE STEM STOP CASE STEM STOP CASE STEM STOPR-10.76 0.760.84 0.80 0.78 0.84 0.98 0.98 0.99 0.98 0.98 0.99R-20.84 0.840.83 0.87 0.87 0.86 0.99 0.99 0.99 0.99 0.99 0.99R-30.82 0.830.80 0.86 0.86 0.85 0.99 0.99 0.99 0.99 0.99 0.99R-40.81 0.810.77 0.84 0.84 0.83 0.99 0.99 0.98 0.99 0.99 0.99R-50.79 0.790.75 0.83 0.83 0.81 0.99 0.99 0.98 0.99 0.99 0.98R-60.76 0.770.71 0.81 0.81 0.79 0.98 0.99 0.97 0.99 0.99 0.98R-70.73 0.740.65 0.79 0.80 0.76 0.98 0.98 0.97 0.99 0.99 0.97R-80.69 0.710.61 0.78 0.78 0.72 0.98 0.98 0.96 0.99 0.99 0.97R-90.65 0.670.59 0.76 0.76 0.69 0.97 0.97 0.95 0.98 0.98 0.96R-L0.83 0.830.83 0.86 0.86 0.86 0.99 0.99 0.99 0.99 0.99 0.99R-S*0.74 0.740.80 0.78 0.77 0.82 0.98 0.98 0.98 0.98 0.97 0.98R-S40.84 0.850.84 0.87 0.88 0.87 0.99 0.99 0.99 0.99 0.99 0.99R-S90.84 0.850.84 0.87 0.88 0.87 0.99 0.99 0.99 0.99 0.99 0.99R-SU*0.74 0.740.81 0.78 0.77 0.83 0.98 0.98 0.98 0.98 0.98 0.98R-SU40.84 0.840.85 0.87 0.87 0.87 0.99 0.99 0.99 0.99 0.99 0.99R-SU90.84 0.840.85 0.87 0.87 0.87 0.99 0.99 0.99 0.99 0.99 0.99R-W-1.2 0.85 0.850.85 0.87 0.87 0.87 0.99 0.99 0.99 0.99 0.99 0.99Table 1: Pearsons correlations of 17 ROUGEmeasure scores vs. human judgments for the DUC2001 and 2002 100 words single document summarization tasksMethodR-1R-2R-3R-4R-5R-6R-7R-8R-9R-LR-S*R-S4R-S9R-SU*R-SU4R-SU9R-W-1.2DUC 2003 10 WORDS SINGLE DOC1 REF 4REFS 1 REF 4 REFS 1 REF 4 REFSCASESTEMSTOP0.960.950.950.950.900.900.750.760.750.750.760.770.710.700.700.680.730.700.640.650.620.630.690.660.620.640.600.630.630.600.570.620.550.610.460.540.560.560.580.600.460.440.550.530.540.550.000.240.510.470.510.490.000.140.970.960.970.960.970.960.890.870.880.850.950.920.880.890.880.880.950.960.920.920.920.910.970.950.930.900.910.890.960.940.970.960.960.950.980.970.970.950.960.940.970.950.960.960.960.960.960.96Table 2: Pearsons correlations of 17 ROUGEmeasure scores vs. human judgments for the DUC2003 very short summary taskBesides these human judgments, we also have 3 setsof manual summaries for DUC 2001, 2 sets forDUC 2002, and 4 sets for DUC 2003. Humanjudges assigned content coverage scores to a candidate summary by examining the percentage of content overlap between a manual summary unit, i.e.elementary discourse unit or sentence, and the candidate summary using Summary Evaluation Environment 3 (SEE) developed by the University ofSouthern Californias Information Sciences Institute(ISI). The overall candidate summary score is theaverage of the content coverage scores of all theunits in the manual summary. Note that humanjudges used only one manual summary in all theevaluations although multiple alternative summarieswere available.With the DUC data, we computed Pearsonsproduct moment correlation coefficients, Spearmans rank order correlation coefficients, andKendalls correlation coefficients between systemsaverage ROUGE scores and their human assignedaverage coverage scores using single reference andmultiple references. To investigate the effect ofstemming and inclusion or exclusion of stopwords,we also ran experiments over orig inal automatic and3SEE is available online at http://www.isi.edu/~cyl.manual summaries (CASE set), stemmed4 version ofthe summaries (STEM set), and stopped version ofthe summaries (STOP set). For example, we computed ROUGE scores for the 12 systems participatedin the DUC 2001 single document summarizationevaluation using the CASE set with single referenceand then calculated the three correlation scores forthese 12 systems ROUGE scores vs. human assignedaverage coverage scores. After that we repeated theprocess using multiple references and then usingSTEM and STOP sets. Therefore, 2 (multi or single)x 3 (CASE, STEM, or STOP) x 3 (Pearson, Spearman, or Kendall) = 18 data points were collected foreach ROUGE measure and each DUC task. To assessthe significance of the results, we applied bootstrapresampling technique (Davison and Hinkley, 1997)to estimate 95% confidence intervals for every correlation computation.17 ROUGE measures were tested for each run using ROUGE evaluation package v1.2.1: ROUGE-Nwith N = 1 to 9, ROUGE-L, ROUGE-W withweighting factor  = 1.2, ROUGE-S and ROUGE-SUwith maximum skip distance d skip = 1, 4, and 9. Dueto limitation of space, we only report correlationanalysis results based on Pearsons correlation coefficient. Correlation analyses based on Spearmansand Kendalls correlation coefficients are trackingPearsons very closely and will be posted later at theROUGE website 5 for reference. The critical value 6for Pearsons correlation is 0.632 at 95% confidencewith 8 degrees of freedom.Table 1 shows the Pearsons correlation coefficients of the 17 ROUGE measures vs. human judgments on DUC 2001 and 2002 100 words singledocument summarization data. The best values ineach column are marked with dark (green) color andstatistically equivalent values to the best values aremarked with gray. We found that correlations werenot affected by stemming or removal of stopwordsin this data set, ROUGE-2 performed better amongthe ROUGE-N variants, ROUGE-L, ROUGE-W, andROUGE-S were all performing well, and using multiple references improved performance though notmuch. All ROUGE measures achieved very goodcorrelation with human judgments in the DUC 2002data. This might due to the double sample size inDUC 2002 (295 vs. 149 in DUC 2001) for each system.Table 2 shows the correlation analysis results onthe DUC 2003 single document very short summarydata. We found that ROUGE-1, ROUGE-L, ROUGE4Porters stemmer was used.ROUGE website: http://www.isi.edu/~cyl/ROUGE.6 The critical values for Pearsons correlation at 95%confidence with 10, 12, 14, and 16 degrees of freedomare 0.576, 0.532, 0.497, and 0.468 respectively.5(A1) DUC 2001 100 WORDS MULTI(A2) DUC 2002 100 WORDS MULTI(A3) DUC 2003 100 WORDS MULTI1 RFF3 REFS1 REF2 REFS1 REF4 REFSMethod CASE STEM STOP CASE STEM STOP CASE STEM STOP CASE STEM STOP CASE STEM STOP CASE STEM STOPR-10.480.560.86 0.530.57 0.870.660.660.77 0.710.710.78 0.580.570.71 0.58 0.570.71R-20.550.570.64 0.590.61 0.710.830.830.80 0.880.870.85 0.690.670.71 0.79 0.790.81R-30.460.450.47 0.530.53 0.550.850.840.76 0.890.880.83 0.540.510.48 0.76 0.750.74R-40.390.390.43 0.480.49 0.470.800.800.63 0.830.820.75 0.370.360.36 0.62 0.610.52R-50.380.390.33 0.470.48 0.430.730.730.45 0.730.730.62 0.250.250.27 0.45 0.440.38R-60.390.390.20 0.450.46 0.390.710.720.38 0.660.640.46 0.210.210.26 0.34 0.310.29R-70.310.310.17 0.440.44 0.360.630.650.33 0.560.530.44 0.200.200.23 0.29 0.270.25R-80.180.190.09 0.400.40 0.310.550.550.52 0.500.460.52 0.180.180.21 0.23 0.220.23R-90.110.120.06 0.380.38 0.280.540.540.52 0.450.420.52 0.160.160.19 0.21 0.210.21R-L0.490.490.49 0.560.56 0.560.620.620.62 0.650.650.65 0.500.500.50 0.53 0.530.53R-S*0.450.520.84 0.510.54 0.860.690.690.77 0.730.730.79 0.600.600.67 0.61 0.600.70R-S40.460.500.71 0.540.57 0.780.790.800.79 0.840.850.82 0.630.640.70 0.73 0.730.78R-S90.420.490.77 0.530.56 0.810.790.800.78 0.830.840.81 0.650.650.70 0.70 0.700.76R-SU*0.450.520.84 0.510.54 0.870.690.690.77 0.730.730.79 0.600.590.67 0.60 0.600.70R-SU40.470.530.80 0.550.58 0.830.760.760.79 0.800.810.81 0.640.640.74 0.68 0.680.76R-SU90.440.500.80 0.530.57 0.840.770.780.78 0.810.820.81 0.650.650.72 0.68 0.680.75R-W-1.2 0.520.520.52 0.600.60 0.600.670.670.67 0.690.690.69 0.530.530.53 0.58 0.580.58(C)Method CASER-10.71R-20.82R-30.59R-40.25R-5-0.25R-60.00R-70.00R-80.00R-90.00R-L0.78R-S*0.83R-S40.85R-S90.82R-SU*0.75R-SU40.76R-SU90.74R-W-1.2 0.78DUC02STEM0.680.850.740.36-0.250.000.000.000.000.780.820.860.810.740.750.730.7810(D1) DUC01 50(D2) DUC02 50(E1) DUC01 200(E2)STOP CASE STEM STOP CASE STEM STOP CASE STEM STOP CASE0.49 0.490.49 0.730.440.480.80 0.810.810.90 0.840.80 0.430.45 0.590.470.490.62 0.840.850.86 0.930.75 0.320.33 0.390.360.360.45 0.800.800.81 0.900.16 0.280.26 0.360.280.280.39 0.770.780.78 0.87-0.24 0.300.29 0.310.280.300.49 0.770.760.72 0.820.00 0.220.23 0.410.180.21 -0.17 0.750.750.67 0.780.00 0.260.23 0.500.110.160.00 0.720.720.62 0.720.00 0.320.32 0.34-0.11 -0.11 0.00 0.680.680.54 0.710.00 0.300.30 0.34-0.14 -0.14 0.00 0.640.640.48 0.700.78 0.560.56 0.560.500.500.50 0.810.810.81 0.880.69 0.460.45 0.740.460.490.80 0.800.800.90 0.840.76 0.400.41 0.690.420.440.73 0.820.820.87 0.910.69 0.420.41 0.720.400.430.78 0.810.820.86 0.900.56 0.460.46 0.740.460.490.80 0.800.800.90 0.840.58 0.450.45 0.720.440.460.78 0.820.830.89 0.900.56 0.440.44 0.730.410.450.79 0.820.820.88 0.890.78 0.560.56 0.560.510.510.51 0.840.840.84 0.90DUC02 200STEM STOP0.840.910.930.940.910.910.880.880.830.840.790.770.730.740.710.700.690.590.880.880.850.930.910.930.900.920.850.930.900.930.890.920.900.90(F) DUC01CASE STEM0.74 0.730.88 0.880.84 0.840.80 0.800.77 0.770.74 0.740.70 0.700.66 0.660.63 0.620.82 0.820.75 0.740.85 0.850.83 0.830.75 0.740.84 0.840.83 0.820.86 0.86400STOP0.900.870.820.750.700.630.580.520.460.820.890.850.840.890.880.870.86Table 3: Pearsons correlations of 17 ROUGE measure scores vs. human judgments forthe DUC 2001, 2002, and 2003 mult i-document summarization tasksSU4 and 9, and ROUGE-W were very good measuresin this category, ROUGE-N with N &gt; 1 performedsignificantly worse than all other measures, and exclusion of stopwords improved performance in general except for ROUGE-1. Due to the large numberof samples (624) in this data set, using multiple references did not improve correlations.In Table 3 A1, A2, and A3, we show correlationanalysis results on DUC 2001, 2002, and 2003 100words multi-document summarization data. Theresults indicated that using multiple references improved correlation and exclusion of stopwords usually improved performance. ROUGE-1, 2, and 3performed fine but were not consistent. ROUGE-1,ROUGE-S4, ROUGE-SU4, ROUGE-S9, and ROUGESU9 with stopword removal had correlation above0.70. ROUGE-L and ROUGE-W did not work well inthis set of data.Table 3 C, D1, D2, E1, E2, and F show the correlation analyses using multiple references on the restof DUC data. These results again suggested thatexclusion of stopwords achieved better performanceespecially in multi-document summaries of 50words. Better correlations (&gt; 0.70) were observedon long summary tasks, i.e. 200 and 400 wordssummaries. The relative performance of ROUGEmeasures followed the pattern of the 100 wordsmulti-document summarization task.Comparing the results in Table 3 with Table s 1and 2, we found that correlation values in the multidocument tasks rarely reached high 90% except inlong summary tasks. One possible explanation ofthis outcome is that we did not have large amount ofsamples for the multi-document tasks. In the singledocument summarization tasks we had over 100samples; while we only had about 30 samples in themulti-document tasks. The only tasks that had over30 samples was from DUC 2002 and the correlations of ROUGE measures with human judgments onthe 100 words summary task were much better andmore stable than similar tasks in DUC 2001 and2003. Statistically stable human judgments of system performance might not be obtained due to lackof samples and this in turn caused instability of correlation analyses.7</corps><conclusion>ConclusionsIn this paper, we introduced ROUGE, an automaticevaluation package for summarization, and conducted comprehensive evaluations of the automaticmeasures included in the ROUGE package usingthree years of DUC data. To check the significanceof the results, we estimated confidence intervals ofcorrelations using bootstrap resampling. We foundthat (1) ROUGE-2, ROUGE-L, ROUGE-W, andROUGE-S worked well in single document summarization tasks, (2) ROUGE-1, ROUGE-L, ROUGE-W,ROUGE-SU4, and ROUGE-SU9 performed great inevaluating very short summaries (or headline-likesummaries), (3) correlation of high 90% was hard toachieve for multi-document summarization tasks butROUGE-1, ROUGE-2, ROUGE-S4, ROUGE-S9,ROUGE-SU4, and ROUGE-SU9 worked reasonablywell when stopwords were excluded from matching,(4) exclusion of stopwords usually improved correlation, and (5) correlations to human judgmentswere increased by using multiple references.In summary, we showed that the ROUGE packagecould be used effectively in automatic evaluation ofsummaries. In a separate study (Lin and Och, 2004),ROUGE-L, W, and S were also shown to be veryeffective in automatic evaluation of machinetranslation. The stability and reliability of ROUGE atdifferent sample sizes was reported by the author in(Lin, 2004). However, how to achieve high correlation with human judgments in multi-documentsummarization tasks as ROUGE already did in singledocument summarization tasks is still an open research topic.8AcknowledgementsThe author would like to thank the anonymous reviewers for their constructive comments, Paul Overat NIST, U.S.A, and ROUGE users around the worldfor testing and providing useful feedback on earlierversions of the ROUGE evaluation package, and theDARPA TIDES project for supporting this research.ReferencesCormen, T. R., C. E. Leiserson, and R. L. Rivest.1989. Introduction to Algorithms. The MIT Press.Davison, A. C. and D. V. Hinkley. 1997. BootstrapMethods and Their Application. Cambridge University Press.Lin, C.-Y. and E. H. Hovy. 2003. Automatic evaluation of summaries using n-gram co-occurrencestatistics. In Proceedings of 2003 LanguageTechnology Conference (HLT-NAACL 2003),Edmonton, Canada.Lin, C.-Y. 2004. Looking for a few good metrics:ROUGE and its evaluation. In Proceedings ofNTCIR Workshop 2004, Tokyo, Japan.Lin, C.-Y. and F. J. Och. 2004. Automatic evaluation of machine translation quality using longestcommon subsequence and skip-bigram statistics.In Proceedings of 42nd Annual Meeting of ACL(ACL 2004), Barcelona, Spain.Mani, I. 2001. Automatic Summarization. John Benjamins Publishing Co.Melamed, I. D. 1995. Automatic evaluation and uniform filter cascades for inducing n-best translation lexicons. In Proceedings of the 3 rd Workshopon Very Large Corpora (WVLC3). Boston,U.S.A.Melamed, I. D., R. Green and J. P. Turian (2003).Precision and recall of machine translation. InProceedings of 2003 Language Technology Conference (HLT-NAACL 2003), Edmonton, Canada.Over, P. and J. Yen. 2003. An introduction to DUC2003  Intrinsic evaluation of generic news textsummarizationsystems.AAAAAAAAAAhttp://www-nlpir.nist.gov/projects/duc/pubs/2003slides/duc2003intro.pdfPapineni, K., S. Roukos, T. Ward, and W.-J. Zhu.2001. BLEU : A method for automatic evaluationof machine translation. IBM Research ReportRC22176 (W0109-022).Saggion H., D. Radev, S. Teufel, and W. Lam.2002. Meta-evaluation of summaries in a crosslingual environment using content-based metrics.In Proceedings of COLING-2002, Taipei, Taiwan.Radev, D. S. Teufel, H. Saggion, W. Lam, J. Blitzer, A. Gelebi, H. Qi, E. Drabek, and D. Liu.2002. Evaluation of Text Summarization in aCross-Lingual Information Retrieval Framework.Technical report, Center for Language andSpeech Processing, Johns Hopkins University,Baltimore, MD, USA.Van Rijsbergen, C. J. 1979. Information Retrieval.Butterworths. London.</conclusion><discussion></discussion><biblio>Multiple ReferencesSo far, we only demonstrated how to computeROUGE-N using a single reference. When multiplereferences are used, we compute pairwise summarylevel ROUGE-N between a candidate summary s andevery reference, ri , in the reference set. We thentake the maximum of pairwise summary-levelROUGE-N scores as the final multiple referenceROUGE-N score. This can be written as follows:ROUGE-Nmulti = argmaxi ROUGE-N(ri ,s)This procedure is also applied to computation ofROUGE-L (Section 3), ROUGE-W (Section 4) , andROUGE-S (Section 5). In the implementation, we usea Jackknifing procedure. Given M references, wecompute the best score over M sets of M-1 references. The final ROUGE-N score is the average ofthe M ROUGE-N scores using different M-1 references. The Jackknifing procedure is adopted sincewe often need to compare system and human performance and the reference summaries are usuallythe only human summaries available. Using thisprocedure, we are able to estimate average humanperformance by averaging M ROUGE-N scores ofone reference vs. the rest M-1 references. Althoughthe Jackknif ing procedure is not necessary when wejust want to compute ROUGE scores using multiplereferences, it is applied in all ROUGE score computations in the ROUGE evaluation package.In the next section, we describe a ROUGE measurebased on longest common subsequences betweentwo summaries.3ROUGE-L: Longest Common Subs equenceA sequence Z = [z1 , z2, ..., zn ] is a subsequence ofanother sequence X = [x1 , x2 , ..., x m ], if there exists astrict increasing sequence [i1 , i2 , ..., ik] of indices ofX such that for all j = 1, 2, ..., k, we have xij = zj(Cormen et al., 1989). Given two sequences X andY, the longest common subsequence (LCS) of X andY is a common subsequence with maximum length.LCS has been used in identifying cognate candidates during construction of N-best translation lexicon from parallel text. Melamed (1995) used theratio (LCSR) between the length of the LCS of twowords and the length of the longer word of the twowords to measure the cognateness between them.He used LCS as an approximate string matchingalgorithm. Saggion et al. (2002) used normalizedpairwise LCS to compare simila rity between twotexts in automatic summarization evaluation.3.1Sentence-Level LCSTo apply LCS in summarization evaluation, weview a summary sentence as a sequence of words.The intuition is that the longer the LCS of twosummary sentences is, the more similar the twosummaries are. We propose using LCS-based Fmeasure to estimate the similarity between twosummaries X of length m and Y of length n, assuming X is a reference summary sentence and Y is acandidate summary sentence, as follows:LCS ( X , Y )(2)mLCS ( X , Y )Plcs =(3)n(1 +  2 ) Rlcs PlcsFlcs =(4)Rlcs +  2PlcsRlcs =Where LCS(X,Y) is the length of a longest common subsequence of X and Y, and  = Plcs/Rlcs when?Flcs/?Rlcs_ =_ ?Flcs/?Plcs . In DUC,  is set to a verybig number (? 8 ). Therefore, only Rlcs is considered. We call the LCS-based F-measure, i.e. Equation 4, ROUGE-L. Notice that ROUGE-L is 1 when X= Y; while ROUGE-L is zero when LCS(X,Y) = 0, i.e.there is nothing in common between X and Y. Fmeasure or its equivalents has been shown to havemet several theoretical criteria in measuring accuracy involving more than one factor (Van Rijsbergen, 1979). The composite factors are LCS-basedrecall and precision in this case. Melamed et al.(2003) used unigram F-measure to estimate machinetranslation quality and showed that unigram Fmeasure was as good as BLEU .One advantage of using LCS is that it does not require consecutive matches but in-sequence matchesthat reflect sentence level word order as n-grams.The other advantage is that it automatically includeslongest in-sequence common n-grams, therefore nopredefined n-gram length is necessary.ROUGE-L as defined in Equation 4 has the property that its value is less than or equal to the min imum of unigram F-measure of X and Y. Unigramrecall reflects the proportion of words in X (reference summary sentence) that are also present in Y(candidate summary sentence); while unigram precision is the proportion of words in Y that are also inX. Unigram recall and precision count all cooccurring words regardless their orders; whileROUGE-L counts only in-sequence co-occurrences.By only awarding credit to in-sequence unigrammatches, ROUGE-L also captures sentence levelstructure in a natural way. Consider the followingexample:S1. police killed the gunmanS2. police kill the gunmanS3. the gunman kill policeWe only consider ROUGE-2, i.e. N=2, for the purpose of explanation. Using S1 as the reference andS2 and S3 as the candidate summary sentences, S2and S3 would have the same ROUGE-2 score, sincethey both have one bigram, i.e. the gunman. However, S2 and S3 have very different meanings. In thecase of ROUGE-L, S2 has a score of 3/4 = 0.75 andS3 has a score of 2/4 = 0.5, with  = 1. Therefore S2is better than S3 according to ROUGE-L. This example also illustrated that ROUGE-L can work reliablyat sentence level.However, LCS suffers one disadvantage that itonly counts the main in-sequence words; therefore,other alternative LCSes and shorter sequences arenot reflected in the final score. For example, giventhe following candidate sentence:S4. the gunman police killedUsing S1 as its reference, LCS counts either thegunman or police killed, but not both; therefore,S4 has the same ROUGE-L score as S3. ROUGE-2would prefer S4 than S3.3.2Previous section described how to compute sentence-level LCS-based F-measure score. When applying to summary-level, we take the union LCSmatches between a reference summary sentence, ri ,and every candidate summary sentence, cj . Given areference summary of u sentences containing a totalof m words and a candidate summary of v sentencescontaining a total of n words, the summary-levelLCS-based F-measure can be computed as follows:uRlcs = LCS ( ri , C )i =1m(5)uPlcs = LCS ( ri , C )i =1n(6)(7)Again  is set to a very big number (? 8 ) inDUC, i.e. only Rlcs is considered. LCS  ( ri , C) is theLCS score of the union longest common subsequence between reference sentence ri and candidatesummary C. For example, if ri = w1 w2 w3 w4 w5 , andC contains two sentences: c1 = w1 w2 w6 w7 w8 and c2= w1 w3 w8 w9 w5 , then the longest common subsequence of ri and c1 is w1 w2  and the longest common subsequence of ri and c2 is w1 w3 w5 . Theunion longest common subsequence of ri , c1 , and c2is w1 w2 w3 w5 and LCS  ( ri , C) = 4/5.3.3ROUGE-L vs. Normalized Pairwise LCSThe normalized pairwise LCS proposed by Radev etal. (page 51, 2002) between two summaries S1 andS2, LCS(S1 ,S 2 )MEAD , is written as follows:s i S 1length( s ) + max s j S 2 LCS (si , s j ) +s i S 1is j S2s j S 2max s i S 1 LCS (s i , s j )(8)length (s j )Assuming S1 has m words and S2 has n words,Equation 8 can be rewritten as Equation 9 due tosymmetry:2*s i S1max s j S2 LCS ( s i , s j )m+n(9)We then define MEAD LCS recall (Rlcs-MEAD) andMEAD LCS precision (Plcs-MEAD ) as follows:Rlcs-MEAD =Summary-Level LCS(1 +  2 ) Rlcs PlcsRlcs +  2PlcsFlcs =Plcs-MEAD =siS1max s jS2 LCS ( si , s j )msiS1max s jS2 LCS ( si , s j )n(10)(11)We can rewrite Equation (9) in terms of Rlcs-MEADand Plcs-MEAD with a constant parameter  = 1 as follows:LCS(S1 ,S 2 )MEAD =(1 +  2 ) Rlcs  MEAD Plcs  MEAD(12)Rlcs  MEAD +  2Plcs  MEADEquation 12 shows that normalized pairwise LCSas defined in Radev et al. (2002) and implementedin MEAD is also a F-measure with  = 1. Sentencelevel normalized pairwise LCS is the same asROUGE-L with  = 1. Besides setting  = 1, summary-level normalized pairwise LCS is differentfrom ROUGE-L in how a sentence gets its LCS scorefrom its references. Normalized pairwise LCS takesthe best LCS score while ROUGE-L takes the unionLCS score.4ROUGE-W: Weighted Longest Common SubsequenceLCS has many nice properties as we have describedin the previous sections. Unfortunately, the basicLCS also has a problem that it does not differentiateLCSes of different spatial relations within their embedding sequences. For example, given a referencesequence X and two candidate sequences Y1 and Y2as follows:X:Y1 :Y2 :[A B C D E F G][A B C D H I K][A H B K C I D]Y1 and Y2 have the same ROUGE-L score. However, in this case, Y1 should be the better choice thanY2 because Y1 has consecutive matches. To improvethe basic LCS method, we can simply remember thelength of consecutive matches encountered so far toa regular two dimensional dynamic program tablecomputing LCS. We call this weighted LCS(WLCS) and use k to indicate the length of the current consecutive matches ending at words xi and yj.Given two sentences X and Y, the WLCS score of Xand Y can be computed using the following dynamicprogramming procedure:(1) For (i = 0; i &lt;=m; i++)c(i,j) = 0 // initialize c-tablew(i,j) = 0 // initialize w-table(2) For (i = 1; i &lt;= m; i++)For (j = 1; j &lt;= n; j++)If xi = yj Then// the length of consecutive matches at// position i-1 and j -1k = w(i-1,j-1)c(i,j) = c(i-1,j-1) + f(k+1 )  f(k)// remember the length of consecutive// matches at position i, jw(i,j) = k+1OtherwiseIf c(i-1,j) &gt; c(i,j-1) Thenc(i,j) = c(i-1,j)w(i,j) = 0// no match at i, jElse c(i,j) = c(i,j-1)w(i,j) = 0// no match at i, j(3) WLCS(X,Y) = c(m,n)Where c is the dynamic programming table, c(i,j)stores the WLCS score ending at word xi of X and yjof Y, w is the table storing the length of consecutivematches ended at c table position i and j, and f is afunction of consecutive matches at the table posi-tion, c(i,j). Notice that by providing differentweighting function f, we can parameterize theWLCS algorithm to assign different credit to consecutive in-sequence matches.The weighting function f must have the propertythat f(x+y) &gt; f(x) + f(y) for any positive integers xand y. In other words, consecutive matches areawarded more scores than non-consecutive matches.For example, f(k)-=-k   when k &gt;= 0, and ,  &gt;0. This function charges a gap penalty of  foreach non-consecutive n-gram sequences. Anotherpossible function family is the polynomial family ofthe form k  where - &gt; 1. However, in order tonorma lize the final ROUGE-W score, we also preferto have a function that has a close form inversefunction. For example, f(k)-=-k 2 has a close forminverse function f -1 (k)-=-k 1/2. F-measure based onWLCS can be computed as follows, given two sequences X of length m and Y of length n: WLCS ( X , Y ) f ( m) WLCS ( X , Y ) = f 1 f (n )Rwlcs = fPwlcsFwlcs1(1 +  2 ) Rwlcs Pwlcs=Rwlcs +  2 Pwlcs(13)(14)(15)Where f -1 is the inverse function of f. In DUC,  isset to a very big number (? 8 ). Therefore, onlyRwlcs is considered. We call the WLCS-based Fmeasure, i.e. Equation 15, ROUGE-W. Using Equation 15 and f(k)-=-k2 as the weighting function, theROUGE-W scores for sequences Y1 and Y2 are 0.571and 0.286 respectively. Therefore, Y1 would beranked higher than Y2 using WLCS. We use thepolynomial function of the form k  in the ROUGEevaluation package. In the next section, we introduce the skip-bigram co-occurrence statistics.5ROUGE-S: Skip-Bigram Co-Occurrence StatisticsSkip-bigram is any pair of words in their sentenceorder, allowing for arbitrary gaps. Skip-bigram cooccurrence statistics measure the overlap of skipbigrams between a candidate translation and a set ofreference translations. Using the example given inSection 3.1:S1.S2.S3.S4.police killed the gunmanpolice kill the gunmanthe gunman kill policethe gunman police killedeach sentence has C(4,2) 1 = 6 skip-bigrams. For example, S1 has the following skip-bigrams:(police killed, police the, police gunman,killed the, killed gunman, the gunman)S2 has three skip-bigram matches with S1 (police the, police gunman, the gunman), S3 hasone skip-bigram match with S1 (the gunman), andS4 has two skip-bigram matches with S1 (policekille d, the gunman). Given translations X oflength m and Y of length n, assuming X is a reference translation and Y is a candidate translation, wecompute skip-bigram-based F-measure as follows:SKIP 2( X , Y )(16)C ( m, 2)SKIP 2( X , Y )Pskip2 =(17)C ( n,2)(1 +  2 ) Rskip2 Pskip2Fskip2 =(18)Rskip2 +  2 Pskip2Rskip2 =Where SKIP2(X,Y) is the number of skip-bigrammatches between X and Y,  controlling the relativeimportance of Pskip2 and Rskip2 , and C is the combination function. We call the skip-bigram-based Fmeasure, i.e. Equation 18, ROUGE-S.Using Equation 18 with  = 1 and S1 as the reference, S2s ROUGE-S score is 0.5, S3 is 0.167, andS4 is 0.333. Therefore, S2 is better than S3 and S4,and S4 is better than S3. This result is more intuitivethan using BLEU -2 and ROUGE-L. One advantage ofskip-bigram vs. BLEU is that it does not require consecutive matches but is still sensitive to word order.Comparing skip-bigram with LCS, skip-bigramcounts all in-order matching word pairs while LCSonly counts one longest common subsequence.Applying skip-bigram without any constraint onthe distance between the words, spurious matchessuch as the the or of in might be counted asvalid matches. To reduce these spurious matches,we can limit the maximum skip distance, d skip , between two in-order words that is allowed to form askip-bigram. For example, if we set d skip to 0 thenROUGE-S is equivalent to bigram overlap Fmeasure. If we set d skip to 4 then only word pairs ofat most 4 words apart can form skip-bigrams.Adjusting Equations 16, 17, and 18 to use maximum skip distance limit is straightforward: we onlycount the skip-bigram matches, SKIP2 (X,Y), withinthe maximum skip distance and replace denominators of Equations 16, C(m,2), and 17, C(n,2), withthe actual numbers of within distance skip-bigramsfrom the reference and the candidate respectively.5.1One potential problem for ROUGE-S is that it doesnot give any credit to a candidate sentence if thesentence does not have any word pair co-occurringwith its references. For example, the following sentence has a ROUGE-S score of zero:S5. gunman the killed policeS5 is the exact reverse of S1 and there is no skipbigram match between them. However, we wouldlike to differentiate sentences similar to S5 fromsentences that do not have single word cooccurrence with S1. To achieve this, we extendROUGE-S with the addition of unigram as countingunit. The extended version is called ROUGE-SU. Wecan also obtain ROUGE-SU from ROUGE-S by adding a begin-of-sentence marker at the beginning ofcandidate and reference sentences.6Evaluations of ROUGETo assess the effectiveness of ROUGE measures, wecompute the correlation between ROUGE assignedsummary scores and human assigned summaryscores. The intuition is that a good evaluation measure should assign a good score to a good summaryand a bad score to a bad summary. The ground truthis based on human assigned scores. Acquiring human judgments are usually very expensive; fortunately, we have DUC 2001, 2002, and 2003evaluation data that include human judgments forthe following: Single document summaries of about 100words: 12 systems 2 for DUC 2001 and 14 systems for 2002. 149 single document summarieswere judged per system in DUC 2001 and 295were judged in DUC 2002. Single document very short summaries of about10 words (headline-like, keywords, or phrases):14 systems for DUC 2003. 624 very short summaries were judged per system in DUC 2003. Multi-document summaries of about 10 words:6 systems for DUC 2002; 50 words: 14 systemsfor DUC 2001 and 10 systems for DUC 2002;100 words: 14 systems for DUC 2001, 10 systems for DUC 2002, and 18 systems for DUC2003; 200 words: 14 systems for DUC 2001 and10 systems for DUC 2002; 400 words: 14 systems for DUC 2001. 29 summaries were judgedper system per summary size in DUC 2001, 59were judged in DUC 2002, and 30 were judgedin DUC 2003.21C(4,2) = 4!/(2!*2!) = 6.ROUGE-SU: Extension of ROUGE-SAll systems include 1 or 2 baselines. Please see DUCwebsite for details.DUC 2001 100 WORDS SINGLE DOCDUC 2002 100 WORDS SINGLE DOC1 REF3 REFS1 REF2 REFSMethod CASE STEM STOP CASE STEM STOP CASE STEM STOP CASE STEM STOPR-10.76 0.760.84 0.80 0.78 0.84 0.98 0.98 0.99 0.98 0.98 0.99R-20.84 0.840.83 0.87 0.87 0.86 0.99 0.99 0.99 0.99 0.99 0.99R-30.82 0.830.80 0.86 0.86 0.85 0.99 0.99 0.99 0.99 0.99 0.99R-40.81 0.810.77 0.84 0.84 0.83 0.99 0.99 0.98 0.99 0.99 0.99R-50.79 0.790.75 0.83 0.83 0.81 0.99 0.99 0.98 0.99 0.99 0.98R-60.76 0.770.71 0.81 0.81 0.79 0.98 0.99 0.97 0.99 0.99 0.98R-70.73 0.740.65 0.79 0.80 0.76 0.98 0.98 0.97 0.99 0.99 0.97R-80.69 0.710.61 0.78 0.78 0.72 0.98 0.98 0.96 0.99 0.99 0.97R-90.65 0.670.59 0.76 0.76 0.69 0.97 0.97 0.95 0.98 0.98 0.96R-L0.83 0.830.83 0.86 0.86 0.86 0.99 0.99 0.99 0.99 0.99 0.99R-S*0.74 0.740.80 0.78 0.77 0.82 0.98 0.98 0.98 0.98 0.97 0.98R-S40.84 0.850.84 0.87 0.88 0.87 0.99 0.99 0.99 0.99 0.99 0.99R-S90.84 0.850.84 0.87 0.88 0.87 0.99 0.99 0.99 0.99 0.99 0.99R-SU*0.74 0.740.81 0.78 0.77 0.83 0.98 0.98 0.98 0.98 0.98 0.98R-SU40.84 0.840.85 0.87 0.87 0.87 0.99 0.99 0.99 0.99 0.99 0.99R-SU90.84 0.840.85 0.87 0.87 0.87 0.99 0.99 0.99 0.99 0.99 0.99R-W-1.2 0.85 0.850.85 0.87 0.87 0.87 0.99 0.99 0.99 0.99 0.99 0.99Table 1: Pearsons correlations of 17 ROUGEmeasure scores vs. human judgments for the DUC2001 and 2002 100 words single document summarization tasksMethodR-1R-2R-3R-4R-5R-6R-7R-8R-9R-LR-S*R-S4R-S9R-SU*R-SU4R-SU9R-W-1.2DUC 2003 10 WORDS SINGLE DOC1 REF 4REFS 1 REF 4 REFS 1 REF 4 REFSCASESTEMSTOP0.960.950.950.950.900.900.750.760.750.750.760.770.710.700.700.680.730.700.640.650.620.630.690.660.620.640.600.630.630.600.570.620.550.610.460.540.560.560.580.600.460.440.550.530.540.550.000.240.510.470.510.490.000.140.970.960.970.960.970.960.890.870.880.850.950.920.880.890.880.880.950.960.920.920.920.910.970.950.930.900.910.890.960.940.970.960.960.950.980.970.970.950.960.940.970.950.960.960.960.960.960.96Table 2: Pearsons correlations of 17 ROUGEmeasure scores vs. human judgments for the DUC2003 very short summary taskBesides these human judgments, we also have 3 setsof manual summaries for DUC 2001, 2 sets forDUC 2002, and 4 sets for DUC 2003. Humanjudges assigned content coverage scores to a candidate summary by examining the percentage of content overlap between a manual summary unit, i.e.elementary discourse unit or sentence, and the candidate summary using Summary Evaluation Environment 3 (SEE) developed by the University ofSouthern Californias Information Sciences Institute(ISI). The overall candidate summary score is theaverage of the content coverage scores of all theunits in the manual summary. Note that humanjudges used only one manual summary in all theevaluations although multiple alternative summarieswere available.With the DUC data, we computed Pearsonsproduct moment correlation coefficients, Spearmans rank order correlation coefficients, andKendalls correlation coefficients between systemsaverage ROUGE scores and their human assignedaverage coverage scores using single reference andmultiple references. To investigate the effect ofstemming and inclusion or exclusion of stopwords,we also ran experiments over orig inal automatic and3SEE is available online at http://www.isi.edu/~cyl.manual summaries (CASE set), stemmed4 version ofthe summaries (STEM set), and stopped version ofthe summaries (STOP set). For example, we computed ROUGE scores for the 12 systems participatedin the DUC 2001 single document summarizationevaluation using the CASE set with single referenceand then calculated the three correlation scores forthese 12 systems ROUGE scores vs. human assignedaverage coverage scores. After that we repeated theprocess using multiple references and then usingSTEM and STOP sets. Therefore, 2 (multi or single)x 3 (CASE, STEM, or STOP) x 3 (Pearson, Spearman, or Kendall) = 18 data points were collected foreach ROUGE measure and each DUC task. To assessthe significance of the results, we applied bootstrapresampling technique (Davison and Hinkley, 1997)to estimate 95% confidence intervals for every correlation computation.17 ROUGE measures were tested for each run using ROUGE evaluation package v1.2.1: ROUGE-Nwith N = 1 to 9, ROUGE-L, ROUGE-W withweighting factor  = 1.2, ROUGE-S and ROUGE-SUwith maximum skip distance d skip = 1, 4, and 9. Dueto limitation of space, we only report correlationanalysis results based on Pearsons correlation coefficient. Correlation analyses based on Spearmansand Kendalls correlation coefficients are trackingPearsons very closely and will be posted later at theROUGE website 5 for reference. The critical value 6for Pearsons correlation is 0.632 at 95% confidencewith 8 degrees of freedom.Table 1 shows the Pearsons correlation coefficients of the 17 ROUGE measures vs. human judgments on DUC 2001 and 2002 100 words singledocument summarization data. The best values ineach column are marked with dark (green) color andstatistically equivalent values to the best values aremarked with gray. We found that correlations werenot affected by stemming or removal of stopwordsin this data set, ROUGE-2 performed better amongthe ROUGE-N variants, ROUGE-L, ROUGE-W, andROUGE-S were all performing well, and using multiple references improved performance though notmuch. All ROUGE measures achieved very goodcorrelation with human judgments in the DUC 2002data. This might due to the double sample size inDUC 2002 (295 vs. 149 in DUC 2001) for each system.Table 2 shows the correlation analysis results onthe DUC 2003 single document very short summarydata. We found that ROUGE-1, ROUGE-L, ROUGE4Porters stemmer was used.ROUGE website: http://www.isi.edu/~cyl/ROUGE.6 The critical values for Pearsons correlation at 95%confidence with 10, 12, 14, and 16 degrees of freedomare 0.576, 0.532, 0.497, and 0.468 respectively.5(A1) DUC 2001 100 WORDS MULTI(A2) DUC 2002 100 WORDS MULTI(A3) DUC 2003 100 WORDS MULTI1 RFF3 REFS1 REF2 REFS1 REF4 REFSMethod CASE STEM STOP CASE STEM STOP CASE STEM STOP CASE STEM STOP CASE STEM STOP CASE STEM STOPR-10.480.560.86 0.530.57 0.870.660.660.77 0.710.710.78 0.580.570.71 0.58 0.570.71R-20.550.570.64 0.590.61 0.710.830.830.80 0.880.870.85 0.690.670.71 0.79 0.790.81R-30.460.450.47 0.530.53 0.550.850.840.76 0.890.880.83 0.540.510.48 0.76 0.750.74R-40.390.390.43 0.480.49 0.470.800.800.63 0.830.820.75 0.370.360.36 0.62 0.610.52R-50.380.390.33 0.470.48 0.430.730.730.45 0.730.730.62 0.250.250.27 0.45 0.440.38R-60.390.390.20 0.450.46 0.390.710.720.38 0.660.640.46 0.210.210.26 0.34 0.310.29R-70.310.310.17 0.440.44 0.360.630.650.33 0.560.530.44 0.200.200.23 0.29 0.270.25R-80.180.190.09 0.400.40 0.310.550.550.52 0.500.460.52 0.180.180.21 0.23 0.220.23R-90.110.120.06 0.380.38 0.280.540.540.52 0.450.420.52 0.160.160.19 0.21 0.210.21R-L0.490.490.49 0.560.56 0.560.620.620.62 0.650.650.65 0.500.500.50 0.53 0.530.53R-S*0.450.520.84 0.510.54 0.860.690.690.77 0.730.730.79 0.600.600.67 0.61 0.600.70R-S40.460.500.71 0.540.57 0.780.790.800.79 0.840.850.82 0.630.640.70 0.73 0.730.78R-S90.420.490.77 0.530.56 0.810.790.800.78 0.830.840.81 0.650.650.70 0.70 0.700.76R-SU*0.450.520.84 0.510.54 0.870.690.690.77 0.730.730.79 0.600.590.67 0.60 0.600.70R-SU40.470.530.80 0.550.58 0.830.760.760.79 0.800.810.81 0.640.640.74 0.68 0.680.76R-SU90.440.500.80 0.530.57 0.840.770.780.78 0.810.820.81 0.650.650.72 0.68 0.680.75R-W-1.2 0.520.520.52 0.600.60 0.600.670.670.67 0.690.690.69 0.530.530.53 0.58 0.580.58(C)Method CASER-10.71R-20.82R-30.59R-40.25R-5-0.25R-60.00R-70.00R-80.00R-90.00R-L0.78R-S*0.83R-S40.85R-S90.82R-SU*0.75R-SU40.76R-SU90.74R-W-1.2 0.78DUC02STEM0.680.850.740.36-0.250.000.000.000.000.780.820.860.810.740.750.730.7810(D1) DUC01 50(D2) DUC02 50(E1) DUC01 200(E2)STOP CASE STEM STOP CASE STEM STOP CASE STEM STOP CASE0.49 0.490.49 0.730.440.480.80 0.810.810.90 0.840.80 0.430.45 0.590.470.490.62 0.840.850.86 0.930.75 0.320.33 0.390.360.360.45 0.800.800.81 0.900.16 0.280.26 0.360.280.280.39 0.770.780.78 0.87-0.24 0.300.29 0.310.280.300.49 0.770.760.72 0.820.00 0.220.23 0.410.180.21 -0.17 0.750.750.67 0.780.00 0.260.23 0.500.110.160.00 0.720.720.62 0.720.00 0.320.32 0.34-0.11 -0.11 0.00 0.680.680.54 0.710.00 0.300.30 0.34-0.14 -0.14 0.00 0.640.640.48 0.700.78 0.560.56 0.560.500.500.50 0.810.810.81 0.880.69 0.460.45 0.740.460.490.80 0.800.800.90 0.840.76 0.400.41 0.690.420.440.73 0.820.820.87 0.910.69 0.420.41 0.720.400.430.78 0.810.820.86 0.900.56 0.460.46 0.740.460.490.80 0.800.800.90 0.840.58 0.450.45 0.720.440.460.78 0.820.830.89 0.900.56 0.440.44 0.730.410.450.79 0.820.820.88 0.890.78 0.560.56 0.560.510.510.51 0.840.840.84 0.90DUC02 200STEM STOP0.840.910.930.940.910.910.880.880.830.840.790.770.730.740.710.700.690.590.880.880.850.930.910.930.900.920.850.930.900.930.890.920.900.90(F) DUC01CASE STEM0.74 0.730.88 0.880.84 0.840.80 0.800.77 0.770.74 0.740.70 0.700.66 0.660.63 0.620.82 0.820.75 0.740.85 0.850.83 0.830.75 0.740.84 0.840.83 0.820.86 0.86400STOP0.900.870.820.750.700.630.580.520.460.820.890.850.840.890.880.870.86Table 3: Pearsons correlations of 17 ROUGE measure scores vs. human judgments forthe DUC 2001, 2002, and 2003 mult i-document summarization tasksSU4 and 9, and ROUGE-W were very good measuresin this category, ROUGE-N with N &gt; 1 performedsignificantly worse than all other measures, and exclusion of stopwords improved performance in general except for ROUGE-1. Due to the large numberof samples (624) in this data set, using multiple references did not improve correlations.In Table 3 A1, A2, and A3, we show correlationanalysis results on DUC 2001, 2002, and 2003 100words multi-document summarization data. Theresults indicated that using multiple references improved correlation and exclusion of stopwords usually improved performance. ROUGE-1, 2, and 3performed fine but were not consistent. ROUGE-1,ROUGE-S4, ROUGE-SU4, ROUGE-S9, and ROUGESU9 with stopword removal had correlation above0.70. ROUGE-L and ROUGE-W did not work well inthis set of data.Table 3 C, D1, D2, E1, E2, and F show the correlation analyses using multiple references on the restof DUC data. These results again suggested thatexclusion of stopwords achieved better performanceespecially in multi-document summaries of 50words. Better correlations (&gt; 0.70) were observedon long summary tasks, i.e. 200 and 400 wordssummaries. The relative performance of ROUGEmeasures followed the pattern of the 100 wordsmulti-document summarization task.Comparing the results in Table 3 with Table s 1and 2, we found that correlation values in the multidocument tasks rarely reached high 90% except inlong summary tasks. One possible explanation ofthis outcome is that we did not have large amount ofsamples for the multi-document tasks. In the singledocument summarization tasks we had over 100samples; while we only had about 30 samples in themulti-document tasks. The only tasks that had over30 samples was from DUC 2002 and the correlations of ROUGE measures with human judgments onthe 100 words summary task were much better andmore stable than similar tasks in DUC 2001 and2003. Statistically stable human judgments of system performance might not be obtained due to lackof samples and this in turn caused instability of correlation analyses.7ConclusionsIn this paper, we introduced ROUGE, an automaticevaluation package for summarization, and conducted comprehensive evaluations of the automaticmeasures included in the ROUGE package usingthree years of DUC data. To check the significanceof the results, we estimated confidence intervals ofcorrelations using bootstrap resampling. We foundthat (1) ROUGE-2, ROUGE-L, ROUGE-W, andROUGE-S worked well in single document summarization tasks, (2) ROUGE-1, ROUGE-L, ROUGE-W,ROUGE-SU4, and ROUGE-SU9 performed great inevaluating very short summaries (or headline-likesummaries), (3) correlation of high 90% was hard toachieve for multi-document summarization tasks butROUGE-1, ROUGE-2, ROUGE-S4, ROUGE-S9,ROUGE-SU4, and ROUGE-SU9 worked reasonablywell when stopwords were excluded from matching,(4) exclusion of stopwords usually improved correlation, and (5) correlations to human judgmentswere increased by using multiple references.In summary, we showed that the ROUGE packagecould be used effectively in automatic evaluation ofsummaries. In a separate study (Lin and Och, 2004),ROUGE-L, W, and S were also shown to be veryeffective in automatic evaluation of machinetranslation. The stability and reliability of ROUGE atdifferent sample sizes was reported by the author in(Lin, 2004). However, how to achieve high correlation with human judgments in multi-documentsummarization tasks as ROUGE already did in singledocument summarization tasks is still an open research topic.8AcknowledgementsThe author would like to thank the anonymous reviewers for their constructive comments, Paul Overat NIST, U.S.A, and ROUGE users around the worldfor testing and providing useful feedback on earlierversions of the ROUGE evaluation package, and theDARPA TIDES project for supporting this research.ReferencesCormen, T. R., C. E. Leiserson, and R. L. Rivest.1989. Introduction to Algorithms. The MIT Press.Davison, A. C. and D. V. Hinkley. 1997. BootstrapMethods and Their Application. Cambridge University Press.Lin, C.-Y. and E. H. Hovy. 2003. Automatic evaluation of summaries using n-gram co-occurrencestatistics. In Proceedings of 2003 LanguageTechnology Conference (HLT-NAACL 2003),Edmonton, Canada.Lin, C.-Y. 2004. Looking for a few good metrics:ROUGE and its evaluation. In Proceedings ofNTCIR Workshop 2004, Tokyo, Japan.Lin, C.-Y. and F. J. Och. 2004. Automatic evaluation of machine translation quality using longestcommon subsequence and skip-bigram statistics.In Proceedings of 42nd Annual Meeting of ACL(ACL 2004), Barcelona, Spain.Mani, I. 2001. Automatic Summarization. John Benjamins Publishing Co.Melamed, I. D. 1995. Automatic evaluation and uniform filter cascades for inducing n-best translation lexicons. In Proceedings of the 3 rd Workshopon Very Large Corpora (WVLC3). Boston,U.S.A.Melamed, I. D., R. Green and J. P. Turian (2003).Precision and recall of machine translation. InProceedings of 2003 Language Technology Conference (HLT-NAACL 2003), Edmonton, Canada.Over, P. and J. Yen. 2003. An introduction to DUC2003  Intrinsic evaluation of generic news textsummarizationsystems.AAAAAAAAAAhttp://www-nlpir.nist.gov/projects/duc/pubs/2003slides/duc2003intro.pdfPapineni, K., S. Roukos, T. Ward, and W.-J. Zhu.2001. BLEU : A method for automatic evaluationof machine translation. IBM Research ReportRC22176 (W0109-022).Saggion H., D. Radev, S. Teufel, and W. Lam.2002. Meta-evaluation of summaries in a crosslingual environment using content-based metrics.In Proceedings of COLING-2002, Taipei, Taiwan.Radev, D. S. Teufel, H. Saggion, W. Lam, J. Blitzer, A. Gelebi, H. Qi, E. Drabek, and D. Liu.2002. Evaluation of Text Summarization in aCross-Lingual Information Retrieval Framework.Technical report, Center for Language andSpeech Processing, Johns Hopkins University,Baltimore, MD, USA.Van Rijsbergen, C. J. 1979. Information Retrieval.Butterworths. London.</biblio></article></xml>